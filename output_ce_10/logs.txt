[06/13 12:51:05][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/13 12:51:05][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/13 12:51:05][INFO] visual_prompt:   99: Command line arguments: None
[06/13 12:51:05][INFO] visual_prompt:  108: Training with config:
[06/13 12:51:05][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': False,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.5,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/13 12:51:08][INFO] visual_prompt:   56: Total Parameters: 93872656	 Gradient Parameters: 8074768
[06/13 12:51:08][INFO] visual_prompt:   58: tuned percent:8.602
[06/13 12:51:09][INFO] visual_prompt:   44: Device used for model: 0
[06/13 12:51:09][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/13 12:51:09][INFO] visual_prompt:   49: Constructing imagenet dataset train...
[06/13 12:51:11][INFO] visual_prompt:  118: Number of images: 1281167
[06/13 12:51:11][INFO] visual_prompt:  119: Number of classes: 1000
[06/13 12:51:11][INFO] visual_prompt:   78: Loading validation data...
[06/13 12:51:11][INFO] visual_prompt:   49: Constructing imagenet dataset val...
[06/13 12:51:12][INFO] visual_prompt:  118: Number of images: 50000
[06/13 12:51:12][INFO] visual_prompt:  119: Number of classes: 1000
[06/13 12:51:12][INFO] visual_prompt:   81: Loading test data...
[06/13 12:51:12][INFO] visual_prompt:   83: ...no test data is constructed
[06/13 12:51:12][INFO] visual_prompt:  111: Constructing models...
[06/13 12:51:12][INFO] visual_prompt:  114: Setting up Evalutator...
[06/13 12:51:12][INFO] visual_prompt:  116: Setting up Trainer...
[06/13 12:51:12][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/13 12:51:12][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/13 12:51:12][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/13 12:51:12][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/13 12:51:12][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/13 12:51:12][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/13 12:51:12][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/13 12:51:12][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/13 12:51:12][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/13 12:51:12][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/13 12:51:12][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/13 12:51:12][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/13 12:51:12][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/13 12:51:12][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/13 12:51:12][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/13 12:51:12][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/13 12:51:12][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/13 12:51:12][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/13 12:51:12][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/13 12:51:12][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/13 12:51:12][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/13 12:51:12][INFO] visual_prompt:  229: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/13 12:51:12][INFO] visual_prompt:  248: Training 1 / 100 epoch, with learning rate 0.0
[06/13 12:54:36][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/13 12:54:36][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/13 12:54:36][INFO] visual_prompt:   99: Command line arguments: None
[06/13 12:54:36][INFO] visual_prompt:  108: Training with config:
[06/13 12:54:36][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': False,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.5,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/13 12:54:39][INFO] visual_prompt:   56: Total Parameters: 93872656	 Gradient Parameters: 8074768
[06/13 12:54:39][INFO] visual_prompt:   58: tuned percent:8.602
[06/13 12:54:40][INFO] visual_prompt:   44: Device used for model: 0
[06/13 12:54:40][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/13 12:54:40][INFO] visual_prompt:   49: Constructing imagenet dataset train...
[06/13 12:54:43][INFO] visual_prompt:  119: Number of images: 1281167
[06/13 12:54:43][INFO] visual_prompt:  120: Number of classes: 1000
[06/13 12:55:09][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/13 12:55:09][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/13 12:55:09][INFO] visual_prompt:   99: Command line arguments: None
[06/13 12:55:09][INFO] visual_prompt:  108: Training with config:
[06/13 12:55:09][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': False,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.5,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/13 12:55:13][INFO] visual_prompt:   56: Total Parameters: 93872656	 Gradient Parameters: 8074768
[06/13 12:55:13][INFO] visual_prompt:   58: tuned percent:8.602
[06/13 12:55:13][INFO] visual_prompt:   44: Device used for model: 0
[06/13 12:55:13][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/13 12:55:13][INFO] visual_prompt:   49: Constructing imagenet dataset train...
[06/13 12:55:16][INFO] visual_prompt:  119: Number of images: 1281167
[06/13 12:55:16][INFO] visual_prompt:  120: Number of classes: 1000
[06/13 12:55:16][INFO] visual_prompt:   78: Loading validation data...
[06/13 12:55:16][INFO] visual_prompt:   49: Constructing imagenet dataset val...
[06/13 12:55:16][INFO] visual_prompt:  119: Number of images: 50000
[06/13 12:55:16][INFO] visual_prompt:  120: Number of classes: 1000
[06/13 12:55:16][INFO] visual_prompt:   81: Loading test data...
[06/13 12:55:16][INFO] visual_prompt:   83: ...no test data is constructed
[06/13 12:55:16][INFO] visual_prompt:  111: Constructing models...
[06/13 12:55:16][INFO] visual_prompt:  114: Setting up Evalutator...
[06/13 12:55:16][INFO] visual_prompt:  116: Setting up Trainer...
[06/13 12:55:16][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/13 12:55:16][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/13 12:55:16][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/13 12:55:16][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/13 12:55:16][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/13 12:55:16][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/13 12:55:16][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/13 12:55:16][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/13 12:55:16][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/13 12:55:16][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/13 12:55:16][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/13 12:55:16][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/13 12:55:16][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/13 12:55:16][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/13 12:55:16][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/13 12:55:16][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/13 12:55:16][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/13 12:55:16][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/13 12:55:16][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/13 12:55:16][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/13 12:55:16][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/13 12:55:16][INFO] visual_prompt:  229: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/13 12:55:16][INFO] visual_prompt:  248: Training 1 / 100 epoch, with learning rate 0.0
[06/13 12:57:09][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/13 12:57:10][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/13 12:57:10][INFO] visual_prompt:   99: Command line arguments: None
[06/13 12:57:10][INFO] visual_prompt:  108: Training with config:
[06/13 12:57:10][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': False,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.5,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/13 12:57:13][INFO] visual_prompt:   56: Total Parameters: 93872656	 Gradient Parameters: 8074768
[06/13 12:57:13][INFO] visual_prompt:   58: tuned percent:8.602
[06/13 12:57:13][INFO] visual_prompt:   44: Device used for model: 0
[06/13 12:57:13][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/13 12:57:13][INFO] visual_prompt:   49: Constructing imagenet dataset train...
[06/13 12:57:16][INFO] visual_prompt:  120: Number of images: 1281167
[06/13 12:57:16][INFO] visual_prompt:  121: Number of classes: 1000
[06/13 12:57:16][INFO] visual_prompt:   78: Loading validation data...
[06/13 12:57:16][INFO] visual_prompt:   49: Constructing imagenet dataset val...
[06/13 12:57:16][INFO] visual_prompt:  120: Number of images: 50000
[06/13 12:57:16][INFO] visual_prompt:  121: Number of classes: 1000
[06/13 12:57:16][INFO] visual_prompt:   81: Loading test data...
[06/13 12:57:16][INFO] visual_prompt:   83: ...no test data is constructed
[06/13 12:57:16][INFO] visual_prompt:  111: Constructing models...
[06/13 12:57:16][INFO] visual_prompt:  114: Setting up Evalutator...
[06/13 12:57:16][INFO] visual_prompt:  116: Setting up Trainer...
[06/13 12:57:16][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/13 12:57:16][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/13 12:57:16][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/13 12:57:16][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/13 12:57:16][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/13 12:57:16][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/13 12:57:16][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/13 12:57:16][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/13 12:57:16][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/13 12:57:16][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/13 12:57:16][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/13 12:57:16][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/13 12:57:16][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/13 12:57:16][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/13 12:57:16][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/13 12:57:16][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/13 12:57:16][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/13 12:57:16][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/13 12:57:16][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/13 12:57:16][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/13 12:57:16][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/13 12:57:16][INFO] visual_prompt:  229: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/13 12:57:16][INFO] visual_prompt:  248: Training 1 / 100 epoch, with learning rate 0.0
[06/13 12:58:53][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/13 12:58:53][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/13 12:58:53][INFO] visual_prompt:   99: Command line arguments: None
[06/13 12:58:53][INFO] visual_prompt:  108: Training with config:
[06/13 12:58:53][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': False,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.5,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/13 12:58:57][INFO] visual_prompt:   56: Total Parameters: 93872656	 Gradient Parameters: 8074768
[06/13 12:58:57][INFO] visual_prompt:   58: tuned percent:8.602
[06/13 12:58:57][INFO] visual_prompt:   44: Device used for model: 0
[06/13 12:58:57][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/13 12:58:57][INFO] visual_prompt:   49: Constructing imagenet dataset train...
[06/13 12:59:00][INFO] visual_prompt:  121: Number of images: 1281167
[06/13 12:59:00][INFO] visual_prompt:  122: Number of classes: 1000
[06/13 12:59:00][INFO] visual_prompt:   78: Loading validation data...
[06/13 12:59:00][INFO] visual_prompt:   49: Constructing imagenet dataset val...
[06/13 12:59:00][INFO] visual_prompt:  121: Number of images: 50000
[06/13 12:59:00][INFO] visual_prompt:  122: Number of classes: 1000
[06/13 12:59:00][INFO] visual_prompt:   81: Loading test data...
[06/13 12:59:00][INFO] visual_prompt:   83: ...no test data is constructed
[06/13 12:59:00][INFO] visual_prompt:  111: Constructing models...
[06/13 12:59:00][INFO] visual_prompt:  114: Setting up Evalutator...
[06/13 12:59:00][INFO] visual_prompt:  116: Setting up Trainer...
[06/13 12:59:00][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/13 12:59:00][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/13 12:59:00][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/13 12:59:00][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/13 12:59:00][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/13 12:59:00][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/13 12:59:00][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/13 12:59:00][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/13 12:59:00][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/13 12:59:00][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/13 12:59:00][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/13 12:59:00][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/13 12:59:00][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/13 12:59:00][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/13 12:59:00][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/13 12:59:00][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/13 12:59:00][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/13 12:59:00][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/13 12:59:00][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/13 12:59:00][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/13 12:59:00][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/13 12:59:00][INFO] visual_prompt:  229: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/13 12:59:00][INFO] visual_prompt:  248: Training 1 / 100 epoch, with learning rate 0.0
[06/13 13:10:55][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/13 13:10:55][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/13 13:10:55][INFO] visual_prompt:   99: Command line arguments: None
[06/13 13:10:55][INFO] visual_prompt:  108: Training with config:
[06/13 13:10:55][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': False,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.5,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/13 13:10:59][INFO] visual_prompt:   56: Total Parameters: 93872656	 Gradient Parameters: 8074768
[06/13 13:10:59][INFO] visual_prompt:   58: tuned percent:8.602
[06/13 13:10:59][INFO] visual_prompt:   44: Device used for model: 0
[06/13 13:10:59][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/13 13:10:59][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/13 13:11:02][INFO] visual_prompt:  158: Number of images: 1281167
[06/13 13:11:02][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 13:11:02][INFO] visual_prompt:   78: Loading validation data...
[06/13 13:11:02][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/13 13:11:02][INFO] visual_prompt:  158: Number of images: 50000
[06/13 13:11:02][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 13:11:02][INFO] visual_prompt:   81: Loading test data...
[06/13 13:11:02][INFO] visual_prompt:   83: ...no test data is constructed
[06/13 13:11:02][INFO] visual_prompt:  111: Constructing models...
[06/13 13:11:02][INFO] visual_prompt:  114: Setting up Evalutator...
[06/13 13:11:02][INFO] visual_prompt:  116: Setting up Trainer...
[06/13 13:11:02][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/13 13:11:02][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/13 13:11:02][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/13 13:11:02][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/13 13:11:02][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/13 13:11:02][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/13 13:11:02][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/13 13:11:02][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/13 13:11:02][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/13 13:11:02][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/13 13:11:02][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/13 13:11:02][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/13 13:11:02][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/13 13:11:02][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/13 13:11:02][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/13 13:11:02][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/13 13:11:02][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/13 13:11:02][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/13 13:11:02][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/13 13:11:02][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/13 13:11:02][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/13 13:11:02][INFO] visual_prompt:  229: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/13 13:11:02][INFO] visual_prompt:  248: Training 1 / 100 epoch, with learning rate 0.0
[06/13 13:12:51][INFO] visual_prompt:  297: 	Training 100/5004. train loss: 6.9781,	0.8731 s / batch. (data: 3.31e-04). ETA=5 days, 1:20:24, max mem: 15.0 GB 
[06/13 13:13:52][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/13 13:13:52][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/13 13:13:52][INFO] visual_prompt:   99: Command line arguments: None
[06/13 13:13:52][INFO] visual_prompt:  108: Training with config:
[06/13 13:13:52][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.5,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/13 13:13:55][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/13 13:13:55][INFO] visual_prompt:   56: Total Parameters: 93872656	 Gradient Parameters: 8074768
[06/13 13:13:55][INFO] visual_prompt:   58: tuned percent:8.602
[06/13 13:13:55][INFO] visual_prompt:   44: Device used for model: 0
[06/13 13:13:55][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/13 13:13:55][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/13 13:13:58][INFO] visual_prompt:  158: Number of images: 1281167
[06/13 13:13:58][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 13:13:58][INFO] visual_prompt:   78: Loading validation data...
[06/13 13:13:58][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/13 13:13:58][INFO] visual_prompt:  158: Number of images: 50000
[06/13 13:13:58][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 13:13:58][INFO] visual_prompt:   81: Loading test data...
[06/13 13:13:58][INFO] visual_prompt:   83: ...no test data is constructed
[06/13 13:13:58][INFO] visual_prompt:  111: Constructing models...
[06/13 13:13:58][INFO] visual_prompt:  114: Setting up Evalutator...
[06/13 13:13:58][INFO] visual_prompt:  116: Setting up Trainer...
[06/13 13:13:58][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/13 13:13:58][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/13 13:13:58][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/13 13:13:58][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/13 13:13:58][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/13 13:13:58][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/13 13:13:58][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/13 13:13:58][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/13 13:13:58][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/13 13:13:58][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/13 13:13:58][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/13 13:13:58][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/13 13:13:58][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/13 13:13:58][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/13 13:13:58][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/13 13:13:58][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/13 13:13:58][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/13 13:13:58][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/13 13:13:58][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/13 13:13:58][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/13 13:13:58][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/13 13:13:58][INFO] visual_prompt:  232: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/13 13:13:58][INFO] visual_prompt:  251: Training 1 / 100 epoch, with learning rate 0.0
[06/13 13:14:19][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 13:14:19][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 13:14:22][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 13:14:23][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 13:14:23][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 13:14:24][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 13:14:25][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 13:14:25][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 13:14:27][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 13:14:27][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 13:14:27][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 13:14:29][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 13:14:29][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 13:14:29][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 13:14:31][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 13:14:32][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 13:14:32][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 13:14:34][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 13:14:34][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 13:14:34][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 13:14:36][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 13:14:36][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 13:14:36][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 13:14:38][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 13:14:39][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 13:14:39][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 13:14:41][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 13:14:41][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 13:14:41][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 13:14:43][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 13:14:43][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 13:14:43][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 13:21:27][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/13 13:21:27][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/13 13:21:27][INFO] visual_prompt:   99: Command line arguments: None
[06/13 13:21:27][INFO] visual_prompt:  108: Training with config:
[06/13 13:21:27][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.5,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/13 13:21:31][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/13 13:21:31][INFO] visual_prompt:   56: Total Parameters: 93872656	 Gradient Parameters: 8074768
[06/13 13:21:31][INFO] visual_prompt:   58: tuned percent:8.602
[06/13 13:21:31][INFO] visual_prompt:   44: Device used for model: 0
[06/13 13:21:31][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/13 13:21:31][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/13 13:21:34][INFO] visual_prompt:  158: Number of images: 1281167
[06/13 13:21:34][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 13:21:34][INFO] visual_prompt:   78: Loading validation data...
[06/13 13:21:34][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/13 13:21:34][INFO] visual_prompt:  158: Number of images: 50000
[06/13 13:21:34][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 13:21:34][INFO] visual_prompt:   81: Loading test data...
[06/13 13:21:34][INFO] visual_prompt:   83: ...no test data is constructed
[06/13 13:21:34][INFO] visual_prompt:  111: Constructing models...
[06/13 13:21:34][INFO] visual_prompt:  114: Setting up Evalutator...
[06/13 13:21:34][INFO] visual_prompt:  116: Setting up Trainer...
[06/13 13:21:34][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/13 13:21:34][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/13 13:21:34][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/13 13:21:34][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/13 13:21:34][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/13 13:21:34][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/13 13:21:34][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/13 13:21:34][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/13 13:21:34][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/13 13:21:34][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/13 13:21:34][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/13 13:21:34][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/13 13:21:34][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/13 13:21:34][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/13 13:21:34][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/13 13:21:34][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/13 13:21:34][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/13 13:21:34][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/13 13:21:34][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/13 13:21:34][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/13 13:21:34][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/13 13:21:34][INFO] visual_prompt:  232: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/13 13:21:34][INFO] visual_prompt:  251: Training 1 / 100 epoch, with learning rate 0.0
[06/13 14:27:44][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/13 14:27:44][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/13 14:27:44][INFO] visual_prompt:   99: Command line arguments: None
[06/13 14:27:44][INFO] visual_prompt:  108: Training with config:
[06/13 14:27:44][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.5,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/13 14:27:47][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/13 14:27:47][INFO] visual_prompt:   56: Total Parameters: 93872656	 Gradient Parameters: 8074768
[06/13 14:27:47][INFO] visual_prompt:   58: tuned percent:8.602
[06/13 14:27:48][INFO] visual_prompt:   44: Device used for model: 0
[06/13 14:27:48][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/13 14:27:48][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/13 14:27:51][INFO] visual_prompt:  158: Number of images: 1281167
[06/13 14:27:51][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 14:27:51][INFO] visual_prompt:   78: Loading validation data...
[06/13 14:27:51][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/13 14:27:51][INFO] visual_prompt:  158: Number of images: 50000
[06/13 14:27:51][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 14:27:51][INFO] visual_prompt:   81: Loading test data...
[06/13 14:27:51][INFO] visual_prompt:   83: ...no test data is constructed
[06/13 14:27:51][INFO] visual_prompt:  111: Constructing models...
[06/13 14:27:51][INFO] visual_prompt:  114: Setting up Evalutator...
[06/13 14:27:51][INFO] visual_prompt:  116: Setting up Trainer...
[06/13 14:27:51][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/13 14:27:51][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/13 14:27:51][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/13 14:27:51][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/13 14:27:51][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/13 14:27:51][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/13 14:27:51][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/13 14:27:51][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/13 14:27:51][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/13 14:27:51][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/13 14:27:51][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/13 14:27:51][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/13 14:27:51][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/13 14:27:51][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/13 14:27:51][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/13 14:27:51][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/13 14:27:51][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/13 14:27:51][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/13 14:27:51][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/13 14:27:51][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/13 14:27:51][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/13 14:27:51][INFO] visual_prompt:  232: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/13 14:27:51][INFO] visual_prompt:  251: Training 1 / 100 epoch, with learning rate 0.0
[06/13 14:29:21][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/13 14:29:21][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/13 14:29:21][INFO] visual_prompt:   99: Command line arguments: None
[06/13 14:29:21][INFO] visual_prompt:  108: Training with config:
[06/13 14:29:21][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.5,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/13 14:29:24][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/13 14:29:24][INFO] visual_prompt:   56: Total Parameters: 93872656	 Gradient Parameters: 8074768
[06/13 14:29:24][INFO] visual_prompt:   58: tuned percent:8.602
[06/13 14:29:24][INFO] visual_prompt:   44: Device used for model: 0
[06/13 14:29:24][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/13 14:29:24][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/13 14:29:27][INFO] visual_prompt:  158: Number of images: 1281167
[06/13 14:29:27][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 14:29:27][INFO] visual_prompt:   78: Loading validation data...
[06/13 14:29:27][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/13 14:29:27][INFO] visual_prompt:  158: Number of images: 50000
[06/13 14:29:27][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 14:29:27][INFO] visual_prompt:   81: Loading test data...
[06/13 14:29:27][INFO] visual_prompt:   83: ...no test data is constructed
[06/13 14:29:27][INFO] visual_prompt:  111: Constructing models...
[06/13 14:29:27][INFO] visual_prompt:  114: Setting up Evalutator...
[06/13 14:29:27][INFO] visual_prompt:  116: Setting up Trainer...
[06/13 14:29:27][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/13 14:29:27][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/13 14:29:27][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/13 14:29:27][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/13 14:29:27][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/13 14:29:27][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/13 14:29:27][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/13 14:29:27][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/13 14:29:27][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/13 14:29:27][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/13 14:29:27][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/13 14:29:27][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/13 14:29:27][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/13 14:29:27][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/13 14:29:27][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/13 14:29:27][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/13 14:29:27][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/13 14:29:27][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/13 14:29:27][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/13 14:29:27][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/13 14:29:27][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/13 14:29:27][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/13 14:29:27][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/13 14:32:32][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/13 14:32:32][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/13 14:32:32][INFO] visual_prompt:   99: Command line arguments: None
[06/13 14:32:32][INFO] visual_prompt:  108: Training with config:
[06/13 14:32:32][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.5,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/13 14:32:36][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/13 14:32:36][INFO] visual_prompt:   56: Total Parameters: 93872656	 Gradient Parameters: 8074768
[06/13 14:32:36][INFO] visual_prompt:   58: tuned percent:8.602
[06/13 14:32:36][INFO] visual_prompt:   44: Device used for model: 0
[06/13 14:32:36][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/13 14:32:36][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/13 14:32:39][INFO] visual_prompt:  158: Number of images: 1281167
[06/13 14:32:39][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 14:32:39][INFO] visual_prompt:   78: Loading validation data...
[06/13 14:32:39][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/13 14:32:39][INFO] visual_prompt:  158: Number of images: 50000
[06/13 14:32:39][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 14:32:39][INFO] visual_prompt:   81: Loading test data...
[06/13 14:32:39][INFO] visual_prompt:   83: ...no test data is constructed
[06/13 14:32:39][INFO] visual_prompt:  111: Constructing models...
[06/13 14:32:39][INFO] visual_prompt:  114: Setting up Evalutator...
[06/13 14:32:39][INFO] visual_prompt:  116: Setting up Trainer...
[06/13 14:32:39][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/13 14:32:39][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/13 14:32:39][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/13 14:32:39][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/13 14:32:39][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/13 14:32:39][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/13 14:32:39][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/13 14:32:39][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/13 14:32:39][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/13 14:32:39][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/13 14:32:39][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/13 14:32:39][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/13 14:32:39][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/13 14:32:39][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/13 14:32:39][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/13 14:32:39][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/13 14:32:39][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/13 14:32:39][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/13 14:32:39][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/13 14:32:39][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/13 14:32:39][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/13 14:32:40][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/13 14:32:40][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/13 14:36:11][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/13 14:36:11][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/13 14:36:11][INFO] visual_prompt:   99: Command line arguments: None
[06/13 14:36:11][INFO] visual_prompt:  108: Training with config:
[06/13 14:36:11][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.5,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/13 14:36:14][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/13 14:36:14][INFO] visual_prompt:   56: Total Parameters: 93872656	 Gradient Parameters: 8074768
[06/13 14:36:14][INFO] visual_prompt:   58: tuned percent:8.602
[06/13 14:36:14][INFO] visual_prompt:   44: Device used for model: 0
[06/13 14:36:14][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/13 14:36:14][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/13 14:36:17][INFO] visual_prompt:  158: Number of images: 1281167
[06/13 14:36:17][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 14:36:17][INFO] visual_prompt:   78: Loading validation data...
[06/13 14:36:17][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/13 14:36:17][INFO] visual_prompt:  158: Number of images: 50000
[06/13 14:36:17][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 14:36:17][INFO] visual_prompt:   81: Loading test data...
[06/13 14:36:17][INFO] visual_prompt:   83: ...no test data is constructed
[06/13 14:36:17][INFO] visual_prompt:  111: Constructing models...
[06/13 14:36:17][INFO] visual_prompt:  114: Setting up Evalutator...
[06/13 14:36:17][INFO] visual_prompt:  116: Setting up Trainer...
[06/13 14:36:17][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/13 14:36:17][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/13 14:36:17][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/13 14:36:17][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/13 14:36:17][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/13 14:36:17][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/13 14:36:17][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/13 14:36:17][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/13 14:36:17][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/13 14:36:17][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/13 14:36:17][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/13 14:36:17][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/13 14:36:17][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/13 14:36:17][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/13 14:36:17][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/13 14:36:17][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/13 14:36:17][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/13 14:36:17][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/13 14:36:17][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/13 14:36:17][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/13 14:36:17][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/13 14:36:17][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/13 14:36:17][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/13 14:36:37][INFO] visual_prompt:   99: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:36:37][INFO] visual_prompt:  100: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:36:40][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:36:41][INFO] visual_prompt:   99: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:36:41][INFO] visual_prompt:  100: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:36:43][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:36:43][INFO] visual_prompt:   99: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:36:43][INFO] visual_prompt:  100: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:36:45][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:36:45][INFO] visual_prompt:   99: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:36:45][INFO] visual_prompt:  100: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:36:47][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:36:48][INFO] visual_prompt:   99: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:36:48][INFO] visual_prompt:  100: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:36:50][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:36:50][INFO] visual_prompt:   99: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:36:50][INFO] visual_prompt:  100: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:36:52][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:36:52][INFO] visual_prompt:   99: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:36:52][INFO] visual_prompt:  100: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:36:54][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:36:55][INFO] visual_prompt:   99: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:36:55][INFO] visual_prompt:  100: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:36:57][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:36:57][INFO] visual_prompt:   99: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:36:57][INFO] visual_prompt:  100: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:36:59][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:36:59][INFO] visual_prompt:   99: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:36:59][INFO] visual_prompt:  100: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:37:01][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:37:02][INFO] visual_prompt:   99: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:37:02][INFO] visual_prompt:  100: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:37:04][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:37:04][INFO] visual_prompt:   99: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:37:04][INFO] visual_prompt:  100: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:37:06][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:37:06][INFO] visual_prompt:   99: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:37:06][INFO] visual_prompt:  100: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:37:08][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:37:09][INFO] visual_prompt:   99: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:37:09][INFO] visual_prompt:  100: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:37:11][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:37:11][INFO] visual_prompt:   99: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:37:11][INFO] visual_prompt:  100: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:37:13][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:37:13][INFO] visual_prompt:   99: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:37:13][INFO] visual_prompt:  100: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:37:15][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:37:16][INFO] visual_prompt:   99: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:37:16][INFO] visual_prompt:  100: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:37:18][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:37:18][INFO] visual_prompt:   99: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:37:18][INFO] visual_prompt:  100: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:37:20][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:37:20][INFO] visual_prompt:   99: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:37:20][INFO] visual_prompt:  100: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:37:22][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:37:23][INFO] visual_prompt:   99: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:37:23][INFO] visual_prompt:  100: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:37:25][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:37:26][INFO] visual_prompt:  314: Epoch 1 / 100: avg data time: 9.83e-01, avg batch time: 3.3812, average train loss: 6.9358
[06/13 14:37:39][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/13 14:37:39][INFO] visual_prompt:   99: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:37:39][INFO] visual_prompt:  100: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:44:26][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/13 14:44:26][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/13 14:44:26][INFO] visual_prompt:   99: Command line arguments: None
[06/13 14:44:26][INFO] visual_prompt:  108: Training with config:
[06/13 14:44:26][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.5,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/13 14:44:29][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/13 14:44:29][INFO] visual_prompt:   56: Total Parameters: 93872656	 Gradient Parameters: 8074768
[06/13 14:44:29][INFO] visual_prompt:   58: tuned percent:8.602
[06/13 14:44:30][INFO] visual_prompt:   44: Device used for model: 0
[06/13 14:44:30][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/13 14:44:30][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/13 14:44:33][INFO] visual_prompt:  158: Number of images: 1281167
[06/13 14:44:33][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 14:44:33][INFO] visual_prompt:   78: Loading validation data...
[06/13 14:44:33][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/13 14:44:33][INFO] visual_prompt:  158: Number of images: 50000
[06/13 14:44:33][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 14:44:33][INFO] visual_prompt:   81: Loading test data...
[06/13 14:44:33][INFO] visual_prompt:   83: ...no test data is constructed
[06/13 14:44:33][INFO] visual_prompt:  111: Constructing models...
[06/13 14:44:33][INFO] visual_prompt:  114: Setting up Evalutator...
[06/13 14:44:33][INFO] visual_prompt:  116: Setting up Trainer...
[06/13 14:44:33][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/13 14:44:33][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/13 14:44:33][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/13 14:44:33][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/13 14:44:33][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/13 14:44:33][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/13 14:44:33][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/13 14:44:33][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/13 14:44:33][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/13 14:44:33][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/13 14:44:33][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/13 14:44:33][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/13 14:44:33][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/13 14:44:33][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/13 14:44:33][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/13 14:44:33][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/13 14:44:33][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/13 14:44:33][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/13 14:44:33][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/13 14:44:33][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/13 14:44:33][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/13 14:44:33][INFO] visual_prompt:  232: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/13 14:44:33][INFO] visual_prompt:  251: Training 1 / 100 epoch, with learning rate 0.0
[06/13 14:46:03][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/13 14:46:03][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/13 14:46:03][INFO] visual_prompt:   99: Command line arguments: None
[06/13 14:46:03][INFO] visual_prompt:  108: Training with config:
[06/13 14:46:03][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.5,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/13 14:46:06][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/13 14:46:06][INFO] visual_prompt:   56: Total Parameters: 93872656	 Gradient Parameters: 8074768
[06/13 14:46:06][INFO] visual_prompt:   58: tuned percent:8.602
[06/13 14:46:07][INFO] visual_prompt:   44: Device used for model: 0
[06/13 14:46:07][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/13 14:46:07][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/13 14:46:09][INFO] visual_prompt:  158: Number of images: 1281167
[06/13 14:46:09][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 14:46:09][INFO] visual_prompt:   78: Loading validation data...
[06/13 14:46:09][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/13 14:46:09][INFO] visual_prompt:  158: Number of images: 50000
[06/13 14:46:09][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 14:46:09][INFO] visual_prompt:   81: Loading test data...
[06/13 14:46:09][INFO] visual_prompt:   83: ...no test data is constructed
[06/13 14:46:09][INFO] visual_prompt:  111: Constructing models...
[06/13 14:46:09][INFO] visual_prompt:  114: Setting up Evalutator...
[06/13 14:46:09][INFO] visual_prompt:  116: Setting up Trainer...
[06/13 14:46:09][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/13 14:46:10][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/13 14:46:10][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/13 14:46:10][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/13 14:46:10][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/13 14:46:10][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/13 14:46:10][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/13 14:46:10][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/13 14:46:10][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/13 14:46:10][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/13 14:46:10][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/13 14:46:10][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/13 14:46:10][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/13 14:46:10][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/13 14:46:10][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/13 14:46:10][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/13 14:46:10][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/13 14:46:10][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/13 14:46:10][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/13 14:46:10][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/13 14:46:10][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/13 14:46:10][INFO] visual_prompt:  232: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/13 14:46:10][INFO] visual_prompt:  251: Training 1 / 100 epoch, with learning rate 0.0
[06/13 14:46:30][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:46:30][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:46:33][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:46:34][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:46:34][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:46:35][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:46:36][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:46:36][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:46:38][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:46:38][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:46:38][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:46:40][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:46:40][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:46:40][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:46:42][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:46:43][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:46:43][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:46:45][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:46:45][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:46:45][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:46:47][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:46:48][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:46:48][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:46:50][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:46:50][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:46:50][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:46:52][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:46:52][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:46:52][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:46:54][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:46:55][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:46:55][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:46:56][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:46:57][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:46:57][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:46:59][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:46:59][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:46:59][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:47:01][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:47:02][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:47:02][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:47:04][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:47:04][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:47:04][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:47:06][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:47:07][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:47:07][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:47:09][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:47:09][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:47:09][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:47:11][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:47:11][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:47:11][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:47:13][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:47:14][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:47:14][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:47:16][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:47:16][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:47:16][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:47:18][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:47:20][INFO] visual_prompt:  313: Epoch 1 / 100: avg data time: 1.01e+00, avg batch time: 3.4470, average train loss: 6.9463
[06/13 14:48:31][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/13 14:48:31][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/13 14:48:31][INFO] visual_prompt:   99: Command line arguments: None
[06/13 14:48:31][INFO] visual_prompt:  108: Training with config:
[06/13 14:48:31][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.5,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/13 14:48:34][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/13 14:48:34][INFO] visual_prompt:   56: Total Parameters: 93872656	 Gradient Parameters: 8074768
[06/13 14:48:34][INFO] visual_prompt:   58: tuned percent:8.602
[06/13 14:48:35][INFO] visual_prompt:   44: Device used for model: 0
[06/13 14:48:35][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/13 14:48:35][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/13 14:48:37][INFO] visual_prompt:  158: Number of images: 1281167
[06/13 14:48:37][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 14:48:37][INFO] visual_prompt:   78: Loading validation data...
[06/13 14:48:37][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/13 14:48:37][INFO] visual_prompt:  158: Number of images: 50000
[06/13 14:48:37][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 14:48:37][INFO] visual_prompt:   81: Loading test data...
[06/13 14:48:37][INFO] visual_prompt:   83: ...no test data is constructed
[06/13 14:48:37][INFO] visual_prompt:  111: Constructing models...
[06/13 14:48:37][INFO] visual_prompt:  114: Setting up Evalutator...
[06/13 14:48:37][INFO] visual_prompt:  116: Setting up Trainer...
[06/13 14:48:37][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/13 14:48:37][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/13 14:48:37][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/13 14:48:37][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/13 14:48:37][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/13 14:48:37][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/13 14:48:37][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/13 14:48:37][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/13 14:48:37][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/13 14:48:37][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/13 14:48:37][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/13 14:48:37][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/13 14:48:37][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/13 14:48:37][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/13 14:48:37][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/13 14:48:37][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/13 14:48:37][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/13 14:48:37][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/13 14:48:37][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/13 14:48:37][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/13 14:48:37][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/13 14:48:37][INFO] visual_prompt:  232: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/13 14:48:37][INFO] visual_prompt:  251: Training 1 / 100 epoch, with learning rate 0.0
[06/13 14:48:57][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:48:57][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:49:00][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:49:01][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:49:01][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:49:03][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:49:03][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:49:03][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:49:05][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:49:05][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:49:05][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:49:07][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:49:08][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:49:08][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:49:10][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:49:10][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:49:10][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:49:12][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:49:13][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:49:13][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:49:15][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:49:15][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:49:15][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:49:17][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:49:17][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:49:17][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:49:19][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:49:20][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:49:20][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:49:22][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:49:22][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:49:22][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:49:24][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:49:24][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:49:24][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:49:26][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:49:27][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:49:27][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:49:28][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:49:29][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:49:29][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:49:31][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:49:31][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:49:31][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:49:33][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:49:34][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:49:34][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:49:35][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:49:36][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:49:36][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:49:38][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:49:38][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:49:38][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:49:40][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:49:40][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:49:40][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:49:42][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:49:43][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:49:43][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:49:45][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:49:46][INFO] visual_prompt:  313: Epoch 1 / 100: avg data time: 9.66e-01, avg batch time: 3.3745, average train loss: 6.9268
[06/13 14:50:01][INFO] visual_prompt:  411: during eval: torch.Size([64, 3, 224, 224])
[06/13 14:50:01][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/13 14:50:01][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/13 14:51:19][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/13 14:51:19][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/13 14:51:19][INFO] visual_prompt:   99: Command line arguments: None
[06/13 14:51:19][INFO] visual_prompt:  108: Training with config:
[06/13 14:51:19][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.5,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/13 14:51:23][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/13 14:51:23][INFO] visual_prompt:   56: Total Parameters: 93872656	 Gradient Parameters: 8074768
[06/13 14:51:23][INFO] visual_prompt:   58: tuned percent:8.602
[06/13 14:51:23][INFO] visual_prompt:   44: Device used for model: 0
[06/13 14:51:23][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/13 14:51:23][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/13 14:51:26][INFO] visual_prompt:  158: Number of images: 1281167
[06/13 14:51:26][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 14:51:26][INFO] visual_prompt:   78: Loading validation data...
[06/13 14:51:26][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/13 14:51:26][INFO] visual_prompt:  158: Number of images: 50000
[06/13 14:51:26][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 14:51:26][INFO] visual_prompt:   81: Loading test data...
[06/13 14:51:26][INFO] visual_prompt:   83: ...no test data is constructed
[06/13 14:51:26][INFO] visual_prompt:  111: Constructing models...
[06/13 14:51:26][INFO] visual_prompt:  114: Setting up Evalutator...
[06/13 14:51:26][INFO] visual_prompt:  116: Setting up Trainer...
[06/13 14:51:26][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/13 14:51:26][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/13 14:51:26][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/13 14:51:26][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/13 14:51:26][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/13 14:51:26][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/13 14:51:26][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/13 14:51:26][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/13 14:51:26][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/13 14:51:26][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/13 14:51:26][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/13 14:51:26][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/13 14:51:26][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/13 14:51:26][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/13 14:51:26][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/13 14:51:26][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/13 14:51:26][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/13 14:51:26][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/13 14:51:26][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/13 14:51:26][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/13 14:51:26][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/13 14:51:26][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/13 14:51:26][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/13 14:51:45][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:51:45][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:51:49][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:51:50][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:51:50][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:51:52][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:51:52][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:51:52][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:51:54][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:51:55][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:51:55][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:51:57][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:51:57][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:51:57][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:51:59][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:51:59][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:51:59][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:52:01][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:52:02][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:52:02][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:52:03][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:52:04][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:52:04][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:52:06][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:52:06][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:52:06][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:52:08][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:52:09][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:52:09][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:52:11][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:52:11][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:52:11][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:52:13][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:52:13][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:52:13][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:52:15][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:52:16][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:52:16][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:52:17][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:52:18][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:52:18][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:52:20][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:52:20][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:52:20][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:52:22][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:52:23][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:52:23][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:52:24][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:52:25][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:52:25][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:52:27][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:52:27][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:52:27][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:52:29][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:52:30][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:52:30][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:52:31][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:52:32][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 14:52:32][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 14:52:34][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/13 14:52:36][INFO] visual_prompt:  314: Epoch 1 / 100: avg data time: 9.65e-01, avg batch time: 3.4131, average train loss: 6.9256
[06/13 14:52:52][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/13 14:52:52][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/13 14:52:52][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/13 14:52:59][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/13 14:52:59][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/13 14:52:59][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/13 14:52:59][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/13 14:53:04][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/13 14:53:04][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/13 14:53:04][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/13 14:53:04][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/13 14:53:09][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/13 14:53:09][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/13 14:53:09][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/13 14:53:09][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/13 14:53:14][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/13 14:53:14][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/13 14:53:14][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/13 14:53:14][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/13 14:53:20][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/13 14:53:20][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/13 14:53:20][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/13 14:53:20][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/13 14:53:25][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/13 14:53:25][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/13 14:53:25][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/13 14:53:25][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/13 14:53:30][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/13 14:53:30][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/13 14:53:30][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/13 14:53:30][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/13 14:53:35][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/13 14:53:35][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/13 14:53:35][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/13 14:53:35][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/13 14:53:40][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/13 14:53:40][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/13 14:53:40][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/13 14:53:40][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/13 14:53:46][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/13 14:53:46][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/13 14:53:46][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/13 14:53:46][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/13 14:53:51][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/13 14:53:51][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/13 14:53:51][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/13 14:53:51][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/13 14:53:56][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/13 14:53:56][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/13 14:53:56][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/13 14:53:56][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/13 14:54:01][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/13 14:54:01][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/13 14:54:01][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/13 14:54:01][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/13 14:54:07][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/13 14:54:07][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/13 14:54:07][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/13 14:54:07][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/13 14:54:12][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/13 14:54:12][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/13 14:54:12][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/13 14:54:12][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/13 14:54:17][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/13 14:54:17][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/13 14:54:17][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/13 14:54:17][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/13 14:54:22][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/13 14:54:22][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/13 14:54:22][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/13 14:54:22][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/13 14:54:27][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/13 14:54:27][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/13 14:54:27][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/13 14:54:27][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/13 14:54:33][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/13 14:54:33][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/13 14:54:33][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/13 14:54:33][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/13 14:54:38][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/13 14:54:41][INFO] visual_prompt:  471: Inference (val):avg data time: 1.98e-04, avg batch time: 5.2912, average loss: 69.3117
[06/13 14:54:41][INFO] visual_prompt:  488: Saved invariances for val_imagenet at output_ce_10/val_imagenet_invariances.json
[06/13 14:54:41][INFO] visual_prompt:  252: Training 2 / 100 epoch, with learning rate 0.05
[06/13 14:55:52][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/13 14:55:52][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/13 14:55:52][INFO] visual_prompt:   99: Command line arguments: None
[06/13 14:55:52][INFO] visual_prompt:  108: Training with config:
[06/13 14:55:52][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': False,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.5,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/13 14:55:55][INFO] visual_prompt:   56: Total Parameters: 93872656	 Gradient Parameters: 8074768
[06/13 14:55:55][INFO] visual_prompt:   58: tuned percent:8.602
[06/13 14:55:55][INFO] visual_prompt:   44: Device used for model: 0
[06/13 14:55:55][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/13 14:55:55][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/13 14:55:58][INFO] visual_prompt:  158: Number of images: 1281167
[06/13 14:55:58][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 14:55:58][INFO] visual_prompt:   78: Loading validation data...
[06/13 14:55:58][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/13 14:55:58][INFO] visual_prompt:  158: Number of images: 50000
[06/13 14:55:58][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 14:55:58][INFO] visual_prompt:   81: Loading test data...
[06/13 14:55:58][INFO] visual_prompt:   83: ...no test data is constructed
[06/13 14:55:58][INFO] visual_prompt:  111: Constructing models...
[06/13 14:55:58][INFO] visual_prompt:  114: Setting up Evalutator...
[06/13 14:55:58][INFO] visual_prompt:  116: Setting up Trainer...
[06/13 14:55:58][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/13 14:55:58][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/13 14:55:58][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/13 14:55:58][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/13 14:55:58][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/13 14:55:58][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/13 14:55:58][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/13 14:55:58][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/13 14:55:58][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/13 14:55:58][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/13 14:55:58][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/13 14:55:58][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/13 14:55:58][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/13 14:55:58][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/13 14:55:58][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/13 14:55:58][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/13 14:55:58][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/13 14:55:58][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/13 14:55:58][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/13 14:55:58][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/13 14:55:58][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/13 14:55:58][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/13 14:55:58][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/13 14:57:47][INFO] visual_prompt:  301: 	Training 100/5004. train loss: 6.9260,	0.8816 s / batch. (data: 2.83e-04). ETA=5 days, 2:31:20, max mem: 15.0 GB 
[06/13 14:59:15][INFO] visual_prompt:  301: 	Training 200/5004. train loss: 6.9504,	0.8724 s / batch. (data: 2.92e-04). ETA=5 days, 1:13:01, max mem: 15.0 GB 
[06/13 15:00:42][INFO] visual_prompt:  301: 	Training 300/5004. train loss: 6.9501,	0.8757 s / batch. (data: 3.29e-04). ETA=5 days, 1:39:07, max mem: 15.0 GB 
[06/13 15:02:10][INFO] visual_prompt:  301: 	Training 400/5004. train loss: 6.9658,	0.8784 s / batch. (data: 3.18e-04). ETA=5 days, 2:00:09, max mem: 15.0 GB 
[06/13 15:03:37][INFO] visual_prompt:  301: 	Training 500/5004. train loss: 6.9522,	0.8763 s / batch. (data: 3.31e-04). ETA=5 days, 1:40:58, max mem: 15.0 GB 
[06/13 15:05:05][INFO] visual_prompt:  301: 	Training 600/5004. train loss: 6.9466,	0.8781 s / batch. (data: 3.37e-04). ETA=5 days, 1:54:33, max mem: 15.0 GB 
[06/13 15:06:33][INFO] visual_prompt:  301: 	Training 700/5004. train loss: 6.9139,	0.8791 s / batch. (data: 3.71e-04). ETA=5 days, 2:01:19, max mem: 15.0 GB 
[06/13 15:08:00][INFO] visual_prompt:  301: 	Training 800/5004. train loss: 6.9359,	0.8725 s / batch. (data: 3.37e-04). ETA=5 days, 1:04:38, max mem: 15.0 GB 
[06/13 15:09:28][INFO] visual_prompt:  301: 	Training 900/5004. train loss: 6.9625,	0.8839 s / batch. (data: 2.97e-04). ETA=5 days, 2:38:51, max mem: 15.0 GB 
[06/13 15:10:55][INFO] visual_prompt:  301: 	Training 1000/5004. train loss: 6.9014,	0.8780 s / batch. (data: 2.31e-04). ETA=5 days, 1:48:03, max mem: 15.0 GB 
[06/13 15:12:23][INFO] visual_prompt:  301: 	Training 1100/5004. train loss: 6.9345,	0.8796 s / batch. (data: 3.31e-04). ETA=5 days, 1:59:40, max mem: 15.0 GB 
[06/13 15:13:50][INFO] visual_prompt:  301: 	Training 1200/5004. train loss: 6.9237,	0.8788 s / batch. (data: 3.20e-04). ETA=5 days, 1:51:33, max mem: 15.0 GB 
[06/13 15:15:18][INFO] visual_prompt:  301: 	Training 1300/5004. train loss: 6.9543,	0.8740 s / batch. (data: 2.78e-04). ETA=5 days, 1:09:50, max mem: 15.0 GB 
[06/13 15:16:46][INFO] visual_prompt:  301: 	Training 1400/5004. train loss: 6.9328,	0.8810 s / batch. (data: 3.38e-04). ETA=5 days, 2:06:49, max mem: 15.0 GB 
[06/13 15:18:13][INFO] visual_prompt:  301: 	Training 1500/5004. train loss: 6.9137,	0.8752 s / batch. (data: 3.15e-04). ETA=5 days, 1:17:16, max mem: 15.0 GB 
[06/13 15:19:41][INFO] visual_prompt:  301: 	Training 1600/5004. train loss: 6.9244,	0.8756 s / batch. (data: 2.88e-04). ETA=5 days, 1:19:33, max mem: 15.0 GB 
[06/13 15:21:09][INFO] visual_prompt:  301: 	Training 1700/5004. train loss: 6.9589,	0.8788 s / batch. (data: 3.30e-04). ETA=5 days, 1:44:23, max mem: 15.0 GB 
[06/13 15:22:36][INFO] visual_prompt:  301: 	Training 1800/5004. train loss: 6.9424,	0.8793 s / batch. (data: 3.85e-04). ETA=5 days, 1:46:54, max mem: 15.0 GB 
[06/13 15:24:04][INFO] visual_prompt:  301: 	Training 1900/5004. train loss: 6.9175,	0.8774 s / batch. (data: 3.03e-04). ETA=5 days, 1:29:22, max mem: 15.0 GB 
[06/13 15:25:31][INFO] visual_prompt:  301: 	Training 2000/5004. train loss: 6.9472,	0.8744 s / batch. (data: 3.24e-04). ETA=5 days, 1:03:23, max mem: 15.0 GB 
[06/13 15:26:59][INFO] visual_prompt:  301: 	Training 2100/5004. train loss: 6.9474,	0.8755 s / batch. (data: 4.31e-04). ETA=5 days, 1:11:07, max mem: 15.0 GB 
[06/13 15:28:26][INFO] visual_prompt:  301: 	Training 2200/5004. train loss: 6.9385,	0.8694 s / batch. (data: 7.91e-04). ETA=5 days, 0:19:13, max mem: 15.0 GB 
[06/13 15:29:54][INFO] visual_prompt:  301: 	Training 2300/5004. train loss: 6.9465,	0.8765 s / batch. (data: 3.91e-04). ETA=5 days, 1:16:21, max mem: 15.0 GB 
[06/13 15:31:22][INFO] visual_prompt:  301: 	Training 2400/5004. train loss: 6.9239,	0.8740 s / batch. (data: 3.12e-04). ETA=5 days, 0:53:56, max mem: 15.0 GB 
[06/13 15:32:49][INFO] visual_prompt:  301: 	Training 2500/5004. train loss: 6.9405,	0.8757 s / batch. (data: 3.38e-04). ETA=5 days, 1:07:10, max mem: 15.0 GB 
[06/13 15:34:17][INFO] visual_prompt:  301: 	Training 2600/5004. train loss: 6.9121,	0.8773 s / batch. (data: 3.33e-04). ETA=5 days, 1:18:42, max mem: 15.0 GB 
[06/13 15:35:44][INFO] visual_prompt:  301: 	Training 2700/5004. train loss: 6.9323,	0.8774 s / batch. (data: 3.07e-04). ETA=5 days, 1:17:41, max mem: 15.0 GB 
[06/13 15:37:12][INFO] visual_prompt:  301: 	Training 2800/5004. train loss: 6.9398,	0.8775 s / batch. (data: 3.82e-04). ETA=5 days, 1:17:45, max mem: 15.0 GB 
[06/13 15:38:40][INFO] visual_prompt:  301: 	Training 2900/5004. train loss: 6.9724,	0.8790 s / batch. (data: 3.21e-04). ETA=5 days, 1:28:19, max mem: 15.0 GB 
[06/13 15:40:07][INFO] visual_prompt:  301: 	Training 3000/5004. train loss: 6.9611,	0.8785 s / batch. (data: 3.54e-04). ETA=5 days, 1:23:10, max mem: 15.0 GB 
[06/13 15:41:35][INFO] visual_prompt:  301: 	Training 3100/5004. train loss: 6.9679,	0.8782 s / batch. (data: 3.32e-04). ETA=5 days, 1:19:11, max mem: 15.0 GB 
[06/13 15:43:03][INFO] visual_prompt:  301: 	Training 3200/5004. train loss: 6.9074,	0.8724 s / batch. (data: 3.61e-04). ETA=5 days, 0:29:31, max mem: 15.0 GB 
[06/13 15:44:30][INFO] visual_prompt:  301: 	Training 3300/5004. train loss: 6.9618,	0.8768 s / batch. (data: 3.03e-04). ETA=5 days, 1:04:26, max mem: 15.0 GB 
[06/13 15:45:58][INFO] visual_prompt:  301: 	Training 3400/5004. train loss: 6.9448,	0.8703 s / batch. (data: 3.34e-04). ETA=5 days, 0:08:55, max mem: 15.0 GB 
[06/13 15:47:25][INFO] visual_prompt:  301: 	Training 3500/5004. train loss: 6.9160,	0.8769 s / batch. (data: 3.29e-04). ETA=5 days, 1:02:01, max mem: 15.0 GB 
[06/13 15:48:53][INFO] visual_prompt:  301: 	Training 3600/5004. train loss: 6.8974,	0.8756 s / batch. (data: 3.43e-04). ETA=5 days, 0:49:46, max mem: 15.0 GB 
[06/13 15:50:21][INFO] visual_prompt:  301: 	Training 3700/5004. train loss: 6.9212,	0.8779 s / batch. (data: 2.94e-04). ETA=5 days, 1:07:46, max mem: 15.0 GB 
[06/13 15:51:48][INFO] visual_prompt:  301: 	Training 3800/5004. train loss: 6.9344,	0.8774 s / batch. (data: 3.68e-04). ETA=5 days, 1:01:33, max mem: 15.0 GB 
[06/13 15:53:16][INFO] visual_prompt:  301: 	Training 3900/5004. train loss: 6.8839,	0.8735 s / batch. (data: 3.91e-04). ETA=5 days, 0:28:15, max mem: 15.0 GB 
[06/13 15:54:44][INFO] visual_prompt:  301: 	Training 4000/5004. train loss: 6.9229,	0.8741 s / batch. (data: 3.89e-04). ETA=5 days, 0:31:57, max mem: 15.0 GB 
[06/13 15:56:11][INFO] visual_prompt:  301: 	Training 4100/5004. train loss: 6.9527,	0.8769 s / batch. (data: 3.73e-04). ETA=5 days, 0:53:22, max mem: 15.0 GB 
[06/13 15:57:39][INFO] visual_prompt:  301: 	Training 4200/5004. train loss: 6.8864,	0.8718 s / batch. (data: 2.75e-04). ETA=5 days, 0:09:44, max mem: 15.0 GB 
[06/13 15:59:06][INFO] visual_prompt:  301: 	Training 4300/5004. train loss: 6.9257,	0.8755 s / batch. (data: 2.84e-04). ETA=5 days, 0:38:33, max mem: 15.0 GB 
[06/13 16:00:34][INFO] visual_prompt:  301: 	Training 4400/5004. train loss: 6.9129,	0.8791 s / batch. (data: 3.60e-04). ETA=5 days, 1:06:59, max mem: 15.0 GB 
[06/13 16:02:02][INFO] visual_prompt:  301: 	Training 4500/5004. train loss: 6.9939,	0.8793 s / batch. (data: 3.30e-04). ETA=5 days, 1:07:18, max mem: 15.0 GB 
[06/13 16:03:29][INFO] visual_prompt:  301: 	Training 4600/5004. train loss: 6.9454,	0.8744 s / batch. (data: 2.95e-04). ETA=5 days, 0:25:27, max mem: 15.0 GB 
[06/13 16:04:57][INFO] visual_prompt:  301: 	Training 4700/5004. train loss: 6.9394,	0.8786 s / batch. (data: 3.24e-04). ETA=5 days, 0:58:35, max mem: 15.0 GB 
[06/13 16:06:24][INFO] visual_prompt:  301: 	Training 4800/5004. train loss: 6.9728,	0.8743 s / batch. (data: 3.09e-04). ETA=5 days, 0:21:21, max mem: 15.0 GB 
[06/13 16:07:52][INFO] visual_prompt:  301: 	Training 4900/5004. train loss: 6.9011,	0.8802 s / batch. (data: 3.49e-04). ETA=5 days, 1:09:02, max mem: 15.0 GB 
[06/13 16:09:20][INFO] visual_prompt:  301: 	Training 5000/5004. train loss: 6.9209,	0.8763 s / batch. (data: 1.45e-04). ETA=5 days, 0:35:20, max mem: 15.0 GB 
[06/13 16:09:24][INFO] visual_prompt:  314: Epoch 1 / 100: avg data time: 4.24e-03, avg batch time: 0.8803, average train loss: 6.9281
[06/13 16:18:25][INFO] visual_prompt:  434: 	Test 100/196. loss: 69.306, 5.1977 s / batch. (data: 1.13e-04)max mem: 15.00354 GB 
[06/13 16:26:44][INFO] visual_prompt:  471: Inference (val):avg data time: 1.86e-04, avg batch time: 5.2083, average loss: 69.2943
[06/13 16:26:44][INFO] visual_prompt:  488: Saved invariances for val_imagenet at output_ce_10/val_imagenet_invariances.json
[06/13 16:26:44][INFO] visual_prompt:  252: Training 2 / 100 epoch, with learning rate 0.05
[06/13 16:28:37][INFO] visual_prompt:  301: 	Training 100/5004. train loss: 6.8879,	0.8753 s / batch. (data: 3.35e-04). ETA=5 days, 0:25:48, max mem: 15.0 GB 
[06/13 16:30:05][INFO] visual_prompt:  301: 	Training 200/5004. train loss: 6.8682,	0.8793 s / batch. (data: 2.83e-04). ETA=5 days, 0:57:00, max mem: 15.0 GB 
[06/13 16:31:33][INFO] visual_prompt:  301: 	Training 300/5004. train loss: 6.8398,	0.8745 s / batch. (data: 2.95e-04). ETA=5 days, 0:15:51, max mem: 15.0 GB 
[06/13 16:33:01][INFO] visual_prompt:  301: 	Training 400/5004. train loss: 6.7667,	0.8751 s / batch. (data: 3.71e-04). ETA=5 days, 0:19:29, max mem: 15.0 GB 
[06/13 16:34:29][INFO] visual_prompt:  301: 	Training 500/5004. train loss: 6.7738,	0.8766 s / batch. (data: 3.32e-04). ETA=5 days, 0:30:41, max mem: 15.0 GB 
[06/13 16:35:56][INFO] visual_prompt:  301: 	Training 600/5004. train loss: 6.6928,	0.8772 s / batch. (data: 3.16e-04). ETA=5 days, 0:33:35, max mem: 15.0 GB 
[06/13 16:37:24][INFO] visual_prompt:  301: 	Training 700/5004. train loss: 6.6692,	0.8847 s / batch. (data: 3.50e-04). ETA=5 days, 1:33:53, max mem: 15.0 GB 
[06/13 16:38:52][INFO] visual_prompt:  301: 	Training 800/5004. train loss: 6.6450,	0.8858 s / batch. (data: 4.04e-04). ETA=5 days, 1:42:07, max mem: 15.0 GB 
[06/13 16:40:20][INFO] visual_prompt:  301: 	Training 900/5004. train loss: 6.6263,	0.8775 s / batch. (data: 3.24e-04). ETA=5 days, 0:31:46, max mem: 15.0 GB 
[06/13 16:41:48][INFO] visual_prompt:  301: 	Training 1000/5004. train loss: 6.5567,	0.8808 s / batch. (data: 3.69e-04). ETA=5 days, 0:57:50, max mem: 15.0 GB 
[06/13 16:43:15][INFO] visual_prompt:  301: 	Training 1100/5004. train loss: 6.6187,	0.8789 s / batch. (data: 2.98e-04). ETA=5 days, 0:40:25, max mem: 15.0 GB 
[06/13 16:44:43][INFO] visual_prompt:  301: 	Training 1200/5004. train loss: 6.5236,	0.8773 s / batch. (data: 3.44e-04). ETA=5 days, 0:25:37, max mem: 15.0 GB 
[06/13 16:46:11][INFO] visual_prompt:  301: 	Training 1300/5004. train loss: 6.5103,	0.8778 s / batch. (data: 3.29e-04). ETA=5 days, 0:28:43, max mem: 15.0 GB 
[06/13 16:47:39][INFO] visual_prompt:  301: 	Training 1400/5004. train loss: 6.5003,	0.8786 s / batch. (data: 3.13e-04). ETA=5 days, 0:33:34, max mem: 15.0 GB 
[06/13 16:49:07][INFO] visual_prompt:  301: 	Training 1500/5004. train loss: 6.4929,	0.8782 s / batch. (data: 2.85e-04). ETA=5 days, 0:29:05, max mem: 15.0 GB 
[06/13 16:50:34][INFO] visual_prompt:  301: 	Training 1600/5004. train loss: 6.4472,	0.8771 s / batch. (data: 2.55e-04). ETA=5 days, 0:18:04, max mem: 15.0 GB 
[06/13 16:52:02][INFO] visual_prompt:  301: 	Training 1700/5004. train loss: 6.5155,	0.8766 s / batch. (data: 3.24e-04). ETA=5 days, 0:12:50, max mem: 15.0 GB 
[06/13 16:53:30][INFO] visual_prompt:  301: 	Training 1800/5004. train loss: 6.4859,	0.8790 s / batch. (data: 3.21e-04). ETA=5 days, 0:31:35, max mem: 15.0 GB 
[06/13 16:54:57][INFO] visual_prompt:  301: 	Training 1900/5004. train loss: 6.4372,	0.8778 s / batch. (data: 2.52e-04). ETA=5 days, 0:20:14, max mem: 15.0 GB 
[06/13 16:56:25][INFO] visual_prompt:  301: 	Training 2000/5004. train loss: 6.4674,	0.8756 s / batch. (data: 3.58e-04). ETA=4 days, 23:59:54, max mem: 15.0 GB 
[06/13 16:57:53][INFO] visual_prompt:  301: 	Training 2100/5004. train loss: 6.3617,	0.8762 s / batch. (data: 3.06e-04). ETA=5 days, 0:03:59, max mem: 15.0 GB 
[06/13 16:59:21][INFO] visual_prompt:  301: 	Training 2200/5004. train loss: 6.3058,	0.8777 s / batch. (data: 3.27e-04). ETA=5 days, 0:14:14, max mem: 15.0 GB 
[06/13 17:00:48][INFO] visual_prompt:  301: 	Training 2300/5004. train loss: 6.3070,	0.8799 s / batch. (data: 3.45e-04). ETA=5 days, 0:31:31, max mem: 15.0 GB 
[06/13 17:02:16][INFO] visual_prompt:  301: 	Training 2400/5004. train loss: 6.3368,	0.8732 s / batch. (data: 3.60e-04). ETA=4 days, 23:34:45, max mem: 15.0 GB 
[06/13 17:03:44][INFO] visual_prompt:  301: 	Training 2500/5004. train loss: 6.3515,	0.8773 s / batch. (data: 3.00e-04). ETA=5 days, 0:06:45, max mem: 15.0 GB 
[06/13 17:05:11][INFO] visual_prompt:  301: 	Training 2600/5004. train loss: 6.3367,	0.8758 s / batch. (data: 3.45e-04). ETA=4 days, 23:53:09, max mem: 15.0 GB 
[06/13 17:06:39][INFO] visual_prompt:  301: 	Training 2700/5004. train loss: 6.2577,	0.8833 s / batch. (data: 3.35e-04). ETA=5 days, 0:53:18, max mem: 15.0 GB 
[06/13 17:08:07][INFO] visual_prompt:  301: 	Training 2800/5004. train loss: 6.3118,	0.8792 s / batch. (data: 2.98e-04). ETA=5 days, 0:18:08, max mem: 15.0 GB 
[06/13 17:09:34][INFO] visual_prompt:  301: 	Training 2900/5004. train loss: 6.1839,	0.8779 s / batch. (data: 3.04e-04). ETA=5 days, 0:05:51, max mem: 15.0 GB 
[06/13 17:11:02][INFO] visual_prompt:  301: 	Training 3000/5004. train loss: 6.3098,	0.8749 s / batch. (data: 3.10e-04). ETA=4 days, 23:39:36, max mem: 15.0 GB 
[06/13 17:12:30][INFO] visual_prompt:  301: 	Training 3100/5004. train loss: 6.2681,	0.8858 s / batch. (data: 2.90e-04). ETA=5 days, 1:07:33, max mem: 15.0 GB 
[06/13 17:13:58][INFO] visual_prompt:  301: 	Training 3200/5004. train loss: 6.2080,	0.8800 s / batch. (data: 4.72e-04). ETA=5 days, 0:18:49, max mem: 15.0 GB 
[06/13 17:15:26][INFO] visual_prompt:  301: 	Training 3300/5004. train loss: 6.2750,	0.8787 s / batch. (data: 2.72e-04). ETA=5 days, 0:06:43, max mem: 15.0 GB 
[06/13 17:16:53][INFO] visual_prompt:  301: 	Training 3400/5004. train loss: 6.1855,	0.8798 s / batch. (data: 2.65e-04). ETA=5 days, 0:14:33, max mem: 15.0 GB 
[06/13 17:18:21][INFO] visual_prompt:  301: 	Training 3500/5004. train loss: 6.2630,	0.8792 s / batch. (data: 3.10e-04). ETA=5 days, 0:08:12, max mem: 15.0 GB 
[06/13 17:19:49][INFO] visual_prompt:  301: 	Training 3600/5004. train loss: 6.1433,	0.8703 s / batch. (data: 2.98e-04). ETA=4 days, 22:53:21, max mem: 15.0 GB 
[06/13 17:21:17][INFO] visual_prompt:  301: 	Training 3700/5004. train loss: 6.0875,	0.8751 s / batch. (data: 3.51e-04). ETA=4 days, 23:31:26, max mem: 15.0 GB 
[06/13 17:22:44][INFO] visual_prompt:  301: 	Training 3800/5004. train loss: 6.1221,	0.8769 s / batch. (data: 3.51e-04). ETA=4 days, 23:44:46, max mem: 15.0 GB 
[06/13 17:24:12][INFO] visual_prompt:  301: 	Training 3900/5004. train loss: 6.1962,	0.8769 s / batch. (data: 2.84e-04). ETA=4 days, 23:43:23, max mem: 15.0 GB 
[06/13 17:25:40][INFO] visual_prompt:  301: 	Training 4000/5004. train loss: 6.1529,	0.8781 s / batch. (data: 3.13e-04). ETA=4 days, 23:51:54, max mem: 15.0 GB 
[06/13 17:27:08][INFO] visual_prompt:  301: 	Training 4100/5004. train loss: 6.1740,	0.8737 s / batch. (data: 3.76e-04). ETA=4 days, 23:14:07, max mem: 15.0 GB 
[06/13 17:28:35][INFO] visual_prompt:  301: 	Training 4200/5004. train loss: 6.1601,	0.8804 s / batch. (data: 3.14e-04). ETA=5 days, 0:07:13, max mem: 15.0 GB 
[06/13 17:30:03][INFO] visual_prompt:  301: 	Training 4300/5004. train loss: 6.1740,	0.8778 s / batch. (data: 4.32e-04). ETA=4 days, 23:44:51, max mem: 15.0 GB 
[06/13 17:31:31][INFO] visual_prompt:  301: 	Training 4400/5004. train loss: 6.1665,	0.8796 s / batch. (data: 2.38e-04). ETA=4 days, 23:58:17, max mem: 15.0 GB 
[06/13 17:32:59][INFO] visual_prompt:  301: 	Training 4500/5004. train loss: 6.1212,	0.8834 s / batch. (data: 3.66e-04). ETA=5 days, 0:27:38, max mem: 15.0 GB 
[06/13 17:34:26][INFO] visual_prompt:  301: 	Training 4600/5004. train loss: 6.0549,	0.8791 s / batch. (data: 3.05e-04). ETA=4 days, 23:50:39, max mem: 15.0 GB 
[06/13 17:35:55][INFO] visual_prompt:  301: 	Training 4700/5004. train loss: 6.0797,	0.8758 s / batch. (data: 2.64e-04). ETA=4 days, 23:22:53, max mem: 15.0 GB 
[06/13 17:37:23][INFO] visual_prompt:  301: 	Training 4800/5004. train loss: 6.0152,	0.8801 s / batch. (data: 3.52e-04). ETA=4 days, 23:56:06, max mem: 15.0 GB 
[06/13 17:38:51][INFO] visual_prompt:  301: 	Training 4900/5004. train loss: 6.0104,	0.8792 s / batch. (data: 3.00e-04). ETA=4 days, 23:47:32, max mem: 15.0 GB 
[06/13 17:40:19][INFO] visual_prompt:  301: 	Training 5000/5004. train loss: 6.0753,	0.8759 s / batch. (data: 1.55e-04). ETA=4 days, 23:18:57, max mem: 15.0 GB 
[06/13 17:40:23][INFO] visual_prompt:  314: Epoch 2 / 100: avg data time: 4.48e-03, avg batch time: 0.8830, average train loss: 6.3839
[06/13 17:49:20][INFO] visual_prompt:  434: 	Test 100/196. loss: 61.114, 5.2065 s / batch. (data: 1.23e-04)max mem: 15.00354 GB 
[06/13 17:57:39][INFO] visual_prompt:  471: Inference (val):avg data time: 1.50e-04, avg batch time: 5.1918, average loss: 61.2292
[06/13 17:57:39][INFO] visual_prompt:  488: Saved invariances for val_imagenet at output_ce_10/val_imagenet_invariances.json
[06/13 17:57:39][INFO] visual_prompt:  252: Training 3 / 100 epoch, with learning rate 0.1
[06/13 17:59:34][INFO] visual_prompt:  301: 	Training 100/5004. train loss: 6.1398,	0.8800 s / batch. (data: 3.60e-04). ETA=4 days, 23:51:12, max mem: 15.0 GB 
[06/13 18:01:02][INFO] visual_prompt:  301: 	Training 200/5004. train loss: 6.1674,	0.8797 s / batch. (data: 5.14e-04). ETA=4 days, 23:46:39, max mem: 15.0 GB 
[06/13 18:02:30][INFO] visual_prompt:  301: 	Training 300/5004. train loss: 6.0709,	0.8765 s / batch. (data: 3.11e-04). ETA=4 days, 23:19:35, max mem: 15.0 GB 
[06/13 18:03:57][INFO] visual_prompt:  301: 	Training 400/5004. train loss: 6.1569,	0.8762 s / batch. (data: 2.97e-04). ETA=4 days, 23:15:36, max mem: 15.0 GB 
[06/13 18:05:25][INFO] visual_prompt:  301: 	Training 500/5004. train loss: 6.0784,	0.8789 s / batch. (data: 3.01e-04). ETA=4 days, 23:35:45, max mem: 15.0 GB 
[06/13 18:06:53][INFO] visual_prompt:  301: 	Training 600/5004. train loss: 6.1301,	0.8769 s / batch. (data: 3.82e-04). ETA=4 days, 23:18:05, max mem: 15.0 GB 
[06/13 18:08:21][INFO] visual_prompt:  301: 	Training 700/5004. train loss: 6.0298,	0.8843 s / batch. (data: 2.75e-04). ETA=5 days, 0:17:22, max mem: 15.0 GB 
[06/13 18:09:49][INFO] visual_prompt:  301: 	Training 800/5004. train loss: 5.9919,	0.8751 s / batch. (data: 3.69e-04). ETA=4 days, 23:00:34, max mem: 15.0 GB 
[06/13 18:11:16][INFO] visual_prompt:  301: 	Training 900/5004. train loss: 6.0791,	0.8781 s / batch. (data: 3.34e-04). ETA=4 days, 23:23:31, max mem: 15.0 GB 
[06/13 18:12:44][INFO] visual_prompt:  301: 	Training 1000/5004. train loss: 6.0285,	0.8829 s / batch. (data: 3.10e-04). ETA=5 days, 0:01:28, max mem: 15.0 GB 
[06/13 18:14:12][INFO] visual_prompt:  301: 	Training 1100/5004. train loss: 6.0970,	0.8784 s / batch. (data: 3.40e-04). ETA=4 days, 23:22:55, max mem: 15.0 GB 
[06/13 18:15:40][INFO] visual_prompt:  301: 	Training 1200/5004. train loss: 5.9444,	0.8874 s / batch. (data: 3.31e-04). ETA=5 days, 0:34:59, max mem: 15.0 GB 
[06/13 18:17:08][INFO] visual_prompt:  301: 	Training 1300/5004. train loss: 5.9656,	0.8777 s / batch. (data: 3.38e-04). ETA=4 days, 23:14:45, max mem: 15.0 GB 
[06/13 18:18:36][INFO] visual_prompt:  301: 	Training 1400/5004. train loss: 5.9502,	0.8766 s / batch. (data: 3.45e-04). ETA=4 days, 23:03:56, max mem: 15.0 GB 
[06/13 18:20:03][INFO] visual_prompt:  301: 	Training 1500/5004. train loss: 6.0406,	0.8788 s / batch. (data: 3.05e-04). ETA=4 days, 23:20:43, max mem: 15.0 GB 
[06/13 18:21:31][INFO] visual_prompt:  301: 	Training 1600/5004. train loss: 5.9214,	0.8774 s / batch. (data: 2.83e-04). ETA=4 days, 23:07:38, max mem: 15.0 GB 
[06/13 18:22:59][INFO] visual_prompt:  301: 	Training 1700/5004. train loss: 6.1501,	0.8757 s / batch. (data: 2.89e-04). ETA=4 days, 22:52:33, max mem: 15.0 GB 
[06/13 18:24:27][INFO] visual_prompt:  301: 	Training 1800/5004. train loss: 5.9292,	0.8790 s / batch. (data: 2.71e-04). ETA=4 days, 23:18:13, max mem: 15.0 GB 
[06/13 18:25:55][INFO] visual_prompt:  301: 	Training 1900/5004. train loss: 5.9486,	0.8782 s / batch. (data: 2.73e-04). ETA=4 days, 23:10:17, max mem: 15.0 GB 
[06/13 18:27:22][INFO] visual_prompt:  301: 	Training 2000/5004. train loss: 6.0286,	0.8728 s / batch. (data: 2.77e-04). ETA=4 days, 22:24:08, max mem: 15.0 GB 
[06/13 18:28:50][INFO] visual_prompt:  301: 	Training 2100/5004. train loss: 5.9820,	0.8755 s / batch. (data: 2.91e-04). ETA=4 days, 22:44:58, max mem: 15.0 GB 
[06/13 18:30:17][INFO] visual_prompt:  301: 	Training 2200/5004. train loss: 5.9015,	0.8740 s / batch. (data: 3.00e-04). ETA=4 days, 22:31:17, max mem: 15.0 GB 
[06/13 18:31:45][INFO] visual_prompt:  301: 	Training 2300/5004. train loss: 5.9162,	0.8777 s / batch. (data: 2.44e-04). ETA=4 days, 22:59:41, max mem: 15.0 GB 
[06/13 18:33:13][INFO] visual_prompt:  301: 	Training 2400/5004. train loss: 5.8669,	0.8766 s / batch. (data: 3.02e-04). ETA=4 days, 22:49:56, max mem: 15.0 GB 
[06/13 18:34:40][INFO] visual_prompt:  301: 	Training 2500/5004. train loss: 5.8776,	0.8785 s / batch. (data: 2.88e-04). ETA=4 days, 23:03:30, max mem: 15.0 GB 
[06/13 18:36:08][INFO] visual_prompt:  301: 	Training 2600/5004. train loss: 6.0132,	0.8709 s / batch. (data: 3.40e-04). ETA=4 days, 22:00:27, max mem: 15.0 GB 
[06/13 18:37:35][INFO] visual_prompt:  301: 	Training 2700/5004. train loss: 5.9221,	0.8779 s / batch. (data: 3.02e-04). ETA=4 days, 22:56:01, max mem: 15.0 GB 
[06/13 18:39:03][INFO] visual_prompt:  301: 	Training 2800/5004. train loss: 5.9154,	0.8807 s / batch. (data: 2.91e-04). ETA=4 days, 23:17:03, max mem: 15.0 GB 
[06/13 18:40:31][INFO] visual_prompt:  301: 	Training 2900/5004. train loss: 5.7256,	0.8734 s / batch. (data: 3.48e-04). ETA=4 days, 22:16:38, max mem: 15.0 GB 
[06/13 18:41:58][INFO] visual_prompt:  301: 	Training 3000/5004. train loss: 6.0029,	0.8750 s / batch. (data: 2.76e-04). ETA=4 days, 22:27:48, max mem: 15.0 GB 
[06/13 18:43:26][INFO] visual_prompt:  301: 	Training 3100/5004. train loss: 6.0520,	0.8797 s / batch. (data: 2.97e-04). ETA=4 days, 23:04:25, max mem: 15.0 GB 
[06/13 18:44:54][INFO] visual_prompt:  301: 	Training 3200/5004. train loss: 5.9516,	0.8866 s / batch. (data: 3.79e-04). ETA=4 days, 23:58:50, max mem: 15.0 GB 
[06/13 18:46:21][INFO] visual_prompt:  301: 	Training 3300/5004. train loss: 6.0311,	0.8734 s / batch. (data: 3.42e-04). ETA=4 days, 22:10:06, max mem: 15.0 GB 
[06/13 18:47:49][INFO] visual_prompt:  301: 	Training 3400/5004. train loss: 5.8829,	0.8779 s / batch. (data: 2.89e-04). ETA=4 days, 22:45:23, max mem: 15.0 GB 
[06/13 18:49:16][INFO] visual_prompt:  301: 	Training 3500/5004. train loss: 5.9688,	0.8756 s / batch. (data: 2.89e-04). ETA=4 days, 22:25:31, max mem: 15.0 GB 
[06/13 18:50:44][INFO] visual_prompt:  301: 	Training 3600/5004. train loss: 5.9731,	0.8762 s / batch. (data: 2.83e-04). ETA=4 days, 22:28:41, max mem: 15.0 GB 
[06/13 18:52:12][INFO] visual_prompt:  301: 	Training 3700/5004. train loss: 5.9096,	0.8773 s / batch. (data: 2.95e-04). ETA=4 days, 22:36:22, max mem: 15.0 GB 
[06/13 18:53:39][INFO] visual_prompt:  301: 	Training 3800/5004. train loss: 6.0049,	0.8777 s / batch. (data: 2.97e-04). ETA=4 days, 22:37:41, max mem: 15.0 GB 
[06/13 18:55:07][INFO] visual_prompt:  301: 	Training 3900/5004. train loss: 5.9343,	0.8765 s / batch. (data: 4.10e-04). ETA=4 days, 22:26:50, max mem: 15.0 GB 
[06/13 18:56:35][INFO] visual_prompt:  301: 	Training 4000/5004. train loss: 5.8673,	0.8751 s / batch. (data: 3.59e-04). ETA=4 days, 22:13:39, max mem: 15.0 GB 
[06/13 18:58:02][INFO] visual_prompt:  301: 	Training 4100/5004. train loss: 5.8750,	0.8773 s / batch. (data: 3.24e-04). ETA=4 days, 22:30:13, max mem: 15.0 GB 
[06/13 18:59:30][INFO] visual_prompt:  301: 	Training 4200/5004. train loss: 5.9313,	0.8763 s / batch. (data: 3.30e-04). ETA=4 days, 22:21:13, max mem: 15.0 GB 
[06/13 19:00:58][INFO] visual_prompt:  301: 	Training 4300/5004. train loss: 5.9799,	0.8738 s / batch. (data: 3.55e-04). ETA=4 days, 21:59:30, max mem: 15.0 GB 
[06/13 19:02:26][INFO] visual_prompt:  301: 	Training 4400/5004. train loss: 5.9081,	0.8805 s / batch. (data: 3.70e-04). ETA=4 days, 22:51:54, max mem: 15.0 GB 
[06/13 19:03:54][INFO] visual_prompt:  301: 	Training 4500/5004. train loss: 5.9371,	0.8744 s / batch. (data: 3.73e-04). ETA=4 days, 22:01:02, max mem: 15.0 GB 
[06/13 19:05:22][INFO] visual_prompt:  301: 	Training 4600/5004. train loss: 5.9811,	0.8797 s / batch. (data: 3.02e-04). ETA=4 days, 22:42:52, max mem: 15.0 GB 
[06/13 19:06:49][INFO] visual_prompt:  301: 	Training 4700/5004. train loss: 5.8223,	0.8791 s / batch. (data: 2.86e-04). ETA=4 days, 22:36:34, max mem: 15.0 GB 
[06/13 19:08:18][INFO] visual_prompt:  301: 	Training 4800/5004. train loss: 5.8880,	0.8745 s / batch. (data: 3.83e-04). ETA=4 days, 21:57:15, max mem: 15.0 GB 
[06/13 19:09:46][INFO] visual_prompt:  301: 	Training 4900/5004. train loss: 5.7651,	0.8731 s / batch. (data: 3.18e-04). ETA=4 days, 21:44:49, max mem: 15.0 GB 
[06/13 19:11:14][INFO] visual_prompt:  301: 	Training 5000/5004. train loss: 5.9252,	0.8766 s / batch. (data: 1.02e-04). ETA=4 days, 22:11:26, max mem: 15.0 GB 
[06/13 19:11:19][INFO] visual_prompt:  314: Epoch 3 / 100: avg data time: 4.65e-03, avg batch time: 0.8830, average train loss: 5.9865
[06/13 19:20:15][INFO] visual_prompt:  434: 	Test 100/196. loss: 59.344, 5.1816 s / batch. (data: 1.07e-04)max mem: 15.00354 GB 
[06/13 19:28:31][INFO] visual_prompt:  471: Inference (val):avg data time: 1.53e-04, avg batch time: 5.1723, average loss: 59.7487
[06/13 19:28:31][INFO] visual_prompt:  488: Saved invariances for val_imagenet at output_ce_10/val_imagenet_invariances.json
[06/13 19:28:31][INFO] visual_prompt:  252: Training 4 / 100 epoch, with learning rate 0.15
[06/13 19:30:27][INFO] visual_prompt:  301: 	Training 100/5004. train loss: 5.9397,	0.8782 s / batch. (data: 2.89e-04). ETA=4 days, 22:22:39, max mem: 15.0 GB 
[06/13 19:31:55][INFO] visual_prompt:  301: 	Training 200/5004. train loss: 5.9989,	0.8767 s / batch. (data: 3.44e-04). ETA=4 days, 22:09:28, max mem: 15.0 GB 
[06/13 19:33:22][INFO] visual_prompt:  301: 	Training 300/5004. train loss: 5.9847,	0.8710 s / batch. (data: 3.37e-04). ETA=4 days, 21:21:58, max mem: 15.0 GB 
[06/13 19:34:50][INFO] visual_prompt:  301: 	Training 400/5004. train loss: 5.9674,	0.8804 s / batch. (data: 3.13e-04). ETA=4 days, 22:36:17, max mem: 15.0 GB 
[06/13 19:36:18][INFO] visual_prompt:  301: 	Training 500/5004. train loss: 5.9668,	0.8875 s / batch. (data: 3.32e-04). ETA=4 days, 23:31:59, max mem: 15.0 GB 
[06/13 19:37:45][INFO] visual_prompt:  301: 	Training 600/5004. train loss: 5.9281,	0.8804 s / batch. (data: 3.07e-04). ETA=4 days, 22:33:45, max mem: 15.0 GB 
[06/13 19:39:13][INFO] visual_prompt:  301: 	Training 700/5004. train loss: 6.0222,	0.8736 s / batch. (data: 3.29e-04). ETA=4 days, 21:37:05, max mem: 15.0 GB 
[06/13 19:40:41][INFO] visual_prompt:  301: 	Training 800/5004. train loss: 5.8945,	0.8830 s / batch. (data: 3.06e-04). ETA=4 days, 22:51:51, max mem: 15.0 GB 
[06/13 19:42:09][INFO] visual_prompt:  301: 	Training 900/5004. train loss: 5.9260,	0.8803 s / batch. (data: 2.90e-04). ETA=4 days, 22:28:25, max mem: 15.0 GB 
[06/13 19:43:37][INFO] visual_prompt:  301: 	Training 1000/5004. train loss: 5.8702,	0.8784 s / batch. (data: 2.98e-04). ETA=4 days, 22:11:41, max mem: 15.0 GB 
[06/13 19:45:05][INFO] visual_prompt:  301: 	Training 1100/5004. train loss: 5.9777,	0.8789 s / batch. (data: 3.35e-04). ETA=4 days, 22:13:59, max mem: 15.0 GB 
[06/13 19:46:32][INFO] visual_prompt:  301: 	Training 1200/5004. train loss: 5.8542,	0.8813 s / batch. (data: 3.12e-04). ETA=4 days, 22:32:13, max mem: 15.0 GB 
[06/13 19:48:00][INFO] visual_prompt:  301: 	Training 1300/5004. train loss: 5.8151,	0.8778 s / batch. (data: 3.51e-04). ETA=4 days, 22:02:12, max mem: 15.0 GB 
[06/13 19:49:27][INFO] visual_prompt:  301: 	Training 1400/5004. train loss: 5.9143,	0.8705 s / batch. (data: 3.32e-04). ETA=4 days, 21:02:14, max mem: 15.0 GB 
[06/13 19:50:55][INFO] visual_prompt:  301: 	Training 1500/5004. train loss: 6.0364,	0.8708 s / batch. (data: 3.13e-04). ETA=4 days, 21:02:53, max mem: 15.0 GB 
[06/13 19:52:22][INFO] visual_prompt:  301: 	Training 1600/5004. train loss: 5.9503,	0.8786 s / batch. (data: 3.17e-04). ETA=4 days, 22:04:06, max mem: 15.0 GB 
[06/13 19:53:50][INFO] visual_prompt:  301: 	Training 1700/5004. train loss: 6.0571,	0.8729 s / batch. (data: 3.36e-04). ETA=4 days, 21:17:10, max mem: 15.0 GB 
[06/13 19:55:17][INFO] visual_prompt:  301: 	Training 1800/5004. train loss: 5.9223,	0.8802 s / batch. (data: 3.23e-04). ETA=4 days, 22:14:25, max mem: 15.0 GB 
[06/13 19:56:45][INFO] visual_prompt:  301: 	Training 1900/5004. train loss: 5.8667,	0.8723 s / batch. (data: 2.66e-04). ETA=4 days, 21:08:47, max mem: 15.0 GB 
[06/13 19:58:12][INFO] visual_prompt:  301: 	Training 2000/5004. train loss: 5.9658,	0.8743 s / batch. (data: 3.19e-04). ETA=4 days, 21:23:24, max mem: 15.0 GB 
[06/13 19:59:40][INFO] visual_prompt:  301: 	Training 2100/5004. train loss: 5.9013,	0.8726 s / batch. (data: 2.93e-04). ETA=4 days, 21:08:44, max mem: 15.0 GB 
[06/13 20:01:07][INFO] visual_prompt:  301: 	Training 2200/5004. train loss: 5.9532,	0.8738 s / batch. (data: 3.03e-04). ETA=4 days, 21:16:46, max mem: 15.0 GB 
[06/13 20:02:35][INFO] visual_prompt:  301: 	Training 2300/5004. train loss: 5.8419,	0.8721 s / batch. (data: 3.20e-04). ETA=4 days, 21:01:30, max mem: 15.0 GB 
[06/13 20:04:02][INFO] visual_prompt:  301: 	Training 2400/5004. train loss: 5.9440,	0.8714 s / batch. (data: 3.68e-04). ETA=4 days, 20:54:45, max mem: 15.0 GB 
[06/13 20:05:29][INFO] visual_prompt:  301: 	Training 2500/5004. train loss: 5.9983,	0.8746 s / batch. (data: 3.27e-04). ETA=4 days, 21:18:55, max mem: 15.0 GB 
[06/13 20:06:57][INFO] visual_prompt:  301: 	Training 2600/5004. train loss: 5.9151,	0.8739 s / batch. (data: 3.73e-04). ETA=4 days, 21:11:43, max mem: 15.0 GB 
[06/13 20:08:24][INFO] visual_prompt:  301: 	Training 2700/5004. train loss: 5.9343,	0.8749 s / batch. (data: 2.93e-04). ETA=4 days, 21:18:20, max mem: 15.0 GB 
[06/13 20:09:52][INFO] visual_prompt:  301: 	Training 2800/5004. train loss: 6.1472,	0.8727 s / batch. (data: 3.30e-04). ETA=4 days, 20:58:51, max mem: 15.0 GB 
[06/13 20:11:19][INFO] visual_prompt:  301: 	Training 2900/5004. train loss: 5.8328,	0.8733 s / batch. (data: 3.18e-04). ETA=4 days, 21:02:55, max mem: 15.0 GB 
[06/13 20:12:46][INFO] visual_prompt:  301: 	Training 3000/5004. train loss: 5.9521,	0.8768 s / batch. (data: 3.33e-04). ETA=4 days, 21:29:01, max mem: 15.0 GB 
[06/13 20:14:14][INFO] visual_prompt:  301: 	Training 3100/5004. train loss: 6.0687,	0.8770 s / batch. (data: 3.32e-04). ETA=4 days, 21:29:43, max mem: 15.0 GB 
[06/13 20:15:41][INFO] visual_prompt:  301: 	Training 3200/5004. train loss: 5.9475,	0.8735 s / batch. (data: 3.49e-04). ETA=4 days, 20:59:44, max mem: 15.0 GB 
[06/13 20:17:09][INFO] visual_prompt:  301: 	Training 3300/5004. train loss: 6.0296,	0.8757 s / batch. (data: 3.08e-04). ETA=4 days, 21:15:47, max mem: 15.0 GB 
[06/13 20:18:36][INFO] visual_prompt:  301: 	Training 3400/5004. train loss: 5.9404,	0.8706 s / batch. (data: 3.08e-04). ETA=4 days, 20:33:48, max mem: 15.0 GB 
[06/13 20:20:04][INFO] visual_prompt:  301: 	Training 3500/5004. train loss: 5.9996,	0.8770 s / batch. (data: 3.09e-04). ETA=4 days, 21:23:58, max mem: 15.0 GB 
[06/13 20:21:31][INFO] visual_prompt:  301: 	Training 3600/5004. train loss: 5.8441,	0.8748 s / batch. (data: 3.15e-04). ETA=4 days, 21:04:39, max mem: 15.0 GB 
[06/13 20:22:59][INFO] visual_prompt:  301: 	Training 3700/5004. train loss: 5.9452,	0.8747 s / batch. (data: 3.37e-04). ETA=4 days, 21:01:59, max mem: 15.0 GB 
[06/13 20:24:26][INFO] visual_prompt:  301: 	Training 3800/5004. train loss: 5.9947,	0.8753 s / batch. (data: 3.53e-04). ETA=4 days, 21:05:48, max mem: 15.0 GB 
[06/13 20:25:53][INFO] visual_prompt:  301: 	Training 3900/5004. train loss: 5.9546,	0.8752 s / batch. (data: 3.17e-04). ETA=4 days, 21:03:08, max mem: 15.0 GB 
[06/13 20:27:21][INFO] visual_prompt:  301: 	Training 4000/5004. train loss: 5.9558,	0.8736 s / batch. (data: 2.92e-04). ETA=4 days, 20:48:55, max mem: 15.0 GB 
[06/13 20:28:49][INFO] visual_prompt:  301: 	Training 4100/5004. train loss: 5.9692,	0.8739 s / batch. (data: 2.84e-04). ETA=4 days, 20:49:33, max mem: 15.0 GB 
[06/13 20:30:16][INFO] visual_prompt:  301: 	Training 4200/5004. train loss: 5.9153,	0.8764 s / batch. (data: 3.10e-04). ETA=4 days, 21:08:19, max mem: 15.0 GB 
[06/13 20:31:44][INFO] visual_prompt:  301: 	Training 4300/5004. train loss: 5.9318,	0.8725 s / batch. (data: 3.06e-04). ETA=4 days, 20:36:03, max mem: 15.0 GB 
[06/13 20:33:11][INFO] visual_prompt:  301: 	Training 4400/5004. train loss: 5.8990,	0.8761 s / batch. (data: 3.24e-04). ETA=4 days, 21:03:30, max mem: 15.0 GB 
[06/13 20:34:39][INFO] visual_prompt:  301: 	Training 4500/5004. train loss: 5.9486,	0.8768 s / batch. (data: 2.74e-04). ETA=4 days, 21:07:45, max mem: 15.0 GB 
[06/13 20:36:06][INFO] visual_prompt:  301: 	Training 4600/5004. train loss: 6.0071,	0.8781 s / batch. (data: 2.79e-04). ETA=4 days, 21:16:01, max mem: 15.0 GB 
[06/13 20:37:34][INFO] visual_prompt:  301: 	Training 4700/5004. train loss: 5.9956,	0.8745 s / batch. (data: 2.90e-04). ETA=4 days, 20:45:50, max mem: 15.0 GB 
[06/13 20:39:01][INFO] visual_prompt:  301: 	Training 4800/5004. train loss: 5.8529,	0.8714 s / batch. (data: 2.67e-04). ETA=4 days, 20:19:57, max mem: 15.0 GB 
[06/13 20:40:29][INFO] visual_prompt:  301: 	Training 4900/5004. train loss: 5.8307,	0.8758 s / batch. (data: 3.01e-04). ETA=4 days, 20:53:28, max mem: 15.0 GB 
[06/13 20:41:56][INFO] visual_prompt:  301: 	Training 5000/5004. train loss: 5.9402,	0.8702 s / batch. (data: 9.89e-05). ETA=4 days, 20:07:32, max mem: 15.0 GB 
[06/13 20:42:01][INFO] visual_prompt:  314: Epoch 4 / 100: avg data time: 4.20e-03, avg batch time: 0.8810, average train loss: 5.9423
[06/13 20:50:55][INFO] visual_prompt:  434: 	Test 100/196. loss: 59.233, 5.1785 s / batch. (data: 1.20e-04)max mem: 15.00354 GB 
[06/13 20:59:12][INFO] visual_prompt:  471: Inference (val):avg data time: 1.52e-04, avg batch time: 5.1693, average loss: 59.6431
[06/13 20:59:12][INFO] visual_prompt:  488: Saved invariances for val_imagenet at output_ce_10/val_imagenet_invariances.json
[06/13 20:59:12][INFO] visual_prompt:  252: Training 5 / 100 epoch, with learning rate 0.2
[06/13 21:01:10][INFO] visual_prompt:  301: 	Training 100/5004. train loss: 5.9632,	0.8753 s / batch. (data: 2.97e-04). ETA=4 days, 20:46:23, max mem: 15.0 GB 
[06/13 21:02:37][INFO] visual_prompt:  301: 	Training 200/5004. train loss: 6.0440,	0.8750 s / batch. (data: 3.50e-04). ETA=4 days, 20:42:22, max mem: 15.0 GB 
[06/13 21:04:05][INFO] visual_prompt:  301: 	Training 300/5004. train loss: 6.0218,	0.8775 s / batch. (data: 3.20e-04). ETA=4 days, 21:01:16, max mem: 15.0 GB 
[06/13 21:05:33][INFO] visual_prompt:  301: 	Training 400/5004. train loss: 6.0333,	0.8791 s / batch. (data: 3.74e-04). ETA=4 days, 21:12:17, max mem: 15.0 GB 
[06/13 21:07:01][INFO] visual_prompt:  301: 	Training 500/5004. train loss: 5.9527,	0.8765 s / batch. (data: 3.44e-04). ETA=4 days, 20:50:36, max mem: 15.0 GB 
[06/13 21:08:29][INFO] visual_prompt:  301: 	Training 600/5004. train loss: 5.8853,	0.8772 s / batch. (data: 3.81e-04). ETA=4 days, 20:54:29, max mem: 15.0 GB 
[06/13 21:09:57][INFO] visual_prompt:  301: 	Training 700/5004. train loss: 5.8191,	0.8758 s / batch. (data: 3.28e-04). ETA=4 days, 20:41:24, max mem: 15.0 GB 
[06/13 21:11:25][INFO] visual_prompt:  301: 	Training 800/5004. train loss: 6.0268,	0.8875 s / batch. (data: 3.27e-04). ETA=4 days, 22:13:59, max mem: 15.0 GB 
[06/13 21:12:53][INFO] visual_prompt:  301: 	Training 900/5004. train loss: 5.9417,	0.8801 s / batch. (data: 3.18e-04). ETA=4 days, 21:13:05, max mem: 15.0 GB 
[06/13 21:14:20][INFO] visual_prompt:  301: 	Training 1000/5004. train loss: 6.0088,	0.8825 s / batch. (data: 3.08e-04). ETA=4 days, 21:30:42, max mem: 15.0 GB 
[06/13 21:15:48][INFO] visual_prompt:  301: 	Training 1100/5004. train loss: 5.9203,	0.8741 s / batch. (data: 4.19e-04). ETA=4 days, 20:22:23, max mem: 15.0 GB 
[06/13 21:17:16][INFO] visual_prompt:  301: 	Training 1200/5004. train loss: 6.0025,	0.8768 s / batch. (data: 3.46e-04). ETA=4 days, 20:42:27, max mem: 15.0 GB 
[06/13 21:18:44][INFO] visual_prompt:  301: 	Training 1300/5004. train loss: 5.8049,	0.8807 s / batch. (data: 3.03e-04). ETA=4 days, 21:12:25, max mem: 15.0 GB 
[06/13 21:20:12][INFO] visual_prompt:  301: 	Training 1400/5004. train loss: 5.7532,	0.8798 s / batch. (data: 3.40e-04). ETA=4 days, 21:03:11, max mem: 15.0 GB 
[06/13 21:21:40][INFO] visual_prompt:  301: 	Training 1500/5004. train loss: 6.1307,	0.8879 s / batch. (data: 3.21e-04). ETA=4 days, 22:06:35, max mem: 15.0 GB 
[06/13 21:23:08][INFO] visual_prompt:  301: 	Training 1600/5004. train loss: 5.8088,	0.8838 s / batch. (data: 3.75e-04). ETA=4 days, 21:32:20, max mem: 15.0 GB 
[06/13 21:24:36][INFO] visual_prompt:  301: 	Training 1700/5004. train loss: 5.9859,	0.8734 s / batch. (data: 3.15e-04). ETA=4 days, 20:08:12, max mem: 15.0 GB 
[06/13 21:26:03][INFO] visual_prompt:  301: 	Training 1800/5004. train loss: 5.8882,	0.8783 s / batch. (data: 3.36e-04). ETA=4 days, 20:45:56, max mem: 15.0 GB 
[06/13 21:27:31][INFO] visual_prompt:  301: 	Training 1900/5004. train loss: 5.7538,	0.8755 s / batch. (data: 3.40e-04). ETA=4 days, 20:22:09, max mem: 15.0 GB 
[06/13 21:28:59][INFO] visual_prompt:  301: 	Training 2000/5004. train loss: 5.8461,	0.8756 s / batch. (data: 3.44e-04). ETA=4 days, 20:21:32, max mem: 15.0 GB 
[06/13 21:30:27][INFO] visual_prompt:  301: 	Training 2100/5004. train loss: 5.8776,	0.8777 s / batch. (data: 3.24e-04). ETA=4 days, 20:36:13, max mem: 15.0 GB 
[06/13 21:31:55][INFO] visual_prompt:  301: 	Training 2200/5004. train loss: 5.9115,	0.8733 s / batch. (data: 3.38e-04). ETA=4 days, 19:59:57, max mem: 15.0 GB 
[06/13 21:33:22][INFO] visual_prompt:  301: 	Training 2300/5004. train loss: 5.9733,	0.8745 s / batch. (data: 3.16e-04). ETA=4 days, 20:08:15, max mem: 15.0 GB 
[06/13 21:34:50][INFO] visual_prompt:  301: 	Training 2400/5004. train loss: 5.8913,	0.8748 s / batch. (data: 3.10e-04). ETA=4 days, 20:09:13, max mem: 15.0 GB 
[06/13 21:36:18][INFO] visual_prompt:  301: 	Training 2500/5004. train loss: 5.9211,	0.8811 s / batch. (data: 2.90e-04). ETA=4 days, 20:57:34, max mem: 15.0 GB 
[06/13 21:37:45][INFO] visual_prompt:  301: 	Training 2600/5004. train loss: 5.9882,	0.8802 s / batch. (data: 2.86e-04). ETA=4 days, 20:49:06, max mem: 15.0 GB 
[06/13 21:39:13][INFO] visual_prompt:  301: 	Training 2700/5004. train loss: 5.9746,	0.8808 s / batch. (data: 3.65e-04). ETA=4 days, 20:52:31, max mem: 15.0 GB 
[06/13 21:40:41][INFO] visual_prompt:  301: 	Training 2800/5004. train loss: 5.8527,	0.8818 s / batch. (data: 3.36e-04). ETA=4 days, 20:59:06, max mem: 15.0 GB 
[06/13 21:42:09][INFO] visual_prompt:  301: 	Training 2900/5004. train loss: 5.8601,	0.8762 s / batch. (data: 3.27e-04). ETA=4 days, 20:12:52, max mem: 15.0 GB 
[06/13 21:43:37][INFO] visual_prompt:  301: 	Training 3000/5004. train loss: 5.9377,	0.8793 s / batch. (data: 3.37e-04). ETA=4 days, 20:36:08, max mem: 15.0 GB 
[06/13 21:45:05][INFO] visual_prompt:  301: 	Training 3100/5004. train loss: 6.0246,	0.8761 s / batch. (data: 3.27e-04). ETA=4 days, 20:08:51, max mem: 15.0 GB 
[06/13 21:46:33][INFO] visual_prompt:  301: 	Training 3200/5004. train loss: 5.9543,	0.8780 s / batch. (data: 2.97e-04). ETA=4 days, 20:23:08, max mem: 15.0 GB 
[06/13 21:48:01][INFO] visual_prompt:  301: 	Training 3300/5004. train loss: 6.0437,	0.8772 s / batch. (data: 3.66e-04). ETA=4 days, 20:14:44, max mem: 15.0 GB 
[06/13 21:49:29][INFO] visual_prompt:  301: 	Training 3400/5004. train loss: 5.9521,	0.8817 s / batch. (data: 3.14e-04). ETA=4 days, 20:49:20, max mem: 15.0 GB 
[06/13 21:50:57][INFO] visual_prompt:  301: 	Training 3500/5004. train loss: 6.0429,	0.8738 s / batch. (data: 3.83e-04). ETA=4 days, 19:45:09, max mem: 15.0 GB 
[06/13 21:52:24][INFO] visual_prompt:  301: 	Training 3600/5004. train loss: 5.9551,	0.8771 s / batch. (data: 3.34e-04). ETA=4 days, 20:10:10, max mem: 15.0 GB 
[06/13 21:53:52][INFO] visual_prompt:  301: 	Training 3700/5004. train loss: 5.9687,	0.8749 s / batch. (data: 3.23e-04). ETA=4 days, 19:50:43, max mem: 15.0 GB 
[06/13 21:55:20][INFO] visual_prompt:  301: 	Training 3800/5004. train loss: 5.8083,	0.8749 s / batch. (data: 3.46e-04). ETA=4 days, 19:49:42, max mem: 15.0 GB 
[06/13 21:56:48][INFO] visual_prompt:  301: 	Training 3900/5004. train loss: 5.9412,	0.8789 s / batch. (data: 2.90e-04). ETA=4 days, 20:19:53, max mem: 15.0 GB 
[06/13 21:58:16][INFO] visual_prompt:  301: 	Training 4000/5004. train loss: 6.0087,	0.8775 s / batch. (data: 2.72e-04). ETA=4 days, 20:07:08, max mem: 15.0 GB 
[06/13 21:59:43][INFO] visual_prompt:  301: 	Training 4100/5004. train loss: 5.8502,	0.8780 s / batch. (data: 4.09e-04). ETA=4 days, 20:09:42, max mem: 15.0 GB 
[06/13 22:01:11][INFO] visual_prompt:  301: 	Training 4200/5004. train loss: 5.9834,	0.8833 s / batch. (data: 3.18e-04). ETA=4 days, 20:50:08, max mem: 15.0 GB 
[06/13 22:02:39][INFO] visual_prompt:  301: 	Training 4300/5004. train loss: 6.0061,	0.8790 s / batch. (data: 2.71e-04). ETA=4 days, 20:14:40, max mem: 15.0 GB 
[06/13 22:04:07][INFO] visual_prompt:  301: 	Training 4400/5004. train loss: 5.9254,	0.8804 s / batch. (data: 3.76e-04). ETA=4 days, 20:23:53, max mem: 15.0 GB 
[06/13 22:05:35][INFO] visual_prompt:  301: 	Training 4500/5004. train loss: 5.9949,	0.8754 s / batch. (data: 4.33e-04). ETA=4 days, 19:43:19, max mem: 15.0 GB 
[06/13 22:07:02][INFO] visual_prompt:  301: 	Training 4600/5004. train loss: 5.9327,	0.8794 s / batch. (data: 3.38e-04). ETA=4 days, 20:13:19, max mem: 15.0 GB 
[06/13 22:08:30][INFO] visual_prompt:  301: 	Training 4700/5004. train loss: 5.8793,	0.8818 s / batch. (data: 2.99e-04). ETA=4 days, 20:30:41, max mem: 15.0 GB 
[06/13 22:09:58][INFO] visual_prompt:  301: 	Training 4800/5004. train loss: 5.8837,	0.8820 s / batch. (data: 2.64e-04). ETA=4 days, 20:30:41, max mem: 15.0 GB 
[06/13 22:11:26][INFO] visual_prompt:  301: 	Training 4900/5004. train loss: 5.8744,	0.8794 s / batch. (data: 3.11e-04). ETA=4 days, 20:09:01, max mem: 15.0 GB 
[06/13 22:12:54][INFO] visual_prompt:  301: 	Training 5000/5004. train loss: 5.8496,	0.8738 s / batch. (data: 1.12e-04). ETA=4 days, 19:22:48, max mem: 15.0 GB 
[06/13 22:12:59][INFO] visual_prompt:  314: Epoch 5 / 100: avg data time: 4.29e-03, avg batch time: 0.8845, average train loss: 5.9395
[06/13 22:21:56][INFO] visual_prompt:  434: 	Test 100/196. loss: 59.127, 5.2071 s / batch. (data: 1.50e-04)max mem: 15.00354 GB 
[06/13 22:30:14][INFO] visual_prompt:  471: Inference (val):avg data time: 1.41e-04, avg batch time: 5.1883, average loss: 59.6410
[06/13 22:30:14][INFO] visual_prompt:  488: Saved invariances for val_imagenet at output_ce_10/val_imagenet_invariances.json
[06/13 22:30:14][INFO] visual_prompt:  252: Training 6 / 100 epoch, with learning rate 0.25
[06/13 22:38:00][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/13 22:38:00][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/13 22:38:00][INFO] visual_prompt:   99: Command line arguments: None
[06/13 22:38:00][INFO] visual_prompt:  108: Training with config:
[06/13 22:38:00][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.5,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/13 22:38:03][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/13 22:38:03][INFO] visual_prompt:   56: Total Parameters: 98096656	 Gradient Parameters: 12298768
[06/13 22:38:03][INFO] visual_prompt:   58: tuned percent:12.537
[06/13 22:38:04][INFO] visual_prompt:   44: Device used for model: 0
[06/13 22:38:04][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/13 22:38:04][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/13 22:38:07][INFO] visual_prompt:  158: Number of images: 1281167
[06/13 22:38:07][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 22:38:07][INFO] visual_prompt:   78: Loading validation data...
[06/13 22:38:07][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/13 22:38:07][INFO] visual_prompt:  158: Number of images: 50000
[06/13 22:38:07][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 22:38:07][INFO] visual_prompt:   81: Loading test data...
[06/13 22:38:07][INFO] visual_prompt:   83: ...no test data is constructed
[06/13 22:38:07][INFO] visual_prompt:  111: Constructing models...
[06/13 22:38:07][INFO] visual_prompt:  114: Setting up Evalutator...
[06/13 22:38:07][INFO] visual_prompt:  116: Setting up Trainer...
[06/13 22:38:07][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.deep_prompt_embeddings: True
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/13 22:38:07][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/13 22:38:07][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/13 22:38:07][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/13 22:38:07][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/13 22:38:07][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/13 22:38:07][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/13 22:38:07][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/13 22:38:07][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/13 22:38:07][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/13 22:38:07][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/13 22:38:07][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/13 22:38:07][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/13 22:38:07][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/13 22:38:07][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/13 22:38:07][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/13 22:38:07][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/13 22:38:07][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/13 22:38:07][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/13 22:38:07][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/13 22:38:07][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/13 22:38:08][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/13 22:38:08][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/13 22:38:28][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 22:38:28][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 22:39:47][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/13 22:39:47][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/13 22:39:47][INFO] visual_prompt:   99: Command line arguments: None
[06/13 22:39:47][INFO] visual_prompt:  108: Training with config:
[06/13 22:39:47][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 128,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.5,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'kl',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/13 22:39:51][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/13 22:39:51][INFO] visual_prompt:   56: Total Parameters: 98096656	 Gradient Parameters: 12298768
[06/13 22:39:51][INFO] visual_prompt:   58: tuned percent:12.537
[06/13 22:39:51][INFO] visual_prompt:   44: Device used for model: 0
[06/13 22:39:51][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/13 22:39:51][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/13 22:39:54][INFO] visual_prompt:  158: Number of images: 1281167
[06/13 22:39:54][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 22:39:54][INFO] visual_prompt:   78: Loading validation data...
[06/13 22:39:54][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/13 22:39:54][INFO] visual_prompt:  158: Number of images: 50000
[06/13 22:39:54][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 22:39:54][INFO] visual_prompt:   81: Loading test data...
[06/13 22:39:54][INFO] visual_prompt:   83: ...no test data is constructed
[06/13 22:39:54][INFO] visual_prompt:  111: Constructing models...
[06/13 22:39:54][INFO] visual_prompt:  114: Setting up Evalutator...
[06/13 22:39:54][INFO] visual_prompt:  116: Setting up Trainer...
[06/13 22:39:54][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.deep_prompt_embeddings: True
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/13 22:39:54][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/13 22:39:54][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/13 22:39:54][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/13 22:39:54][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/13 22:39:54][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/13 22:39:54][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/13 22:39:54][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/13 22:39:54][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/13 22:39:54][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/13 22:39:54][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/13 22:39:54][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/13 22:39:54][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/13 22:39:54][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/13 22:39:54][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/13 22:39:54][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/13 22:39:54][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/13 22:39:54][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/13 22:39:54][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/13 22:39:54][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/13 22:39:54][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/13 22:39:54][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/13 22:39:54][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/13 22:40:13][INFO] visual_prompt:   98: shape of inputs: torch.Size([32, 3, 224, 224]) torch.Size([32, 3, 224, 224])
[06/13 22:40:13][INFO] visual_prompt:   99: shape of targets: torch.Size([32]) torch.Size([32, 1])
[06/13 22:42:16][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/13 22:42:16][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/13 22:42:16][INFO] visual_prompt:   99: Command line arguments: None
[06/13 22:42:16][INFO] visual_prompt:  108: Training with config:
[06/13 22:42:16][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 10,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.5,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/13 22:42:20][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/13 22:42:20][INFO] visual_prompt:   56: Total Parameters: 98096656	 Gradient Parameters: 12298768
[06/13 22:42:20][INFO] visual_prompt:   58: tuned percent:12.537
[06/13 22:42:20][INFO] visual_prompt:   44: Device used for model: 0
[06/13 22:42:20][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/13 22:42:20][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/13 22:42:23][INFO] visual_prompt:  158: Number of images: 1281167
[06/13 22:42:23][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 22:42:23][INFO] visual_prompt:   78: Loading validation data...
[06/13 22:42:23][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/13 22:42:23][INFO] visual_prompt:  158: Number of images: 50000
[06/13 22:42:23][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 22:42:23][INFO] visual_prompt:   81: Loading test data...
[06/13 22:42:23][INFO] visual_prompt:   83: ...no test data is constructed
[06/13 22:42:23][INFO] visual_prompt:  111: Constructing models...
[06/13 22:42:23][INFO] visual_prompt:  114: Setting up Evalutator...
[06/13 22:42:23][INFO] visual_prompt:  116: Setting up Trainer...
[06/13 22:42:23][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.deep_prompt_embeddings: True
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/13 22:42:23][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/13 22:42:23][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/13 22:42:23][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/13 22:42:23][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/13 22:42:23][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/13 22:42:23][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/13 22:42:23][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/13 22:42:23][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/13 22:42:23][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/13 22:42:23][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/13 22:42:23][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/13 22:42:23][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/13 22:42:23][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/13 22:42:23][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/13 22:42:23][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/13 22:42:23][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/13 22:42:23][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/13 22:42:23][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/13 22:42:23][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/13 22:42:23][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/13 22:42:23][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/13 22:42:23][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/13 22:42:43][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 22:42:43][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 22:44:27][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/13 22:44:27][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/13 22:44:27][INFO] visual_prompt:   99: Command line arguments: None
[06/13 22:44:27][INFO] visual_prompt:  108: Training with config:
[06/13 22:44:27][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 100,
                      'NUM_TOKENS_PER_TYPE': 10,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.5,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/13 22:44:31][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/13 22:44:31][INFO] visual_prompt:   56: Total Parameters: 94410256	 Gradient Parameters: 8612368
[06/13 22:44:31][INFO] visual_prompt:   58: tuned percent:9.122
[06/13 22:44:31][INFO] visual_prompt:   44: Device used for model: 0
[06/13 22:44:31][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/13 22:44:31][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/13 22:44:34][INFO] visual_prompt:  158: Number of images: 1281167
[06/13 22:44:34][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 22:44:34][INFO] visual_prompt:   78: Loading validation data...
[06/13 22:44:34][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/13 22:44:34][INFO] visual_prompt:  158: Number of images: 50000
[06/13 22:44:34][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 22:44:34][INFO] visual_prompt:   81: Loading test data...
[06/13 22:44:34][INFO] visual_prompt:   83: ...no test data is constructed
[06/13 22:44:34][INFO] visual_prompt:  111: Constructing models...
[06/13 22:44:34][INFO] visual_prompt:  114: Setting up Evalutator...
[06/13 22:44:34][INFO] visual_prompt:  116: Setting up Trainer...
[06/13 22:44:34][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.deep_prompt_embeddings: True
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/13 22:44:34][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/13 22:44:34][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/13 22:44:34][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/13 22:44:34][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/13 22:44:34][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/13 22:44:34][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/13 22:44:34][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/13 22:44:34][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/13 22:44:34][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/13 22:44:34][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/13 22:44:34][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/13 22:44:34][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/13 22:44:34][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/13 22:44:34][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/13 22:44:34][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/13 22:44:34][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/13 22:44:34][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/13 22:44:34][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/13 22:44:34][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/13 22:44:34][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/13 22:44:34][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/13 22:44:34][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/13 22:44:54][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/13 22:44:54][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/13 22:46:58][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/13 22:46:58][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/13 22:46:58][INFO] visual_prompt:   99: Command line arguments: None
[06/13 22:46:58][INFO] visual_prompt:  108: Training with config:
[06/13 22:46:58][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 128,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 100,
                      'NUM_TOKENS_PER_TYPE': 10,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.5,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/13 22:47:02][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/13 22:47:02][INFO] visual_prompt:   56: Total Parameters: 94410256	 Gradient Parameters: 8612368
[06/13 22:47:02][INFO] visual_prompt:   58: tuned percent:9.122
[06/13 22:47:02][INFO] visual_prompt:   44: Device used for model: 0
[06/13 22:47:02][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/13 22:47:02][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/13 22:47:05][INFO] visual_prompt:  158: Number of images: 1281167
[06/13 22:47:05][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 22:47:05][INFO] visual_prompt:   78: Loading validation data...
[06/13 22:47:05][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/13 22:47:05][INFO] visual_prompt:  158: Number of images: 50000
[06/13 22:47:05][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 22:47:05][INFO] visual_prompt:   81: Loading test data...
[06/13 22:47:05][INFO] visual_prompt:   83: ...no test data is constructed
[06/13 22:47:05][INFO] visual_prompt:  111: Constructing models...
[06/13 22:47:05][INFO] visual_prompt:  114: Setting up Evalutator...
[06/13 22:47:05][INFO] visual_prompt:  116: Setting up Trainer...
[06/13 22:47:05][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.deep_prompt_embeddings: True
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/13 22:47:05][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/13 22:47:05][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/13 22:47:05][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/13 22:47:05][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/13 22:47:05][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/13 22:47:05][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/13 22:47:05][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/13 22:47:05][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/13 22:47:05][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/13 22:47:05][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/13 22:47:05][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/13 22:47:05][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/13 22:47:05][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/13 22:47:05][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/13 22:47:05][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/13 22:47:05][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/13 22:47:05][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/13 22:47:05][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/13 22:47:05][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/13 22:47:05][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/13 22:47:05][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/13 22:47:05][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/13 22:47:24][INFO] visual_prompt:   98: shape of inputs: torch.Size([32, 3, 224, 224]) torch.Size([32, 3, 224, 224])
[06/13 22:47:24][INFO] visual_prompt:   99: shape of targets: torch.Size([32]) torch.Size([32, 1])
[06/13 22:50:01][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/13 22:50:01][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/13 22:50:01][INFO] visual_prompt:   99: Command line arguments: None
[06/13 22:50:01][INFO] visual_prompt:  108: Training with config:
[06/13 22:50:01][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 100,
                      'NUM_TOKENS_PER_TYPE': 10,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.5,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/13 22:50:04][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/13 22:50:04][INFO] visual_prompt:   56: Total Parameters: 94410256	 Gradient Parameters: 8612368
[06/13 22:50:04][INFO] visual_prompt:   58: tuned percent:9.122
[06/13 22:50:04][INFO] visual_prompt:   44: Device used for model: 0
[06/13 22:50:04][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/13 22:50:04][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/13 22:50:07][INFO] visual_prompt:  158: Number of images: 1281167
[06/13 22:50:07][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 22:50:07][INFO] visual_prompt:   78: Loading validation data...
[06/13 22:50:07][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/13 22:50:07][INFO] visual_prompt:  158: Number of images: 50000
[06/13 22:50:07][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 22:50:07][INFO] visual_prompt:   81: Loading test data...
[06/13 22:50:07][INFO] visual_prompt:   83: ...no test data is constructed
[06/13 22:50:07][INFO] visual_prompt:  111: Constructing models...
[06/13 22:50:07][INFO] visual_prompt:  114: Setting up Evalutator...
[06/13 22:50:07][INFO] visual_prompt:  116: Setting up Trainer...
[06/13 22:50:07][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.deep_prompt_embeddings: True
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/13 22:50:07][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/13 22:50:07][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/13 22:50:07][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/13 22:50:07][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/13 22:50:07][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/13 22:50:07][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/13 22:50:07][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/13 22:50:07][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/13 22:50:07][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/13 22:50:07][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/13 22:50:07][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/13 22:50:07][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/13 22:50:07][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/13 22:50:07][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/13 22:50:07][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/13 22:50:07][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/13 22:50:07][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/13 22:50:07][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/13 22:50:07][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/13 22:50:07][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/13 22:50:07][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/13 22:50:07][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/13 22:50:26][INFO] visual_prompt:   98: shape of inputs: torch.Size([16, 3, 224, 224]) torch.Size([16, 3, 224, 224])
[06/13 22:50:26][INFO] visual_prompt:   99: shape of targets: torch.Size([16]) torch.Size([16, 1])
[06/13 22:50:28][INFO] visual_prompt:  141: shape of model output: torch.Size([16, 768]), targets: torch.Size([16])
[06/13 22:50:30][INFO] visual_prompt:   98: shape of inputs: torch.Size([16, 3, 224, 224]) torch.Size([16, 3, 224, 224])
[06/13 22:50:30][INFO] visual_prompt:   99: shape of targets: torch.Size([16]) torch.Size([16, 1])
[06/13 22:50:31][INFO] visual_prompt:  141: shape of model output: torch.Size([16, 768]), targets: torch.Size([16])
[06/13 22:50:31][INFO] visual_prompt:   98: shape of inputs: torch.Size([16, 3, 224, 224]) torch.Size([16, 3, 224, 224])
[06/13 22:50:31][INFO] visual_prompt:   99: shape of targets: torch.Size([16]) torch.Size([16, 1])
[06/13 22:50:32][INFO] visual_prompt:  141: shape of model output: torch.Size([16, 768]), targets: torch.Size([16])
[06/13 22:50:33][INFO] visual_prompt:   98: shape of inputs: torch.Size([16, 3, 224, 224]) torch.Size([16, 3, 224, 224])
[06/13 22:50:33][INFO] visual_prompt:   99: shape of targets: torch.Size([16]) torch.Size([16, 1])
[06/13 22:50:33][INFO] visual_prompt:  141: shape of model output: torch.Size([16, 768]), targets: torch.Size([16])
[06/13 22:50:34][INFO] visual_prompt:   98: shape of inputs: torch.Size([16, 3, 224, 224]) torch.Size([16, 3, 224, 224])
[06/13 22:50:34][INFO] visual_prompt:   99: shape of targets: torch.Size([16]) torch.Size([16, 1])
[06/13 22:50:34][INFO] visual_prompt:  141: shape of model output: torch.Size([16, 768]), targets: torch.Size([16])
[06/13 22:50:35][INFO] visual_prompt:   98: shape of inputs: torch.Size([16, 3, 224, 224]) torch.Size([16, 3, 224, 224])
[06/13 22:50:35][INFO] visual_prompt:   99: shape of targets: torch.Size([16]) torch.Size([16, 1])
[06/13 22:50:36][INFO] visual_prompt:  141: shape of model output: torch.Size([16, 768]), targets: torch.Size([16])
[06/13 22:50:36][INFO] visual_prompt:   98: shape of inputs: torch.Size([16, 3, 224, 224]) torch.Size([16, 3, 224, 224])
[06/13 22:50:36][INFO] visual_prompt:   99: shape of targets: torch.Size([16]) torch.Size([16, 1])
[06/13 22:50:37][INFO] visual_prompt:  141: shape of model output: torch.Size([16, 768]), targets: torch.Size([16])
[06/13 22:50:37][INFO] visual_prompt:   98: shape of inputs: torch.Size([16, 3, 224, 224]) torch.Size([16, 3, 224, 224])
[06/13 22:50:37][INFO] visual_prompt:   99: shape of targets: torch.Size([16]) torch.Size([16, 1])
[06/13 22:50:38][INFO] visual_prompt:  141: shape of model output: torch.Size([16, 768]), targets: torch.Size([16])
[06/13 22:50:38][INFO] visual_prompt:   98: shape of inputs: torch.Size([16, 3, 224, 224]) torch.Size([16, 3, 224, 224])
[06/13 22:50:38][INFO] visual_prompt:   99: shape of targets: torch.Size([16]) torch.Size([16, 1])
[06/13 22:50:39][INFO] visual_prompt:  141: shape of model output: torch.Size([16, 768]), targets: torch.Size([16])
[06/13 22:50:39][INFO] visual_prompt:   98: shape of inputs: torch.Size([16, 3, 224, 224]) torch.Size([16, 3, 224, 224])
[06/13 22:50:39][INFO] visual_prompt:   99: shape of targets: torch.Size([16]) torch.Size([16, 1])
[06/13 22:50:40][INFO] visual_prompt:  141: shape of model output: torch.Size([16, 768]), targets: torch.Size([16])
[06/13 22:50:40][INFO] visual_prompt:   98: shape of inputs: torch.Size([16, 3, 224, 224]) torch.Size([16, 3, 224, 224])
[06/13 22:50:40][INFO] visual_prompt:   99: shape of targets: torch.Size([16]) torch.Size([16, 1])
[06/13 22:50:41][INFO] visual_prompt:  141: shape of model output: torch.Size([16, 768]), targets: torch.Size([16])
[06/13 22:50:42][INFO] visual_prompt:   98: shape of inputs: torch.Size([16, 3, 224, 224]) torch.Size([16, 3, 224, 224])
[06/13 22:50:42][INFO] visual_prompt:   99: shape of targets: torch.Size([16]) torch.Size([16, 1])
[06/13 22:50:42][INFO] visual_prompt:  141: shape of model output: torch.Size([16, 768]), targets: torch.Size([16])
[06/13 22:50:43][INFO] visual_prompt:   98: shape of inputs: torch.Size([16, 3, 224, 224]) torch.Size([16, 3, 224, 224])
[06/13 22:50:43][INFO] visual_prompt:   99: shape of targets: torch.Size([16]) torch.Size([16, 1])
[06/13 22:50:43][INFO] visual_prompt:  141: shape of model output: torch.Size([16, 768]), targets: torch.Size([16])
[06/13 22:50:44][INFO] visual_prompt:   98: shape of inputs: torch.Size([16, 3, 224, 224]) torch.Size([16, 3, 224, 224])
[06/13 22:50:44][INFO] visual_prompt:   99: shape of targets: torch.Size([16]) torch.Size([16, 1])
[06/13 22:50:45][INFO] visual_prompt:  141: shape of model output: torch.Size([16, 768]), targets: torch.Size([16])
[06/13 22:51:38][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/13 22:51:38][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/13 22:51:38][INFO] visual_prompt:   99: Command line arguments: None
[06/13 22:51:38][INFO] visual_prompt:  108: Training with config:
[06/13 22:51:38][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 100,
                      'NUM_TOKENS_PER_TYPE': 10,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.5,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/13 22:51:42][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/13 22:51:42][INFO] visual_prompt:   56: Total Parameters: 94410256	 Gradient Parameters: 8612368
[06/13 22:51:42][INFO] visual_prompt:   58: tuned percent:9.122
[06/13 22:51:42][INFO] visual_prompt:   44: Device used for model: 0
[06/13 22:51:42][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/13 22:51:42][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/13 22:51:45][INFO] visual_prompt:  158: Number of images: 1281167
[06/13 22:51:45][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 22:51:45][INFO] visual_prompt:   78: Loading validation data...
[06/13 22:51:45][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/13 22:51:45][INFO] visual_prompt:  158: Number of images: 50000
[06/13 22:51:45][INFO] visual_prompt:  159: Number of classes: 1000
[06/13 22:51:45][INFO] visual_prompt:   81: Loading test data...
[06/13 22:51:45][INFO] visual_prompt:   83: ...no test data is constructed
[06/13 22:51:45][INFO] visual_prompt:  111: Constructing models...
[06/13 22:51:45][INFO] visual_prompt:  114: Setting up Evalutator...
[06/13 22:51:45][INFO] visual_prompt:  116: Setting up Trainer...
[06/13 22:51:45][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.deep_prompt_embeddings: True
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/13 22:51:45][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/13 22:51:45][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/13 22:51:45][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/13 22:51:45][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/13 22:51:45][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/13 22:51:45][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/13 22:51:45][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/13 22:51:45][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/13 22:51:45][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/13 22:51:45][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/13 22:51:45][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/13 22:51:45][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/13 22:51:45][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/13 22:51:45][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/13 22:51:45][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/13 22:51:45][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/13 22:51:45][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/13 22:51:45][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/13 22:51:45][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/13 22:51:45][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/13 22:51:45][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/13 22:51:45][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
