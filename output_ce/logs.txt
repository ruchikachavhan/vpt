[06/11 23:50:43][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/11 23:50:43][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/11 23:50:43][INFO] visual_prompt:   99: Command line arguments: None
[06/11 23:50:43][INFO] visual_prompt:  108: Training with config:
[06/11 23:50:43][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': False,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 31,
                      'NUM_TOKENS': 1550,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 1.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/11 23:50:50][INFO] visual_prompt:   56: Total Parameters: 110828056	 Gradient Parameters: 25029400
[06/11 23:50:50][INFO] visual_prompt:   58: tuned percent:22.584
[06/11 23:50:50][INFO] visual_prompt:   44: Device used for model: 0
[06/11 23:50:50][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/11 23:50:50][INFO] visual_prompt:   49: Constructing imagenet dataset train...
[06/11 23:50:54][INFO] visual_prompt:  110: Number of images: 1281167
[06/11 23:50:54][INFO] visual_prompt:  111: Number of classes: 1000
[06/11 23:50:54][INFO] visual_prompt:   78: Loading validation data...
[06/11 23:50:54][INFO] visual_prompt:   49: Constructing imagenet dataset val...
[06/11 23:50:54][INFO] visual_prompt:  110: Number of images: 50000
[06/11 23:50:54][INFO] visual_prompt:  111: Number of classes: 1000
[06/11 23:50:54][INFO] visual_prompt:   81: Loading test data...
[06/11 23:50:54][INFO] visual_prompt:   83: ...no test data is constructed
[06/11 23:50:54][INFO] visual_prompt:  111: Constructing models...
[06/11 23:50:54][INFO] visual_prompt:  114: Setting up Evalutator...
[06/11 23:50:54][INFO] visual_prompt:  116: Setting up Trainer...
[06/11 23:50:54][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.10.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.10.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.11.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.11.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.12.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.12.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.13.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.13.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.14.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.14.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.15.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.15.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.16.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.16.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.17.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.17.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.18.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.18.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.19.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.19.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.20.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.20.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.21.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.21.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.22.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.22.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.23.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.23.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.24.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.24.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.25.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.25.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.26.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.26.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.27.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.27.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.28.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.28.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.29.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.29.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.30.last_layer.weight: True
[06/11 23:50:54][INFO] visual_prompt:   59: module.head.30.last_layer.bias: True
[06/11 23:50:54][INFO] visual_prompt:  219: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/11 23:50:54][INFO] visual_prompt:  239: Training 1 / 100 epoch, with learning rate 0.0
[06/11 23:52:51][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/11 23:52:52][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/11 23:52:52][INFO] visual_prompt:   99: Command line arguments: None
[06/11 23:52:52][INFO] visual_prompt:  108: Training with config:
[06/11 23:52:52][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 31,
                      'NUM_TOKENS': 1550,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 1.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/11 23:52:59][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (10): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (11): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (12): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (13): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (14): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (15): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (16): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (17): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (18): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (19): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (20): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (21): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (22): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (23): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (24): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (25): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (26): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (27): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (28): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (29): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (30): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/11 23:52:59][INFO] visual_prompt:   56: Total Parameters: 110828056	 Gradient Parameters: 25029400
[06/11 23:52:59][INFO] visual_prompt:   58: tuned percent:22.584
[06/11 23:52:59][INFO] visual_prompt:   44: Device used for model: 0
[06/11 23:52:59][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/11 23:52:59][INFO] visual_prompt:   49: Constructing imagenet dataset train...
[06/11 23:53:02][INFO] visual_prompt:  110: Number of images: 1281167
[06/11 23:53:02][INFO] visual_prompt:  111: Number of classes: 1000
[06/11 23:53:03][INFO] visual_prompt:   78: Loading validation data...
[06/11 23:53:03][INFO] visual_prompt:   49: Constructing imagenet dataset val...
[06/11 23:53:03][INFO] visual_prompt:  110: Number of images: 50000
[06/11 23:53:03][INFO] visual_prompt:  111: Number of classes: 1000
[06/11 23:53:03][INFO] visual_prompt:   81: Loading test data...
[06/11 23:53:03][INFO] visual_prompt:   83: ...no test data is constructed
[06/11 23:53:03][INFO] visual_prompt:  111: Constructing models...
[06/11 23:53:03][INFO] visual_prompt:  114: Setting up Evalutator...
[06/11 23:53:03][INFO] visual_prompt:  116: Setting up Trainer...
[06/11 23:53:03][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.10.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.10.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.11.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.11.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.12.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.12.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.13.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.13.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.14.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.14.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.15.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.15.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.16.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.16.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.17.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.17.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.18.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.18.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.19.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.19.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.20.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.20.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.21.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.21.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.22.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.22.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.23.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.23.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.24.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.24.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.25.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.25.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.26.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.26.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.27.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.27.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.28.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.28.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.29.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.29.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.30.last_layer.weight: True
[06/11 23:53:03][INFO] visual_prompt:   59: module.head.30.last_layer.bias: True
[06/11 23:53:03][INFO] visual_prompt:  219: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/11 23:53:03][INFO] visual_prompt:  239: Training 1 / 100 epoch, with learning rate 0.0
[06/11 23:53:29][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/11 23:53:29][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/11 23:53:32][INFO] visual_prompt:  139: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/11 23:57:43][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/11 23:57:43][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/11 23:57:43][INFO] visual_prompt:   99: Command line arguments: None
[06/11 23:57:43][INFO] visual_prompt:  108: Training with config:
[06/11 23:57:43][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 31,
                      'NUM_TOKENS': 1550,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 1.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/11 23:57:50][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (10): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (11): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (12): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (13): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (14): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (15): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (16): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (17): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (18): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (19): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (20): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (21): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (22): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (23): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (24): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (25): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (26): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (27): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (28): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (29): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (30): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/11 23:57:50][INFO] visual_prompt:   56: Total Parameters: 110828056	 Gradient Parameters: 25029400
[06/11 23:57:50][INFO] visual_prompt:   58: tuned percent:22.584
[06/11 23:57:50][INFO] visual_prompt:   44: Device used for model: 0
[06/11 23:57:50][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/11 23:57:50][INFO] visual_prompt:   49: Constructing imagenet dataset train...
[06/11 23:57:54][INFO] visual_prompt:  110: Number of images: 1281167
[06/11 23:57:54][INFO] visual_prompt:  111: Number of classes: 1000
[06/11 23:57:54][INFO] visual_prompt:   78: Loading validation data...
[06/11 23:57:54][INFO] visual_prompt:   49: Constructing imagenet dataset val...
[06/11 23:57:54][INFO] visual_prompt:  110: Number of images: 50000
[06/11 23:57:54][INFO] visual_prompt:  111: Number of classes: 1000
[06/11 23:57:54][INFO] visual_prompt:   81: Loading test data...
[06/11 23:57:54][INFO] visual_prompt:   83: ...no test data is constructed
[06/11 23:57:54][INFO] visual_prompt:  111: Constructing models...
[06/11 23:57:54][INFO] visual_prompt:  114: Setting up Evalutator...
[06/11 23:57:54][INFO] visual_prompt:  116: Setting up Trainer...
[06/11 23:57:54][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.10.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.10.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.11.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.11.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.12.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.12.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.13.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.13.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.14.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.14.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.15.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.15.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.16.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.16.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.17.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.17.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.18.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.18.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.19.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.19.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.20.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.20.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.21.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.21.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.22.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.22.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.23.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.23.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.24.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.24.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.25.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.25.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.26.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.26.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.27.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.27.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.28.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.28.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.29.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.29.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.30.last_layer.weight: True
[06/11 23:57:54][INFO] visual_prompt:   59: module.head.30.last_layer.bias: True
[06/11 23:57:54][INFO] visual_prompt:  220: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/11 23:57:54][INFO] visual_prompt:  240: Training 1 / 100 epoch, with learning rate 0.0
[06/11 23:58:21][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/11 23:58:21][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/11 23:58:24][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/11 23:59:33][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/11 23:59:33][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/11 23:59:33][INFO] visual_prompt:   99: Command line arguments: None
[06/11 23:59:33][INFO] visual_prompt:  108: Training with config:
[06/11 23:59:33][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 31,
                      'NUM_TOKENS': 1550,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 1.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/11 23:59:40][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (10): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (11): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (12): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (13): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (14): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (15): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (16): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (17): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (18): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (19): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (20): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (21): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (22): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (23): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (24): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (25): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (26): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (27): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (28): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (29): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (30): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/11 23:59:40][INFO] visual_prompt:   56: Total Parameters: 110828056	 Gradient Parameters: 25029400
[06/11 23:59:40][INFO] visual_prompt:   58: tuned percent:22.584
[06/11 23:59:40][INFO] visual_prompt:   44: Device used for model: 0
[06/11 23:59:40][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/11 23:59:40][INFO] visual_prompt:   49: Constructing imagenet dataset train...
[06/11 23:59:45][INFO] visual_prompt:  110: Number of images: 1281167
[06/11 23:59:45][INFO] visual_prompt:  111: Number of classes: 1000
[06/11 23:59:45][INFO] visual_prompt:   78: Loading validation data...
[06/11 23:59:45][INFO] visual_prompt:   49: Constructing imagenet dataset val...
[06/11 23:59:45][INFO] visual_prompt:  110: Number of images: 50000
[06/11 23:59:45][INFO] visual_prompt:  111: Number of classes: 1000
[06/11 23:59:45][INFO] visual_prompt:   81: Loading test data...
[06/11 23:59:45][INFO] visual_prompt:   83: ...no test data is constructed
[06/11 23:59:45][INFO] visual_prompt:  111: Constructing models...
[06/11 23:59:45][INFO] visual_prompt:  114: Setting up Evalutator...
[06/11 23:59:45][INFO] visual_prompt:  116: Setting up Trainer...
[06/11 23:59:45][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.10.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.10.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.11.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.11.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.12.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.12.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.13.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.13.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.14.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.14.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.15.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.15.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.16.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.16.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.17.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.17.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.18.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.18.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.19.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.19.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.20.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.20.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.21.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.21.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.22.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.22.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.23.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.23.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.24.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.24.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.25.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.25.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.26.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.26.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.27.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.27.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.28.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.28.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.29.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.29.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.30.last_layer.weight: True
[06/11 23:59:45][INFO] visual_prompt:   59: module.head.30.last_layer.bias: True
[06/11 23:59:45][INFO] visual_prompt:  220: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/11 23:59:45][INFO] visual_prompt:  240: Training 1 / 100 epoch, with learning rate 0.0
[06/12 00:00:12][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:00:12][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:00:15][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:02:06][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/12 00:02:06][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/12 00:02:06][INFO] visual_prompt:   99: Command line arguments: None
[06/12 00:02:06][INFO] visual_prompt:  108: Training with config:
[06/12 00:02:06][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 31,
                      'NUM_TOKENS': 1550,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 1.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/12 00:02:13][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (10): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (11): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (12): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (13): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (14): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (15): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (16): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (17): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (18): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (19): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (20): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (21): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (22): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (23): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (24): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (25): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (26): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (27): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (28): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (29): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (30): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/12 00:02:13][INFO] visual_prompt:   56: Total Parameters: 110828056	 Gradient Parameters: 25029400
[06/12 00:02:13][INFO] visual_prompt:   58: tuned percent:22.584
[06/12 00:02:13][INFO] visual_prompt:   44: Device used for model: 0
[06/12 00:02:13][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/12 00:02:13][INFO] visual_prompt:   49: Constructing imagenet dataset train...
[06/12 00:02:17][INFO] visual_prompt:  110: Number of images: 1281167
[06/12 00:02:17][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:02:17][INFO] visual_prompt:   78: Loading validation data...
[06/12 00:02:17][INFO] visual_prompt:   49: Constructing imagenet dataset val...
[06/12 00:02:18][INFO] visual_prompt:  110: Number of images: 50000
[06/12 00:02:18][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:02:18][INFO] visual_prompt:   81: Loading test data...
[06/12 00:02:18][INFO] visual_prompt:   83: ...no test data is constructed
[06/12 00:02:18][INFO] visual_prompt:  111: Constructing models...
[06/12 00:02:18][INFO] visual_prompt:  114: Setting up Evalutator...
[06/12 00:02:18][INFO] visual_prompt:  116: Setting up Trainer...
[06/12 00:02:18][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.10.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.10.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.11.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.11.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.12.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.12.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.13.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.13.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.14.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.14.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.15.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.15.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.16.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.16.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.17.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.17.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.18.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.18.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.19.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.19.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.20.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.20.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.21.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.21.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.22.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.22.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.23.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.23.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.24.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.24.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.25.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.25.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.26.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.26.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.27.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.27.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.28.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.28.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.29.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.29.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.30.last_layer.weight: True
[06/12 00:02:18][INFO] visual_prompt:   59: module.head.30.last_layer.bias: True
[06/12 00:02:18][INFO] visual_prompt:  223: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/12 00:02:18][INFO] visual_prompt:  243: Training 1 / 100 epoch, with learning rate 0.0
[06/12 00:02:45][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:02:45][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:02:48][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:03:50][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/12 00:03:50][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/12 00:03:50][INFO] visual_prompt:   99: Command line arguments: None
[06/12 00:03:50][INFO] visual_prompt:  108: Training with config:
[06/12 00:03:50][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 31,
                      'NUM_TOKENS': 1550,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 1.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/12 00:03:56][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (10): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (11): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (12): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (13): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (14): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (15): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (16): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (17): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (18): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (19): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (20): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (21): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (22): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (23): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (24): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (25): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (26): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (27): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (28): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (29): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (30): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/12 00:03:56][INFO] visual_prompt:   56: Total Parameters: 110828056	 Gradient Parameters: 25029400
[06/12 00:03:56][INFO] visual_prompt:   58: tuned percent:22.584
[06/12 00:03:56][INFO] visual_prompt:   44: Device used for model: 0
[06/12 00:03:56][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/12 00:03:56][INFO] visual_prompt:   49: Constructing imagenet dataset train...
[06/12 00:04:00][INFO] visual_prompt:  110: Number of images: 1281167
[06/12 00:04:00][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:04:00][INFO] visual_prompt:   78: Loading validation data...
[06/12 00:04:00][INFO] visual_prompt:   49: Constructing imagenet dataset val...
[06/12 00:04:00][INFO] visual_prompt:  110: Number of images: 50000
[06/12 00:04:00][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:04:00][INFO] visual_prompt:   81: Loading test data...
[06/12 00:04:00][INFO] visual_prompt:   83: ...no test data is constructed
[06/12 00:04:00][INFO] visual_prompt:  111: Constructing models...
[06/12 00:04:00][INFO] visual_prompt:  114: Setting up Evalutator...
[06/12 00:04:00][INFO] visual_prompt:  116: Setting up Trainer...
[06/12 00:04:00][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.10.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.10.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.11.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.11.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.12.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.12.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.13.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.13.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.14.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.14.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.15.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.15.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.16.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.16.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.17.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.17.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.18.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.18.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.19.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.19.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.20.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.20.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.21.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.21.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.22.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.22.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.23.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.23.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.24.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.24.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.25.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.25.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.26.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.26.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.27.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.27.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.28.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.28.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.29.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.29.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.30.last_layer.weight: True
[06/12 00:04:00][INFO] visual_prompt:   59: module.head.30.last_layer.bias: True
[06/12 00:04:00][INFO] visual_prompt:  223: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/12 00:04:00][INFO] visual_prompt:  243: Training 1 / 100 epoch, with learning rate 0.0
[06/12 00:04:26][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:04:26][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:04:29][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:04:32][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:04:32][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:04:32][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:04:33][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:04:33][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:04:33][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:04:34][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:04:34][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:04:34][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:04:35][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:04:35][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:04:35][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:04:36][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:04:36][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:04:36][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:04:36][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:04:36][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:04:37][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:04:37][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:04:37][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:04:38][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:04:38][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:04:38][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:04:39][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:04:39][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:04:39][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:04:40][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:04:40][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:04:40][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:04:41][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:04:41][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:04:41][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:04:41][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:04:42][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:04:42][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:04:42][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:04:43][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:04:43][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:04:43][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:04:44][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:04:44][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:04:44][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:04:45][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:04:45][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:04:45][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:04:46][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:04:46][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:04:46][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:04:47][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:04:47][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:04:47][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:04:48][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:04:48][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:04:48][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:04:49][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:04:49][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:04:49][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:04:51][INFO] visual_prompt:  300: Epoch 1 / 100: avg data time: 1.29e+00, avg batch time: 2.4667, average train loss: 6.9269
[06/12 00:05:32][INFO] visual_prompt:  398: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:05:32][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:05:32][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:05:56][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:10:45][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/12 00:10:45][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/12 00:10:45][INFO] visual_prompt:   99: Command line arguments: None
[06/12 00:10:45][INFO] visual_prompt:  108: Training with config:
[06/12 00:10:45][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 31,
                      'NUM_TOKENS': 1550,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 1.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/12 00:10:50][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (10): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (11): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (12): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (13): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (14): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (15): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (16): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (17): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (18): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (19): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (20): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (21): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (22): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (23): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (24): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (25): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (26): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (27): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (28): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (29): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (30): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/12 00:10:50][INFO] visual_prompt:   56: Total Parameters: 110828056	 Gradient Parameters: 25029400
[06/12 00:10:50][INFO] visual_prompt:   58: tuned percent:22.584
[06/12 00:10:51][INFO] visual_prompt:   44: Device used for model: 0
[06/12 00:10:51][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/12 00:10:51][INFO] visual_prompt:   49: Constructing imagenet dataset train...
[06/12 00:10:55][INFO] visual_prompt:  110: Number of images: 1281167
[06/12 00:10:55][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:10:55][INFO] visual_prompt:   78: Loading validation data...
[06/12 00:10:55][INFO] visual_prompt:   49: Constructing imagenet dataset val...
[06/12 00:10:55][INFO] visual_prompt:  110: Number of images: 50000
[06/12 00:10:55][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:10:55][INFO] visual_prompt:   81: Loading test data...
[06/12 00:10:55][INFO] visual_prompt:   83: ...no test data is constructed
[06/12 00:10:55][INFO] visual_prompt:  111: Constructing models...
[06/12 00:10:55][INFO] visual_prompt:  114: Setting up Evalutator...
[06/12 00:10:55][INFO] visual_prompt:  116: Setting up Trainer...
[06/12 00:10:55][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.10.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.10.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.11.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.11.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.12.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.12.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.13.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.13.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.14.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.14.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.15.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.15.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.16.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.16.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.17.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.17.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.18.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.18.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.19.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.19.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.20.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.20.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.21.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.21.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.22.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.22.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.23.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.23.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.24.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.24.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.25.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.25.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.26.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.26.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.27.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.27.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.28.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.28.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.29.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.29.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.30.last_layer.weight: True
[06/12 00:10:55][INFO] visual_prompt:   59: module.head.30.last_layer.bias: True
[06/12 00:10:55][INFO] visual_prompt:  232: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/12 00:10:55][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/12 00:11:21][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:11:21][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:11:25][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:11:26][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:11:26][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:11:26][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:11:27][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:11:27][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:11:27][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:11:27][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:11:27][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:11:28][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:11:28][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:11:28][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:11:29][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:11:29][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:11:29][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:11:30][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:11:30][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:11:30][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:11:31][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:11:31][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:11:31][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:11:31][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:11:32][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:11:32][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:11:32][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:11:33][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:11:33][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:11:33][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:11:34][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:11:34][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:11:34][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:11:35][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:11:35][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:11:35][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:11:36][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:11:36][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:11:36][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:11:37][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:11:37][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:11:37][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:11:38][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:11:38][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:11:38][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:11:39][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:11:39][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:11:39][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:11:40][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:11:40][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:11:40][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:11:40][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:11:40][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:11:41][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:11:41][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:11:41][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:11:42][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:11:42][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:11:42][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:11:43][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:11:44][INFO] visual_prompt:  309: Epoch 1 / 100: avg data time: 1.31e+00, avg batch time: 2.4091, average train loss: 6.9302
[06/12 00:13:06][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/12 00:13:06][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/12 00:13:06][INFO] visual_prompt:   99: Command line arguments: None
[06/12 00:13:06][INFO] visual_prompt:  108: Training with config:
[06/12 00:13:06][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 31,
                      'NUM_TOKENS': 1550,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 1.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/12 00:13:12][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (10): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (11): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (12): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (13): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (14): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (15): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (16): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (17): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (18): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (19): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (20): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (21): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (22): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (23): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (24): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (25): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (26): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (27): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (28): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (29): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (30): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/12 00:13:12][INFO] visual_prompt:   56: Total Parameters: 110828056	 Gradient Parameters: 25029400
[06/12 00:13:12][INFO] visual_prompt:   58: tuned percent:22.584
[06/12 00:13:12][INFO] visual_prompt:   44: Device used for model: 0
[06/12 00:13:12][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/12 00:13:12][INFO] visual_prompt:   49: Constructing imagenet dataset train...
[06/12 00:13:16][INFO] visual_prompt:  110: Number of images: 1281167
[06/12 00:13:16][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:13:17][INFO] visual_prompt:   78: Loading validation data...
[06/12 00:13:17][INFO] visual_prompt:   49: Constructing imagenet dataset val...
[06/12 00:13:17][INFO] visual_prompt:  110: Number of images: 50000
[06/12 00:13:17][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:13:17][INFO] visual_prompt:   81: Loading test data...
[06/12 00:13:17][INFO] visual_prompt:   83: ...no test data is constructed
[06/12 00:13:17][INFO] visual_prompt:  111: Constructing models...
[06/12 00:13:17][INFO] visual_prompt:  114: Setting up Evalutator...
[06/12 00:13:17][INFO] visual_prompt:  116: Setting up Trainer...
[06/12 00:13:17][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.10.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.10.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.11.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.11.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.12.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.12.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.13.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.13.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.14.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.14.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.15.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.15.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.16.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.16.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.17.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.17.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.18.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.18.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.19.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.19.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.20.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.20.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.21.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.21.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.22.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.22.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.23.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.23.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.24.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.24.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.25.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.25.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.26.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.26.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.27.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.27.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.28.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.28.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.29.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.29.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.30.last_layer.weight: True
[06/12 00:13:17][INFO] visual_prompt:   59: module.head.30.last_layer.bias: True
[06/12 00:13:17][INFO] visual_prompt:  232: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/12 00:13:17][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/12 00:13:42][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:13:42][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:13:45][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:13:48][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:13:48][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:13:48][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:13:49][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:13:49][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:13:49][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:13:50][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:13:50][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:13:50][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:13:51][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:13:51][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:13:51][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:13:51][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:13:51][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:13:52][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:13:52][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:13:52][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:13:53][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:13:53][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:13:53][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:13:54][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:13:54][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:13:54][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:13:54][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:13:55][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:13:55][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:13:55][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:13:56][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:13:56][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:13:56][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:13:57][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:13:57][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:13:57][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:13:58][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:13:58][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:13:58][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:13:59][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:13:59][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:13:59][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:14:00][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:14:00][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:14:00][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:14:01][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:14:01][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:14:01][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:14:02][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:14:02][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:14:02][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:14:03][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:14:03][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:14:03][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:14:04][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:14:04][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:14:04][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:14:04][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224])
[06/12 00:14:04][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:14:05][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:14:07][INFO] visual_prompt:  309: Epoch 1 / 100: avg data time: 1.28e+00, avg batch time: 2.4278, average train loss: 6.9301
[06/12 00:16:37][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/12 00:16:37][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/12 00:16:37][INFO] visual_prompt:   99: Command line arguments: None
[06/12 00:16:37][INFO] visual_prompt:  108: Training with config:
[06/12 00:16:37][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 31,
                      'NUM_TOKENS': 1550,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 1.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/12 00:16:43][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (10): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (11): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (12): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (13): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (14): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (15): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (16): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (17): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (18): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (19): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (20): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (21): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (22): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (23): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (24): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (25): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (26): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (27): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (28): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (29): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (30): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/12 00:16:43][INFO] visual_prompt:   56: Total Parameters: 110828056	 Gradient Parameters: 25029400
[06/12 00:16:43][INFO] visual_prompt:   58: tuned percent:22.584
[06/12 00:16:43][INFO] visual_prompt:   44: Device used for model: 0
[06/12 00:16:43][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/12 00:16:43][INFO] visual_prompt:   49: Constructing imagenet dataset train...
[06/12 00:16:48][INFO] visual_prompt:  110: Number of images: 1281167
[06/12 00:16:48][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:16:48][INFO] visual_prompt:   78: Loading validation data...
[06/12 00:16:48][INFO] visual_prompt:   49: Constructing imagenet dataset val...
[06/12 00:16:48][INFO] visual_prompt:  110: Number of images: 50000
[06/12 00:16:48][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:16:48][INFO] visual_prompt:   81: Loading test data...
[06/12 00:16:48][INFO] visual_prompt:   83: ...no test data is constructed
[06/12 00:16:48][INFO] visual_prompt:  111: Constructing models...
[06/12 00:16:48][INFO] visual_prompt:  114: Setting up Evalutator...
[06/12 00:16:48][INFO] visual_prompt:  116: Setting up Trainer...
[06/12 00:16:48][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.10.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.10.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.11.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.11.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.12.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.12.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.13.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.13.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.14.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.14.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.15.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.15.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.16.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.16.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.17.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.17.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.18.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.18.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.19.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.19.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.20.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.20.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.21.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.21.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.22.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.22.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.23.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.23.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.24.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.24.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.25.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.25.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.26.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.26.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.27.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.27.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.28.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.28.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.29.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.29.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.30.last_layer.weight: True
[06/12 00:16:48][INFO] visual_prompt:   59: module.head.30.last_layer.bias: True
[06/12 00:16:48][INFO] visual_prompt:  232: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/12 00:16:48][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/12 00:17:15][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:17:15][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:17:18][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:17:18][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:17:18][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:17:18][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:17:19][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:17:19][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:17:19][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:17:20][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:17:20][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:17:20][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:17:21][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:17:21][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:17:21][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:17:22][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:17:22][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:17:22][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:17:23][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:17:23][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:17:23][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:17:24][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:17:24][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:17:24][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:17:25][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:17:25][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:17:25][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:17:26][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:17:26][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:17:26][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:17:26][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:17:26][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:17:27][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:17:27][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:17:27][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:17:28][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:17:28][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:17:28][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:17:29][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:17:29][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:17:29][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:17:30][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:17:30][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:17:30][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:17:30][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:17:31][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:17:31][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:17:31][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:17:32][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:17:32][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:17:32][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:17:33][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:17:33][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:17:33][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:17:34][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:17:34][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:17:34][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:17:35][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:17:35][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:17:35][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:17:38][INFO] visual_prompt:  309: Epoch 1 / 100: avg data time: 1.35e+00, avg batch time: 2.3905, average train loss: 6.9283
[06/12 00:18:21][INFO] visual_prompt:  407: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:18:21][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:18:21][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:18:44][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:18:45][INFO] visual_prompt:  407: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:18:45][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:18:45][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:19:01][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:19:02][INFO] visual_prompt:  407: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:19:02][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:19:02][INFO] visual_prompt:   99: shape of targets: torch.Size([64])
[06/12 00:24:47][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/12 00:24:47][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/12 00:24:47][INFO] visual_prompt:   99: Command line arguments: None
[06/12 00:24:47][INFO] visual_prompt:  108: Training with config:
[06/12 00:24:47][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 31,
                      'NUM_TOKENS': 1550,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 1.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/12 00:24:53][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (10): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (11): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (12): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (13): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (14): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (15): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (16): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (17): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (18): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (19): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (20): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (21): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (22): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (23): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (24): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (25): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (26): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (27): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (28): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (29): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (30): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/12 00:24:53][INFO] visual_prompt:   56: Total Parameters: 110828056	 Gradient Parameters: 25029400
[06/12 00:24:53][INFO] visual_prompt:   58: tuned percent:22.584
[06/12 00:24:53][INFO] visual_prompt:   44: Device used for model: 0
[06/12 00:24:53][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/12 00:24:53][INFO] visual_prompt:   49: Constructing imagenet dataset train...
[06/12 00:24:58][INFO] visual_prompt:  110: Number of images: 1281167
[06/12 00:24:58][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:24:58][INFO] visual_prompt:   78: Loading validation data...
[06/12 00:24:58][INFO] visual_prompt:   49: Constructing imagenet dataset val...
[06/12 00:24:58][INFO] visual_prompt:  110: Number of images: 50000
[06/12 00:24:58][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:24:58][INFO] visual_prompt:   81: Loading test data...
[06/12 00:24:58][INFO] visual_prompt:   83: ...no test data is constructed
[06/12 00:24:58][INFO] visual_prompt:  111: Constructing models...
[06/12 00:24:58][INFO] visual_prompt:  114: Setting up Evalutator...
[06/12 00:24:58][INFO] visual_prompt:  116: Setting up Trainer...
[06/12 00:24:58][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.10.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.10.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.11.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.11.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.12.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.12.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.13.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.13.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.14.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.14.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.15.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.15.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.16.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.16.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.17.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.17.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.18.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.18.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.19.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.19.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.20.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.20.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.21.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.21.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.22.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.22.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.23.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.23.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.24.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.24.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.25.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.25.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.26.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.26.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.27.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.27.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.28.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.28.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.29.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.29.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.30.last_layer.weight: True
[06/12 00:24:58][INFO] visual_prompt:   59: module.head.30.last_layer.bias: True
[06/12 00:24:58][INFO] visual_prompt:  232: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/12 00:24:58][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/12 00:25:24][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:25:24][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:27:05][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/12 00:27:05][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/12 00:27:05][INFO] visual_prompt:   99: Command line arguments: None
[06/12 00:27:05][INFO] visual_prompt:  108: Training with config:
[06/12 00:27:05][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 31,
                      'NUM_TOKENS': 1550,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 1.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/12 00:27:11][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (10): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (11): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (12): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (13): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (14): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (15): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (16): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (17): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (18): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (19): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (20): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (21): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (22): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (23): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (24): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (25): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (26): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (27): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (28): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (29): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (30): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/12 00:27:11][INFO] visual_prompt:   56: Total Parameters: 110828056	 Gradient Parameters: 25029400
[06/12 00:27:11][INFO] visual_prompt:   58: tuned percent:22.584
[06/12 00:27:11][INFO] visual_prompt:   44: Device used for model: 0
[06/12 00:27:11][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/12 00:27:11][INFO] visual_prompt:   49: Constructing imagenet dataset train...
[06/12 00:27:15][INFO] visual_prompt:  110: Number of images: 1281167
[06/12 00:27:15][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:27:15][INFO] visual_prompt:   78: Loading validation data...
[06/12 00:27:15][INFO] visual_prompt:   49: Constructing imagenet dataset val...
[06/12 00:27:15][INFO] visual_prompt:  110: Number of images: 50000
[06/12 00:27:15][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:27:15][INFO] visual_prompt:   81: Loading test data...
[06/12 00:27:15][INFO] visual_prompt:   83: ...no test data is constructed
[06/12 00:27:15][INFO] visual_prompt:  111: Constructing models...
[06/12 00:27:15][INFO] visual_prompt:  114: Setting up Evalutator...
[06/12 00:27:15][INFO] visual_prompt:  116: Setting up Trainer...
[06/12 00:27:15][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.10.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.10.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.11.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.11.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.12.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.12.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.13.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.13.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.14.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.14.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.15.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.15.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.16.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.16.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.17.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.17.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.18.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.18.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.19.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.19.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.20.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.20.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.21.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.21.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.22.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.22.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.23.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.23.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.24.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.24.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.25.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.25.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.26.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.26.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.27.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.27.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.28.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.28.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.29.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.29.last_layer.bias: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.30.last_layer.weight: True
[06/12 00:27:15][INFO] visual_prompt:   59: module.head.30.last_layer.bias: True
[06/12 00:27:16][INFO] visual_prompt:  232: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/12 00:27:16][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/12 00:27:42][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:27:42][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:28:53][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/12 00:28:53][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/12 00:28:53][INFO] visual_prompt:   99: Command line arguments: None
[06/12 00:28:53][INFO] visual_prompt:  108: Training with config:
[06/12 00:28:53][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 31,
                      'NUM_TOKENS': 1550,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 1.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/12 00:28:59][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (10): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (11): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (12): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (13): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (14): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (15): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (16): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (17): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (18): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (19): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (20): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (21): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (22): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (23): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (24): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (25): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (26): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (27): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (28): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (29): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (30): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/12 00:28:59][INFO] visual_prompt:   56: Total Parameters: 110828056	 Gradient Parameters: 25029400
[06/12 00:28:59][INFO] visual_prompt:   58: tuned percent:22.584
[06/12 00:29:00][INFO] visual_prompt:   44: Device used for model: 0
[06/12 00:29:00][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/12 00:29:00][INFO] visual_prompt:   49: Constructing imagenet dataset train...
[06/12 00:29:04][INFO] visual_prompt:  110: Number of images: 1281167
[06/12 00:29:04][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:29:04][INFO] visual_prompt:   78: Loading validation data...
[06/12 00:29:04][INFO] visual_prompt:   49: Constructing imagenet dataset val...
[06/12 00:29:04][INFO] visual_prompt:  110: Number of images: 50000
[06/12 00:29:04][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:29:04][INFO] visual_prompt:   81: Loading test data...
[06/12 00:29:04][INFO] visual_prompt:   83: ...no test data is constructed
[06/12 00:29:04][INFO] visual_prompt:  111: Constructing models...
[06/12 00:29:04][INFO] visual_prompt:  114: Setting up Evalutator...
[06/12 00:29:04][INFO] visual_prompt:  116: Setting up Trainer...
[06/12 00:29:04][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.10.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.10.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.11.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.11.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.12.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.12.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.13.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.13.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.14.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.14.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.15.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.15.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.16.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.16.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.17.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.17.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.18.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.18.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.19.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.19.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.20.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.20.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.21.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.21.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.22.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.22.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.23.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.23.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.24.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.24.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.25.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.25.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.26.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.26.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.27.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.27.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.28.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.28.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.29.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.29.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.30.last_layer.weight: True
[06/12 00:29:04][INFO] visual_prompt:   59: module.head.30.last_layer.bias: True
[06/12 00:29:04][INFO] visual_prompt:  232: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/12 00:29:04][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/12 00:29:30][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:29:30][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:29:33][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:32:42][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/12 00:32:42][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/12 00:32:42][INFO] visual_prompt:   99: Command line arguments: None
[06/12 00:32:42][INFO] visual_prompt:  108: Training with config:
[06/12 00:32:42][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 31,
                      'NUM_TOKENS': 1550,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 1.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/12 00:32:47][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (10): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (11): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (12): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (13): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (14): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (15): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (16): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (17): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (18): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (19): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (20): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (21): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (22): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (23): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (24): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (25): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (26): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (27): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (28): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (29): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (30): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/12 00:32:47][INFO] visual_prompt:   56: Total Parameters: 110828056	 Gradient Parameters: 25029400
[06/12 00:32:47][INFO] visual_prompt:   58: tuned percent:22.584
[06/12 00:32:48][INFO] visual_prompt:   44: Device used for model: 0
[06/12 00:32:48][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/12 00:32:48][INFO] visual_prompt:   49: Constructing imagenet dataset train...
[06/12 00:32:51][INFO] visual_prompt:  110: Number of images: 1281167
[06/12 00:32:51][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:32:52][INFO] visual_prompt:   78: Loading validation data...
[06/12 00:32:52][INFO] visual_prompt:   49: Constructing imagenet dataset val...
[06/12 00:32:52][INFO] visual_prompt:  110: Number of images: 50000
[06/12 00:32:52][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:32:52][INFO] visual_prompt:   81: Loading test data...
[06/12 00:32:52][INFO] visual_prompt:   83: ...no test data is constructed
[06/12 00:32:52][INFO] visual_prompt:  111: Constructing models...
[06/12 00:32:52][INFO] visual_prompt:  114: Setting up Evalutator...
[06/12 00:32:52][INFO] visual_prompt:  116: Setting up Trainer...
[06/12 00:32:52][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.10.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.10.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.11.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.11.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.12.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.12.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.13.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.13.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.14.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.14.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.15.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.15.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.16.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.16.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.17.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.17.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.18.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.18.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.19.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.19.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.20.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.20.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.21.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.21.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.22.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.22.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.23.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.23.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.24.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.24.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.25.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.25.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.26.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.26.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.27.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.27.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.28.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.28.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.29.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.29.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.30.last_layer.weight: True
[06/12 00:32:52][INFO] visual_prompt:   59: module.head.30.last_layer.bias: True
[06/12 00:32:52][INFO] visual_prompt:  232: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/12 00:32:52][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/12 00:33:18][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:33:18][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:33:21][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:35:08][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/12 00:35:08][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/12 00:35:08][INFO] visual_prompt:   99: Command line arguments: None
[06/12 00:35:08][INFO] visual_prompt:  108: Training with config:
[06/12 00:35:08][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 31,
                      'NUM_TOKENS': 1550,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 1.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/12 00:35:14][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (10): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (11): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (12): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (13): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (14): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (15): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (16): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (17): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (18): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (19): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (20): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (21): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (22): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (23): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (24): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (25): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (26): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (27): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (28): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (29): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (30): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/12 00:35:14][INFO] visual_prompt:   56: Total Parameters: 110828056	 Gradient Parameters: 25029400
[06/12 00:35:14][INFO] visual_prompt:   58: tuned percent:22.584
[06/12 00:35:14][INFO] visual_prompt:   44: Device used for model: 0
[06/12 00:35:14][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/12 00:35:14][INFO] visual_prompt:   49: Constructing imagenet dataset train...
[06/12 00:35:19][INFO] visual_prompt:  110: Number of images: 1281167
[06/12 00:35:19][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:35:19][INFO] visual_prompt:   78: Loading validation data...
[06/12 00:35:19][INFO] visual_prompt:   49: Constructing imagenet dataset val...
[06/12 00:35:19][INFO] visual_prompt:  110: Number of images: 50000
[06/12 00:35:19][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:35:19][INFO] visual_prompt:   81: Loading test data...
[06/12 00:35:19][INFO] visual_prompt:   83: ...no test data is constructed
[06/12 00:35:19][INFO] visual_prompt:  111: Constructing models...
[06/12 00:35:19][INFO] visual_prompt:  114: Setting up Evalutator...
[06/12 00:35:19][INFO] visual_prompt:  116: Setting up Trainer...
[06/12 00:35:19][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.10.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.10.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.11.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.11.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.12.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.12.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.13.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.13.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.14.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.14.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.15.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.15.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.16.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.16.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.17.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.17.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.18.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.18.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.19.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.19.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.20.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.20.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.21.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.21.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.22.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.22.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.23.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.23.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.24.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.24.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.25.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.25.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.26.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.26.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.27.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.27.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.28.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.28.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.29.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.29.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.30.last_layer.weight: True
[06/12 00:35:19][INFO] visual_prompt:   59: module.head.30.last_layer.bias: True
[06/12 00:35:19][INFO] visual_prompt:  229: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/12 00:35:19][INFO] visual_prompt:  249: Training 1 / 100 epoch, with learning rate 0.0
[06/12 00:35:45][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:35:45][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:35:48][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:35:48][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:35:48][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:35:49][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:35:49][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:35:49][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:35:50][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:35:50][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:35:50][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:35:50][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:35:51][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:35:51][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:35:51][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:35:52][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:35:52][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:35:52][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:35:53][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:35:53][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:35:53][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:35:54][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:35:54][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:35:54][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:35:55][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:35:55][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:35:55][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:35:56][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:35:56][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:35:56][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:35:57][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:35:57][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:35:57][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:35:58][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:35:58][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:35:58][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:35:58][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:35:58][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:35:59][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:35:59][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:35:59][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:36:00][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:36:00][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:36:00][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:36:01][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:36:01][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:36:01][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:36:02][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:36:02][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:36:02][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:36:03][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:36:03][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:36:03][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:36:04][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:36:04][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:36:04][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:36:05][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:36:05][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:36:05][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:36:06][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:36:07][INFO] visual_prompt:  306: Epoch 1 / 100: avg data time: 1.30e+00, avg batch time: 2.3629, average train loss: 6.9299
[06/12 00:36:52][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:36:53][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:36:53][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:37:14][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:37:57][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/12 00:37:57][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/12 00:37:57][INFO] visual_prompt:   99: Command line arguments: None
[06/12 00:37:57][INFO] visual_prompt:  108: Training with config:
[06/12 00:37:57][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 31,
                      'NUM_TOKENS': 1550,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 1.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/12 00:38:02][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (10): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (11): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (12): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (13): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (14): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (15): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (16): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (17): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (18): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (19): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (20): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (21): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (22): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (23): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (24): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (25): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (26): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (27): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (28): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (29): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (30): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/12 00:38:02][INFO] visual_prompt:   56: Total Parameters: 110828056	 Gradient Parameters: 25029400
[06/12 00:38:02][INFO] visual_prompt:   58: tuned percent:22.584
[06/12 00:38:02][INFO] visual_prompt:   44: Device used for model: 0
[06/12 00:38:02][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/12 00:38:02][INFO] visual_prompt:   49: Constructing imagenet dataset train...
[06/12 00:38:06][INFO] visual_prompt:  110: Number of images: 1281167
[06/12 00:38:06][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:38:06][INFO] visual_prompt:   78: Loading validation data...
[06/12 00:38:06][INFO] visual_prompt:   49: Constructing imagenet dataset val...
[06/12 00:38:06][INFO] visual_prompt:  110: Number of images: 50000
[06/12 00:38:06][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:38:06][INFO] visual_prompt:   81: Loading test data...
[06/12 00:38:06][INFO] visual_prompt:   83: ...no test data is constructed
[06/12 00:38:06][INFO] visual_prompt:  111: Constructing models...
[06/12 00:38:06][INFO] visual_prompt:  114: Setting up Evalutator...
[06/12 00:38:06][INFO] visual_prompt:  116: Setting up Trainer...
[06/12 00:38:06][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.10.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.10.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.11.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.11.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.12.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.12.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.13.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.13.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.14.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.14.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.15.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.15.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.16.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.16.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.17.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.17.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.18.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.18.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.19.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.19.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.20.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.20.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.21.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.21.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.22.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.22.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.23.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.23.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.24.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.24.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.25.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.25.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.26.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.26.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.27.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.27.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.28.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.28.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.29.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.29.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.30.last_layer.weight: True
[06/12 00:38:06][INFO] visual_prompt:   59: module.head.30.last_layer.bias: True
[06/12 00:38:06][INFO] visual_prompt:  229: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/12 00:38:06][INFO] visual_prompt:  249: Training 1 / 100 epoch, with learning rate 0.0
[06/12 00:38:33][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:38:33][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:38:37][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:38:37][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:38:37][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:38:38][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:38:38][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:38:38][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:38:39][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:38:39][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:38:39][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:38:40][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:38:40][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:38:40][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:38:41][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:38:41][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:38:41][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:38:42][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:38:42][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:38:42][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:38:43][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:38:43][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:38:43][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:38:44][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:38:44][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:38:44][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:38:45][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:38:45][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:38:45][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:38:46][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:38:46][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:38:46][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:38:47][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:38:47][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:38:47][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:38:47][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:38:48][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:38:48][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:38:48][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:38:49][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:38:49][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:38:49][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:38:50][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:38:50][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:38:50][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:38:51][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:38:51][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:38:51][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:38:52][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:38:52][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:38:52][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:38:53][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:38:53][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:38:53][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:38:54][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:38:54][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:38:54][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:38:55][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:38:55][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64])
[06/12 00:38:55][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:38:57][INFO] visual_prompt:  306: Epoch 1 / 100: avg data time: 1.35e+00, avg batch time: 2.4540, average train loss: 6.9312
[06/12 00:39:39][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:39:39][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:39:39][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:40:03][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:40:03][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:40:03][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:40:03][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:40:19][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:40:19][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:40:19][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:40:19][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:40:36][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:40:36][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:40:36][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:40:36][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:40:52][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:40:52][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:40:52][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:40:52][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:41:08][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:41:08][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:41:08][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:41:08][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:41:25][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:41:26][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:41:26][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:41:26][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:42:57][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/12 00:42:57][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/12 00:42:57][INFO] visual_prompt:   99: Command line arguments: None
[06/12 00:42:57][INFO] visual_prompt:  108: Training with config:
[06/12 00:42:57][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 31,
                      'NUM_TOKENS': 1550,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 1.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/12 00:43:02][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (10): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (11): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (12): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (13): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (14): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (15): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (16): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (17): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (18): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (19): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (20): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (21): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (22): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (23): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (24): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (25): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (26): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (27): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (28): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (29): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (30): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/12 00:43:02][INFO] visual_prompt:   56: Total Parameters: 110828056	 Gradient Parameters: 25029400
[06/12 00:43:02][INFO] visual_prompt:   58: tuned percent:22.584
[06/12 00:43:03][INFO] visual_prompt:   44: Device used for model: 0
[06/12 00:43:03][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/12 00:43:03][INFO] visual_prompt:   49: Constructing imagenet dataset train...
[06/12 00:43:07][INFO] visual_prompt:  110: Number of images: 1281167
[06/12 00:43:07][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:43:07][INFO] visual_prompt:   78: Loading validation data...
[06/12 00:43:07][INFO] visual_prompt:   49: Constructing imagenet dataset val...
[06/12 00:43:07][INFO] visual_prompt:  110: Number of images: 50000
[06/12 00:43:07][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:43:07][INFO] visual_prompt:   81: Loading test data...
[06/12 00:43:07][INFO] visual_prompt:   83: ...no test data is constructed
[06/12 00:43:07][INFO] visual_prompt:  111: Constructing models...
[06/12 00:43:07][INFO] visual_prompt:  114: Setting up Evalutator...
[06/12 00:43:07][INFO] visual_prompt:  116: Setting up Trainer...
[06/12 00:43:07][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.10.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.10.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.11.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.11.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.12.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.12.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.13.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.13.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.14.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.14.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.15.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.15.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.16.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.16.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.17.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.17.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.18.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.18.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.19.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.19.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.20.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.20.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.21.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.21.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.22.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.22.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.23.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.23.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.24.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.24.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.25.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.25.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.26.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.26.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.27.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.27.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.28.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.28.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.29.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.29.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.30.last_layer.weight: True
[06/12 00:43:07][INFO] visual_prompt:   59: module.head.30.last_layer.bias: True
[06/12 00:43:07][INFO] visual_prompt:  229: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/12 00:43:07][INFO] visual_prompt:  249: Training 1 / 100 epoch, with learning rate 0.0
[06/12 00:43:33][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:43:33][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:43:36][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:43:39][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:43:39][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:43:40][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:43:40][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:43:40][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:43:41][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:43:41][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:43:41][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:43:41][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:43:42][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:43:42][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:43:42][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:43:43][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:43:43][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:43:43][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:43:44][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:43:44][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:43:44][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:43:45][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:43:45][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:43:45][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:43:46][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:43:46][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:43:46][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:43:47][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:43:47][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:43:47][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:43:48][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:43:48][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:43:48][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:43:49][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:43:49][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:43:49][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:43:49][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:43:49][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:43:50][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:43:50][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:43:50][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:43:51][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:43:51][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:43:51][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:43:52][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:43:52][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:43:52][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:43:53][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:43:53][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:43:53][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:43:54][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:43:54][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:43:54][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:43:55][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:43:55][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:43:55][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:43:56][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:43:56][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:43:56][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:43:57][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:43:58][INFO] visual_prompt:  306: Epoch 1 / 100: avg data time: 1.30e+00, avg batch time: 2.4934, average train loss: 6.9340
[06/12 00:44:38][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:44:38][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:44:38][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:45:01][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:45:01][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:45:01][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:45:01][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:45:19][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:45:20][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:45:20][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:45:20][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:45:37][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:45:37][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:45:37][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:45:37][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:45:53][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:45:53][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:45:53][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:45:53][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:46:10][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:46:10][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:46:10][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:46:10][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:46:26][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:46:26][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:46:26][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:46:26][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:46:42][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:46:42][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:46:42][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:46:42][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:46:59][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:46:59][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:46:59][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:46:59][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:47:15][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:47:15][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:47:15][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:47:15][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:47:31][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:47:31][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:47:31][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:47:31][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:47:48][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:47:48][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:47:48][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:47:48][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:48:04][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:48:04][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:48:04][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:48:04][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:48:20][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:48:20][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:48:20][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:48:20][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:48:37][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:48:37][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:48:37][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:48:37][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:48:53][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:48:54][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:48:54][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:48:54][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:49:10][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:49:10][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:49:10][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:49:10][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:49:26][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:49:26][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:49:26][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:49:26][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:49:43][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:49:43][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:49:43][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:49:43][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:49:59][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:49:59][INFO] visual_prompt:  404: during eval: torch.Size([64, 3, 224, 224])
[06/12 00:49:59][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 31, 3, 224, 224])
[06/12 00:49:59][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 31])
[06/12 00:50:15][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 31, 768]), targets: torch.Size([64])
[06/12 00:50:17][INFO] visual_prompt:  463: Inference (val):avg data time: 1.92e-04, avg batch time: 16.8374, average loss: 214.9015
[06/12 00:50:17][INFO] visual_prompt:  480: Saved invariances for val_imagenet at output_ce/val_imagenet_invariances.json
[06/12 00:50:18][INFO] visual_prompt:  249: Training 2 / 100 epoch, with learning rate 0.1
[06/12 00:50:44][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:50:44][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:50:45][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:50:53][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:50:53][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:50:54][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:50:54][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:50:54][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:50:54][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:50:55][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:50:55][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:50:55][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:50:56][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:50:56][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:50:56][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:50:57][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:50:57][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:50:57][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:50:58][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:50:58][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:50:58][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:50:59][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:50:59][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:50:59][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:51:00][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:51:00][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:51:00][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:51:01][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:51:01][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:51:01][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:51:02][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:51:02][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:51:02][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:51:03][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:51:03][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:51:03][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:51:03][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:51:03][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:51:04][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:51:04][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:51:04][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:51:05][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:51:05][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:51:05][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:51:06][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:51:06][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:51:06][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:51:07][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:51:07][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:51:07][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:51:08][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:51:08][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:51:08][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:51:09][INFO] visual_prompt:  137: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/12 00:51:09][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/12 00:51:09][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/12 00:51:55][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/12 00:51:55][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/12 00:51:55][INFO] visual_prompt:   99: Command line arguments: None
[06/12 00:51:55][INFO] visual_prompt:  108: Training with config:
[06/12 00:51:55][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': False,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 31,
                      'NUM_TOKENS': 1550,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_ce',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 1.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'cross_entropy',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/12 00:52:01][INFO] visual_prompt:   56: Total Parameters: 110828056	 Gradient Parameters: 25029400
[06/12 00:52:01][INFO] visual_prompt:   58: tuned percent:22.584
[06/12 00:52:01][INFO] visual_prompt:   44: Device used for model: 0
[06/12 00:52:01][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/12 00:52:01][INFO] visual_prompt:   49: Constructing imagenet dataset train...
[06/12 00:52:05][INFO] visual_prompt:  110: Number of images: 1281167
[06/12 00:52:05][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:52:05][INFO] visual_prompt:   78: Loading validation data...
[06/12 00:52:05][INFO] visual_prompt:   49: Constructing imagenet dataset val...
[06/12 00:52:05][INFO] visual_prompt:  110: Number of images: 50000
[06/12 00:52:05][INFO] visual_prompt:  111: Number of classes: 1000
[06/12 00:52:05][INFO] visual_prompt:   81: Loading test data...
[06/12 00:52:05][INFO] visual_prompt:   83: ...no test data is constructed
[06/12 00:52:05][INFO] visual_prompt:  111: Constructing models...
[06/12 00:52:05][INFO] visual_prompt:  114: Setting up Evalutator...
[06/12 00:52:05][INFO] visual_prompt:  116: Setting up Trainer...
[06/12 00:52:05][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.10.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.10.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.11.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.11.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.12.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.12.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.13.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.13.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.14.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.14.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.15.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.15.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.16.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.16.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.17.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.17.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.18.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.18.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.19.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.19.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.20.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.20.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.21.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.21.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.22.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.22.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.23.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.23.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.24.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.24.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.25.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.25.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.26.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.26.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.27.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.27.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.28.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.28.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.29.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.29.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.30.last_layer.weight: True
[06/12 00:52:05][INFO] visual_prompt:   59: module.head.30.last_layer.bias: True
[06/12 00:52:05][INFO] visual_prompt:  229: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/12 00:52:05][INFO] visual_prompt:  249: Training 1 / 100 epoch, with learning rate 0.0
[06/12 00:54:09][INFO] visual_prompt:  293: 	Training 100/5004. train loss: 6.8955,	0.9575 s / batch. (data: 5.10e-04). ETA=5 days, 13:03:37, max mem: 15.3 GB 
[06/12 00:55:45][INFO] visual_prompt:  293: 	Training 200/5004. train loss: 6.9352,	0.9659 s / batch. (data: 3.62e-04). ETA=5 days, 14:12:12, max mem: 15.3 GB 
[06/12 00:57:21][INFO] visual_prompt:  293: 	Training 300/5004. train loss: 6.9327,	0.9670 s / batch. (data: 4.89e-04). ETA=5 days, 14:20:16, max mem: 15.3 GB 
[06/12 00:58:56][INFO] visual_prompt:  293: 	Training 400/5004. train loss: 6.9214,	0.9667 s / batch. (data: 4.46e-04). ETA=5 days, 14:15:26, max mem: 15.3 GB 
[06/12 01:00:32][INFO] visual_prompt:  293: 	Training 500/5004. train loss: 6.9290,	0.9510 s / batch. (data: 4.88e-04). ETA=5 days, 12:03:41, max mem: 15.3 GB 
[06/12 01:02:08][INFO] visual_prompt:  293: 	Training 600/5004. train loss: 6.8923,	0.9501 s / batch. (data: 3.17e-03). ETA=5 days, 11:54:36, max mem: 15.3 GB 
[06/12 01:03:44][INFO] visual_prompt:  293: 	Training 700/5004. train loss: 6.9595,	0.9629 s / batch. (data: 3.32e-04). ETA=5 days, 13:39:26, max mem: 15.3 GB 
[06/12 01:05:20][INFO] visual_prompt:  293: 	Training 800/5004. train loss: 6.9538,	0.9678 s / batch. (data: 3.58e-04). ETA=5 days, 14:18:49, max mem: 15.3 GB 
[06/12 01:06:56][INFO] visual_prompt:  293: 	Training 900/5004. train loss: 6.9797,	0.9639 s / batch. (data: 4.62e-03). ETA=5 days, 13:44:09, max mem: 15.3 GB 
[06/12 01:08:31][INFO] visual_prompt:  293: 	Training 1000/5004. train loss: 6.9114,	0.9630 s / batch. (data: 3.37e-04). ETA=5 days, 13:35:37, max mem: 15.3 GB 
[06/12 01:10:07][INFO] visual_prompt:  293: 	Training 1100/5004. train loss: 6.8694,	0.9624 s / batch. (data: 3.56e-04). ETA=5 days, 13:28:28, max mem: 15.3 GB 
[06/12 01:11:43][INFO] visual_prompt:  293: 	Training 1200/5004. train loss: 6.8978,	0.9484 s / batch. (data: 4.02e-04). ETA=5 days, 11:30:52, max mem: 15.3 GB 
[06/12 01:13:19][INFO] visual_prompt:  293: 	Training 1300/5004. train loss: 6.9400,	0.9546 s / batch. (data: 3.25e-04). ETA=5 days, 12:20:49, max mem: 15.3 GB 
[06/12 01:14:55][INFO] visual_prompt:  293: 	Training 1400/5004. train loss: 6.9359,	0.9621 s / batch. (data: 2.16e-03). ETA=5 days, 13:21:43, max mem: 15.3 GB 
[06/12 01:16:30][INFO] visual_prompt:  293: 	Training 1500/5004. train loss: 6.9526,	0.9596 s / batch. (data: 5.35e-04). ETA=5 days, 12:59:05, max mem: 15.3 GB 
[06/12 01:18:06][INFO] visual_prompt:  293: 	Training 1600/5004. train loss: 6.8980,	0.9540 s / batch. (data: 3.41e-04). ETA=5 days, 12:11:14, max mem: 15.3 GB 
[06/12 01:19:42][INFO] visual_prompt:  293: 	Training 1700/5004. train loss: 6.8966,	0.9573 s / batch. (data: 4.13e-03). ETA=5 days, 12:36:24, max mem: 15.3 GB 
[06/12 01:21:18][INFO] visual_prompt:  293: 	Training 1800/5004. train loss: 6.9027,	0.9417 s / batch. (data: 2.69e-04). ETA=5 days, 10:25:39, max mem: 15.3 GB 
[06/12 01:22:54][INFO] visual_prompt:  293: 	Training 1900/5004. train loss: 6.9635,	0.9660 s / batch. (data: 3.17e-04). ETA=5 days, 13:45:49, max mem: 15.3 GB 
[06/12 01:24:30][INFO] visual_prompt:  293: 	Training 2000/5004. train loss: 6.9125,	0.9567 s / batch. (data: 3.90e-04). ETA=5 days, 12:27:09, max mem: 15.3 GB 
[06/12 01:26:05][INFO] visual_prompt:  293: 	Training 2100/5004. train loss: 6.9701,	0.9474 s / batch. (data: 3.66e-04). ETA=5 days, 11:08:30, max mem: 15.3 GB 
[06/12 01:27:41][INFO] visual_prompt:  293: 	Training 2200/5004. train loss: 6.9058,	0.9497 s / batch. (data: 2.91e-04). ETA=5 days, 11:25:41, max mem: 15.3 GB 
[06/12 01:29:17][INFO] visual_prompt:  293: 	Training 2300/5004. train loss: 6.9462,	0.9578 s / batch. (data: 5.74e-04). ETA=5 days, 12:31:44, max mem: 15.3 GB 
[06/12 01:30:53][INFO] visual_prompt:  293: 	Training 2400/5004. train loss: 6.9693,	0.9584 s / batch. (data: 3.68e-04). ETA=5 days, 12:34:35, max mem: 15.3 GB 
[06/12 01:32:29][INFO] visual_prompt:  293: 	Training 2500/5004. train loss: 6.9896,	0.9601 s / batch. (data: 4.17e-03). ETA=5 days, 12:47:02, max mem: 15.3 GB 
[06/12 01:34:05][INFO] visual_prompt:  293: 	Training 2600/5004. train loss: 6.9063,	0.9579 s / batch. (data: 4.69e-04). ETA=5 days, 12:27:04, max mem: 15.3 GB 
[06/12 01:35:41][INFO] visual_prompt:  293: 	Training 2700/5004. train loss: 6.9107,	0.9524 s / batch. (data: 5.01e-04). ETA=5 days, 11:40:03, max mem: 15.3 GB 
[06/12 01:37:17][INFO] visual_prompt:  293: 	Training 2800/5004. train loss: 6.9202,	0.9488 s / batch. (data: 3.69e-04). ETA=5 days, 11:08:34, max mem: 15.3 GB 
[06/12 01:38:53][INFO] visual_prompt:  293: 	Training 2900/5004. train loss: 7.0129,	0.9598 s / batch. (data: 3.08e-04). ETA=5 days, 12:38:12, max mem: 15.3 GB 
[06/12 01:40:29][INFO] visual_prompt:  293: 	Training 3000/5004. train loss: 6.9696,	0.9525 s / batch. (data: 2.68e-04). ETA=5 days, 11:36:04, max mem: 15.3 GB 
[06/12 01:42:05][INFO] visual_prompt:  293: 	Training 3100/5004. train loss: 6.9668,	0.9730 s / batch. (data: 3.31e-04). ETA=5 days, 14:24:22, max mem: 15.3 GB 
[06/12 01:43:41][INFO] visual_prompt:  293: 	Training 3200/5004. train loss: 6.9597,	0.9494 s / batch. (data: 3.64e-04). ETA=5 days, 11:07:39, max mem: 15.3 GB 
[06/12 01:45:17][INFO] visual_prompt:  293: 	Training 3300/5004. train loss: 6.9540,	0.9523 s / batch. (data: 4.51e-04). ETA=5 days, 11:29:47, max mem: 15.3 GB 
[06/12 01:46:53][INFO] visual_prompt:  293: 	Training 3400/5004. train loss: 6.9101,	0.9639 s / batch. (data: 5.49e-03). ETA=5 days, 13:04:27, max mem: 15.3 GB 
[06/12 01:48:29][INFO] visual_prompt:  293: 	Training 3500/5004. train loss: 6.8941,	0.9508 s / batch. (data: 2.92e-04). ETA=5 days, 11:14:10, max mem: 15.3 GB 
[06/12 01:50:05][INFO] visual_prompt:  293: 	Training 3600/5004. train loss: 6.9642,	0.9501 s / batch. (data: 3.15e-04). ETA=5 days, 11:07:04, max mem: 15.3 GB 
[06/12 01:51:40][INFO] visual_prompt:  293: 	Training 3700/5004. train loss: 6.9453,	0.9501 s / batch. (data: 3.36e-04). ETA=5 days, 11:05:23, max mem: 15.3 GB 
[06/12 01:53:16][INFO] visual_prompt:  293: 	Training 3800/5004. train loss: 6.9570,	0.9613 s / batch. (data: 4.09e-04). ETA=5 days, 12:36:37, max mem: 15.3 GB 
[06/12 01:54:52][INFO] visual_prompt:  293: 	Training 3900/5004. train loss: 6.9077,	0.9505 s / batch. (data: 3.02e-04). ETA=5 days, 11:05:25, max mem: 15.3 GB 
[06/12 01:56:28][INFO] visual_prompt:  293: 	Training 4000/5004. train loss: 6.9490,	0.9585 s / batch. (data: 3.71e-04). ETA=5 days, 12:09:55, max mem: 15.3 GB 
[06/12 01:58:04][INFO] visual_prompt:  293: 	Training 4100/5004. train loss: 6.9409,	0.9446 s / batch. (data: 2.94e-04). ETA=5 days, 10:13:41, max mem: 15.3 GB 
[06/12 01:59:40][INFO] visual_prompt:  293: 	Training 4200/5004. train loss: 6.9246,	0.9467 s / batch. (data: 3.64e-04). ETA=5 days, 10:29:18, max mem: 15.3 GB 
[06/12 02:01:16][INFO] visual_prompt:  293: 	Training 4300/5004. train loss: 6.9527,	0.9451 s / batch. (data: 4.15e-04). ETA=5 days, 10:14:00, max mem: 15.3 GB 
[06/12 02:02:51][INFO] visual_prompt:  293: 	Training 4400/5004. train loss: 6.9312,	0.9728 s / batch. (data: 3.81e-04). ETA=5 days, 14:01:41, max mem: 15.3 GB 
[06/12 02:04:27][INFO] visual_prompt:  293: 	Training 4500/5004. train loss: 6.9354,	0.9595 s / batch. (data: 3.14e-04). ETA=5 days, 12:10:00, max mem: 15.3 GB 
[06/12 02:06:03][INFO] visual_prompt:  293: 	Training 4600/5004. train loss: 6.9818,	0.9604 s / batch. (data: 4.42e-04). ETA=5 days, 12:15:45, max mem: 15.3 GB 
[06/12 02:07:39][INFO] visual_prompt:  293: 	Training 4700/5004. train loss: 6.8617,	0.9560 s / batch. (data: 3.80e-04). ETA=5 days, 11:38:03, max mem: 15.3 GB 
[06/12 02:09:15][INFO] visual_prompt:  293: 	Training 4800/5004. train loss: 6.9203,	0.9549 s / batch. (data: 3.15e-04). ETA=5 days, 11:27:51, max mem: 15.3 GB 
[06/12 02:10:50][INFO] visual_prompt:  293: 	Training 4900/5004. train loss: 6.9013,	0.9567 s / batch. (data: 3.88e-04). ETA=5 days, 11:41:02, max mem: 15.3 GB 
[06/12 02:12:26][INFO] visual_prompt:  293: 	Training 5000/5004. train loss: 6.9282,	0.9468 s / batch. (data: 1.41e-04). ETA=5 days, 10:17:14, max mem: 15.3 GB 
[06/12 02:12:31][INFO] visual_prompt:  306: Epoch 1 / 100: avg data time: 5.73e-03, avg batch time: 0.9642, average train loss: 6.9298
[06/12 02:40:27][INFO] visual_prompt:  426: 	Test 100/196. loss: 214.651, 16.2478 s / batch. (data: 2.10e-04)max mem: 15.25483 GB 
[06/12 03:06:23][INFO] visual_prompt:  463: Inference (val):avg data time: 2.34e-04, avg batch time: 16.2568, average loss: 214.8332
[06/12 03:06:23][INFO] visual_prompt:  480: Saved invariances for val_imagenet at output_ce/val_imagenet_invariances.json
[06/12 03:06:24][INFO] visual_prompt:  249: Training 2 / 100 epoch, with learning rate 0.1
[06/12 03:09:00][INFO] visual_prompt:  293: 	Training 100/5004. train loss: 6.8486,	0.9489 s / batch. (data: 3.31e-04). ETA=5 days, 10:33:11, max mem: 15.3 GB 
[06/12 03:10:36][INFO] visual_prompt:  293: 	Training 200/5004. train loss: 6.8985,	0.9568 s / batch. (data: 2.82e-04). ETA=5 days, 11:36:36, max mem: 15.3 GB 
[06/12 03:12:11][INFO] visual_prompt:  293: 	Training 300/5004. train loss: 6.8443,	0.9555 s / batch. (data: 4.26e-04). ETA=5 days, 11:24:33, max mem: 15.3 GB 
[06/12 03:13:47][INFO] visual_prompt:  293: 	Training 400/5004. train loss: 6.8594,	0.9651 s / batch. (data: 3.62e-04). ETA=5 days, 12:41:38, max mem: 15.3 GB 
[06/12 03:15:22][INFO] visual_prompt:  293: 	Training 500/5004. train loss: 6.7994,	0.9465 s / batch. (data: 3.42e-04). ETA=5 days, 10:07:15, max mem: 15.3 GB 
[06/12 03:16:57][INFO] visual_prompt:  293: 	Training 600/5004. train loss: 6.7915,	0.9482 s / batch. (data: 4.03e-04). ETA=5 days, 10:19:37, max mem: 15.3 GB 
[06/12 03:18:33][INFO] visual_prompt:  293: 	Training 700/5004. train loss: 6.7951,	0.9587 s / batch. (data: 2.90e-04). ETA=5 days, 11:44:42, max mem: 15.3 GB 
[06/12 03:20:08][INFO] visual_prompt:  293: 	Training 800/5004. train loss: 6.7956,	0.9640 s / batch. (data: 3.30e-04). ETA=5 days, 12:26:27, max mem: 15.3 GB 
[06/12 03:21:44][INFO] visual_prompt:  293: 	Training 900/5004. train loss: 6.7632,	0.9477 s / batch. (data: 3.72e-04). ETA=5 days, 10:10:54, max mem: 15.3 GB 
[06/12 03:23:19][INFO] visual_prompt:  293: 	Training 1000/5004. train loss: 6.7552,	0.9585 s / batch. (data: 3.01e-04). ETA=5 days, 11:37:56, max mem: 15.3 GB 
[06/12 03:24:54][INFO] visual_prompt:  293: 	Training 1100/5004. train loss: 6.7483,	0.9530 s / batch. (data: 3.31e-04). ETA=5 days, 10:50:49, max mem: 15.3 GB 
[06/12 03:26:30][INFO] visual_prompt:  293: 	Training 1200/5004. train loss: 6.7352,	0.9478 s / batch. (data: 3.56e-04). ETA=5 days, 10:06:59, max mem: 15.3 GB 
[06/12 03:28:05][INFO] visual_prompt:  293: 	Training 1300/5004. train loss: 6.6933,	0.9503 s / batch. (data: 4.98e-04). ETA=5 days, 10:26:02, max mem: 15.3 GB 
[06/12 03:29:40][INFO] visual_prompt:  293: 	Training 1400/5004. train loss: 6.7025,	0.9524 s / batch. (data: 3.77e-04). ETA=5 days, 10:41:41, max mem: 15.3 GB 
[06/12 03:31:16][INFO] visual_prompt:  293: 	Training 1500/5004. train loss: 6.7306,	0.9458 s / batch. (data: 4.23e-04). ETA=5 days, 9:45:39, max mem: 15.3 GB 
[06/12 03:32:51][INFO] visual_prompt:  293: 	Training 1600/5004. train loss: 6.6991,	0.9625 s / batch. (data: 8.81e-04). ETA=5 days, 12:01:34, max mem: 15.3 GB 
[06/12 03:34:26][INFO] visual_prompt:  293: 	Training 1700/5004. train loss: 6.7163,	0.9486 s / batch. (data: 4.53e-03). ETA=5 days, 10:05:16, max mem: 15.3 GB 
[06/12 03:36:01][INFO] visual_prompt:  293: 	Training 1800/5004. train loss: 6.7007,	0.9439 s / batch. (data: 4.22e-04). ETA=5 days, 9:25:01, max mem: 15.3 GB 
[06/12 03:37:37][INFO] visual_prompt:  293: 	Training 1900/5004. train loss: 6.6742,	0.9429 s / batch. (data: 3.01e-04). ETA=5 days, 9:15:41, max mem: 15.3 GB 
[06/12 03:39:12][INFO] visual_prompt:  293: 	Training 2000/5004. train loss: 6.6955,	0.9479 s / batch. (data: 3.93e-04). ETA=5 days, 9:55:12, max mem: 15.3 GB 
[06/12 03:40:48][INFO] visual_prompt:  293: 	Training 2100/5004. train loss: 6.6729,	0.9526 s / batch. (data: 3.30e-04). ETA=5 days, 10:32:15, max mem: 15.3 GB 
[06/12 03:42:23][INFO] visual_prompt:  293: 	Training 2200/5004. train loss: 6.6635,	0.9572 s / batch. (data: 2.50e-03). ETA=5 days, 11:07:59, max mem: 15.3 GB 
[06/12 03:43:59][INFO] visual_prompt:  293: 	Training 2300/5004. train loss: 6.6769,	0.9627 s / batch. (data: 3.84e-04). ETA=5 days, 11:51:26, max mem: 15.3 GB 
[06/12 03:45:34][INFO] visual_prompt:  293: 	Training 2400/5004. train loss: 6.6504,	0.9439 s / batch. (data: 2.52e-04). ETA=5 days, 9:15:55, max mem: 15.3 GB 
[06/12 03:47:10][INFO] visual_prompt:  293: 	Training 2500/5004. train loss: 6.6672,	0.9502 s / batch. (data: 3.03e-04). ETA=5 days, 10:05:36, max mem: 15.3 GB 
[06/12 03:48:45][INFO] visual_prompt:  293: 	Training 2600/5004. train loss: 6.6889,	0.9407 s / batch. (data: 2.78e-04). ETA=5 days, 8:46:20, max mem: 15.3 GB 
[06/12 03:50:20][INFO] visual_prompt:  293: 	Training 2700/5004. train loss: 6.6670,	0.9524 s / batch. (data: 4.26e-04). ETA=5 days, 10:20:20, max mem: 15.3 GB 
[06/12 03:51:56][INFO] visual_prompt:  293: 	Training 2800/5004. train loss: 6.6754,	0.9460 s / batch. (data: 3.96e-04). ETA=5 days, 9:26:28, max mem: 15.3 GB 
[06/12 03:53:31][INFO] visual_prompt:  293: 	Training 2900/5004. train loss: 6.6435,	0.9649 s / batch. (data: 3.93e-04). ETA=5 days, 12:00:25, max mem: 15.3 GB 
[06/12 03:55:07][INFO] visual_prompt:  293: 	Training 3000/5004. train loss: 6.6868,	0.9593 s / batch. (data: 3.39e-04). ETA=5 days, 11:12:43, max mem: 15.3 GB 
[06/12 03:56:42][INFO] visual_prompt:  293: 	Training 3100/5004. train loss: 6.6791,	0.9476 s / batch. (data: 2.76e-04). ETA=5 days, 9:35:04, max mem: 15.3 GB 
[06/12 03:58:17][INFO] visual_prompt:  293: 	Training 3200/5004. train loss: 6.6360,	0.9479 s / batch. (data: 5.13e-04). ETA=5 days, 9:36:00, max mem: 15.3 GB 
[06/12 03:59:53][INFO] visual_prompt:  293: 	Training 3300/5004. train loss: 6.6809,	0.9549 s / batch. (data: 2.59e-04). ETA=5 days, 10:31:50, max mem: 15.3 GB 
[06/12 04:01:28][INFO] visual_prompt:  293: 	Training 3400/5004. train loss: 6.6411,	0.9581 s / batch. (data: 2.92e-04). ETA=5 days, 10:56:10, max mem: 15.3 GB 
[06/12 04:03:03][INFO] visual_prompt:  293: 	Training 3500/5004. train loss: 6.6504,	0.9583 s / batch. (data: 2.91e-04). ETA=5 days, 10:56:38, max mem: 15.3 GB 
[06/12 04:04:39][INFO] visual_prompt:  293: 	Training 3600/5004. train loss: 6.6321,	0.9611 s / batch. (data: 2.78e-04). ETA=5 days, 11:17:41, max mem: 15.3 GB 
[06/12 04:06:14][INFO] visual_prompt:  293: 	Training 3700/5004. train loss: 6.6153,	0.9511 s / batch. (data: 5.23e-04). ETA=5 days, 9:54:08, max mem: 15.3 GB 
[06/12 04:07:50][INFO] visual_prompt:  293: 	Training 3800/5004. train loss: 6.6362,	0.9536 s / batch. (data: 5.48e-04). ETA=5 days, 10:12:58, max mem: 15.3 GB 
[06/12 04:09:25][INFO] visual_prompt:  293: 	Training 3900/5004. train loss: 6.6609,	0.9494 s / batch. (data: 3.34e-04). ETA=5 days, 9:37:01, max mem: 15.3 GB 
[06/12 04:11:00][INFO] visual_prompt:  293: 	Training 4000/5004. train loss: 6.6365,	0.9558 s / batch. (data: 3.32e-04). ETA=5 days, 10:27:33, max mem: 15.3 GB 
[06/12 04:12:36][INFO] visual_prompt:  293: 	Training 4100/5004. train loss: 6.6451,	0.9502 s / batch. (data: 3.81e-04). ETA=5 days, 9:40:05, max mem: 15.3 GB 
[06/12 04:14:11][INFO] visual_prompt:  293: 	Training 4200/5004. train loss: 6.6537,	0.9634 s / batch. (data: 3.25e-04). ETA=5 days, 11:26:45, max mem: 15.3 GB 
[06/12 04:15:46][INFO] visual_prompt:  293: 	Training 4300/5004. train loss: 6.6306,	0.9404 s / batch. (data: 3.23e-04). ETA=5 days, 8:16:52, max mem: 15.3 GB 
[06/12 04:17:22][INFO] visual_prompt:  293: 	Training 4400/5004. train loss: 6.6534,	0.9421 s / batch. (data: 3.38e-04). ETA=5 days, 8:29:21, max mem: 15.3 GB 
[06/12 04:18:57][INFO] visual_prompt:  293: 	Training 4500/5004. train loss: 6.6344,	0.9424 s / batch. (data: 3.13e-04). ETA=5 days, 8:30:19, max mem: 15.3 GB 
[06/12 04:20:33][INFO] visual_prompt:  293: 	Training 4600/5004. train loss: 6.6175,	0.9521 s / batch. (data: 2.81e-04). ETA=5 days, 9:48:04, max mem: 15.3 GB 
[06/12 04:22:08][INFO] visual_prompt:  293: 	Training 4700/5004. train loss: 6.6360,	0.9423 s / batch. (data: 3.48e-04). ETA=5 days, 8:26:24, max mem: 15.3 GB 
[06/12 04:23:44][INFO] visual_prompt:  293: 	Training 4800/5004. train loss: 6.6196,	0.9460 s / batch. (data: 6.03e-04). ETA=5 days, 8:54:43, max mem: 15.3 GB 
[06/12 04:25:19][INFO] visual_prompt:  293: 	Training 4900/5004. train loss: 6.5959,	0.9450 s / batch. (data: 3.28e-04). ETA=5 days, 8:45:42, max mem: 15.3 GB 
[06/12 04:26:55][INFO] visual_prompt:  293: 	Training 5000/5004. train loss: 6.6079,	0.9462 s / batch. (data: 1.32e-04). ETA=5 days, 8:53:27, max mem: 15.3 GB 
[06/12 04:27:00][INFO] visual_prompt:  306: Epoch 2 / 100: avg data time: 5.00e-03, avg batch time: 0.9662, average train loss: 6.6955
[06/12 04:54:42][INFO] visual_prompt:  426: 	Test 100/196. loss: 205.371, 16.2631 s / batch. (data: 3.23e-04)max mem: 15.25483 GB 
[06/12 05:20:37][INFO] visual_prompt:  463: Inference (val):avg data time: 2.17e-04, avg batch time: 16.2047, average loss: 205.6996
[06/12 05:20:37][INFO] visual_prompt:  480: Saved invariances for val_imagenet at output_ce/val_imagenet_invariances.json
[06/12 05:20:38][INFO] visual_prompt:  249: Training 3 / 100 epoch, with learning rate 0.2
[06/12 05:23:12][INFO] visual_prompt:  293: 	Training 100/5004. train loss: 6.6468,	0.9407 s / batch. (data: 3.04e-04). ETA=5 days, 8:07:09, max mem: 15.3 GB 
[06/12 05:24:47][INFO] visual_prompt:  293: 	Training 200/5004. train loss: 6.6538,	0.9672 s / batch. (data: 2.36e-04). ETA=5 days, 11:42:14, max mem: 15.3 GB 
[06/12 05:26:22][INFO] visual_prompt:  293: 	Training 300/5004. train loss: 6.6429,	0.9563 s / batch. (data: 2.88e-04). ETA=5 days, 10:11:02, max mem: 15.3 GB 
[06/12 05:27:58][INFO] visual_prompt:  293: 	Training 400/5004. train loss: 6.6187,	0.9550 s / batch. (data: 4.05e-04). ETA=5 days, 9:59:14, max mem: 15.3 GB 
[06/12 05:29:33][INFO] visual_prompt:  293: 	Training 500/5004. train loss: 6.6311,	0.9554 s / batch. (data: 3.49e-04). ETA=5 days, 10:01:02, max mem: 15.3 GB 
[06/12 05:31:09][INFO] visual_prompt:  293: 	Training 600/5004. train loss: 6.5994,	0.9569 s / batch. (data: 2.89e-04). ETA=5 days, 10:11:43, max mem: 15.3 GB 
[06/12 05:32:44][INFO] visual_prompt:  293: 	Training 700/5004. train loss: 6.6433,	0.9616 s / batch. (data: 3.65e-04). ETA=5 days, 10:48:06, max mem: 15.3 GB 
[06/12 05:34:19][INFO] visual_prompt:  293: 	Training 800/5004. train loss: 6.6638,	0.9458 s / batch. (data: 3.10e-04). ETA=5 days, 8:37:50, max mem: 15.3 GB 
[06/12 05:35:55][INFO] visual_prompt:  293: 	Training 900/5004. train loss: 6.6379,	0.9596 s / batch. (data: 3.24e-04). ETA=5 days, 10:28:32, max mem: 15.3 GB 
[06/12 05:37:30][INFO] visual_prompt:  293: 	Training 1000/5004. train loss: 6.5853,	0.9435 s / batch. (data: 3.95e-04). ETA=5 days, 8:15:54, max mem: 15.3 GB 
[06/12 05:39:05][INFO] visual_prompt:  293: 	Training 1100/5004. train loss: 6.6668,	0.9610 s / batch. (data: 4.86e-04). ETA=5 days, 10:37:03, max mem: 15.3 GB 
[06/12 05:40:41][INFO] visual_prompt:  293: 	Training 1200/5004. train loss: 6.6385,	0.9523 s / batch. (data: 4.08e-04). ETA=5 days, 9:24:17, max mem: 15.3 GB 
[06/12 05:42:17][INFO] visual_prompt:  293: 	Training 1300/5004. train loss: 6.6031,	0.9463 s / batch. (data: 3.65e-04). ETA=5 days, 8:34:09, max mem: 15.3 GB 
[06/12 05:43:52][INFO] visual_prompt:  293: 	Training 1400/5004. train loss: 6.5725,	0.9546 s / batch. (data: 3.79e-04). ETA=5 days, 9:39:29, max mem: 15.3 GB 
[06/12 05:45:28][INFO] visual_prompt:  293: 	Training 1500/5004. train loss: 6.6348,	0.9643 s / batch. (data: 2.95e-04). ETA=5 days, 10:57:11, max mem: 15.3 GB 
[06/12 05:47:03][INFO] visual_prompt:  293: 	Training 1600/5004. train loss: 6.6312,	0.9478 s / batch. (data: 2.73e-04). ETA=5 days, 8:41:02, max mem: 15.3 GB 
[06/12 05:48:39][INFO] visual_prompt:  293: 	Training 1700/5004. train loss: 6.5914,	0.9516 s / batch. (data: 2.75e-04). ETA=5 days, 9:10:42, max mem: 15.3 GB 
[06/12 05:50:14][INFO] visual_prompt:  293: 	Training 1800/5004. train loss: 6.5731,	0.9399 s / batch. (data: 2.99e-04). ETA=5 days, 7:33:51, max mem: 15.3 GB 
[06/12 05:51:49][INFO] visual_prompt:  293: 	Training 1900/5004. train loss: 6.5898,	0.9507 s / batch. (data: 3.06e-04). ETA=5 days, 8:59:53, max mem: 15.3 GB 
[06/12 05:53:25][INFO] visual_prompt:  293: 	Training 2000/5004. train loss: 6.6449,	0.9571 s / batch. (data: 3.51e-04). ETA=5 days, 9:50:35, max mem: 15.3 GB 
[06/12 05:55:00][INFO] visual_prompt:  293: 	Training 2100/5004. train loss: 6.5923,	0.9582 s / batch. (data: 3.82e-04). ETA=5 days, 9:58:21, max mem: 15.3 GB 
[06/12 05:56:35][INFO] visual_prompt:  293: 	Training 2200/5004. train loss: 6.6115,	0.9618 s / batch. (data: 3.57e-04). ETA=5 days, 10:26:03, max mem: 15.3 GB 
[06/12 05:58:10][INFO] visual_prompt:  293: 	Training 2300/5004. train loss: 6.6026,	0.9459 s / batch. (data: 2.14e-03). ETA=5 days, 8:15:01, max mem: 15.3 GB 
[06/12 05:59:46][INFO] visual_prompt:  293: 	Training 2400/5004. train loss: 6.5812,	0.9602 s / batch. (data: 1.26e-03). ETA=5 days, 10:09:10, max mem: 15.3 GB 
[06/12 06:01:21][INFO] visual_prompt:  293: 	Training 2500/5004. train loss: 6.6131,	0.9670 s / batch. (data: 5.88e-04). ETA=5 days, 11:03:30, max mem: 15.3 GB 
[06/12 06:02:56][INFO] visual_prompt:  293: 	Training 2600/5004. train loss: 6.6177,	0.9604 s / batch. (data: 3.52e-04). ETA=5 days, 10:07:46, max mem: 15.3 GB 
[06/12 06:04:31][INFO] visual_prompt:  293: 	Training 2700/5004. train loss: 6.6004,	0.9499 s / batch. (data: 3.89e-04). ETA=5 days, 8:41:05, max mem: 15.3 GB 
[06/12 06:06:07][INFO] visual_prompt:  293: 	Training 2800/5004. train loss: 6.6172,	0.9501 s / batch. (data: 3.57e-04). ETA=5 days, 8:41:04, max mem: 15.3 GB 
[06/12 06:07:42][INFO] visual_prompt:  293: 	Training 2900/5004. train loss: 6.5543,	0.9494 s / batch. (data: 3.62e-04). ETA=5 days, 8:33:46, max mem: 15.3 GB 
[06/12 06:09:17][INFO] visual_prompt:  293: 	Training 3000/5004. train loss: 6.6237,	0.9492 s / batch. (data: 3.54e-04). ETA=5 days, 8:30:47, max mem: 15.3 GB 
[06/12 06:10:52][INFO] visual_prompt:  293: 	Training 3100/5004. train loss: 6.6326,	0.9545 s / batch. (data: 2.73e-04). ETA=5 days, 9:11:42, max mem: 15.3 GB 
[06/12 06:12:28][INFO] visual_prompt:  293: 	Training 3200/5004. train loss: 6.5711,	0.9414 s / batch. (data: 4.43e-04). ETA=5 days, 7:24:08, max mem: 15.3 GB 
[06/12 06:14:03][INFO] visual_prompt:  293: 	Training 3300/5004. train loss: 6.6410,	0.9572 s / batch. (data: 3.46e-04). ETA=5 days, 9:30:42, max mem: 15.3 GB 
[06/12 06:15:38][INFO] visual_prompt:  293: 	Training 3400/5004. train loss: 6.6073,	0.9555 s / batch. (data: 3.53e-04). ETA=5 days, 9:14:57, max mem: 15.3 GB 
[06/12 06:17:14][INFO] visual_prompt:  293: 	Training 3500/5004. train loss: 6.5738,	0.9686 s / batch. (data: 3.37e-04). ETA=5 days, 11:00:27, max mem: 15.3 GB 
[06/12 06:18:49][INFO] visual_prompt:  293: 	Training 3600/5004. train loss: 6.6144,	0.9618 s / batch. (data: 3.77e-04). ETA=5 days, 10:03:11, max mem: 15.3 GB 
[06/12 06:20:24][INFO] visual_prompt:  293: 	Training 3700/5004. train loss: 6.6117,	0.9469 s / batch. (data: 3.08e-04). ETA=5 days, 8:01:08, max mem: 15.3 GB 
[06/12 06:21:59][INFO] visual_prompt:  293: 	Training 3800/5004. train loss: 6.6060,	0.9413 s / batch. (data: 3.26e-04). ETA=5 days, 7:13:40, max mem: 15.3 GB 
[06/12 06:23:34][INFO] visual_prompt:  293: 	Training 3900/5004. train loss: 6.6053,	0.9527 s / batch. (data: 2.91e-04). ETA=5 days, 8:44:36, max mem: 15.3 GB 
[06/12 06:25:10][INFO] visual_prompt:  293: 	Training 4000/5004. train loss: 6.5990,	0.9489 s / batch. (data: 4.88e-04). ETA=5 days, 8:12:38, max mem: 15.3 GB 
[06/12 06:26:45][INFO] visual_prompt:  293: 	Training 4100/5004. train loss: 6.6012,	0.9423 s / batch. (data: 4.07e-04). ETA=5 days, 7:16:52, max mem: 15.3 GB 
[06/12 06:28:20][INFO] visual_prompt:  293: 	Training 4200/5004. train loss: 6.6017,	0.9511 s / batch. (data: 3.92e-04). ETA=5 days, 8:26:46, max mem: 15.3 GB 
[06/12 06:29:55][INFO] visual_prompt:  293: 	Training 4300/5004. train loss: 6.6229,	0.9496 s / batch. (data: 3.42e-04). ETA=5 days, 8:13:06, max mem: 15.3 GB 
[06/12 06:31:31][INFO] visual_prompt:  293: 	Training 4400/5004. train loss: 6.6099,	0.9528 s / batch. (data: 2.98e-04). ETA=5 days, 8:37:19, max mem: 15.3 GB 
[06/12 06:33:06][INFO] visual_prompt:  293: 	Training 4500/5004. train loss: 6.6082,	0.9594 s / batch. (data: 2.67e-04). ETA=5 days, 9:29:39, max mem: 15.3 GB 
[06/12 06:34:41][INFO] visual_prompt:  293: 	Training 4600/5004. train loss: 6.5981,	0.9487 s / batch. (data: 3.22e-04). ETA=5 days, 8:01:12, max mem: 15.3 GB 
[06/12 06:36:16][INFO] visual_prompt:  293: 	Training 4700/5004. train loss: 6.6171,	0.9442 s / batch. (data: 3.66e-04). ETA=5 days, 7:23:19, max mem: 15.3 GB 
[06/12 06:37:52][INFO] visual_prompt:  293: 	Training 4800/5004. train loss: 6.6006,	0.9502 s / batch. (data: 3.96e-04). ETA=5 days, 8:10:32, max mem: 15.3 GB 
[06/12 06:39:27][INFO] visual_prompt:  293: 	Training 4900/5004. train loss: 6.5731,	0.9476 s / batch. (data: 2.85e-04). ETA=5 days, 7:47:20, max mem: 15.3 GB 
[06/12 06:41:02][INFO] visual_prompt:  293: 	Training 5000/5004. train loss: 6.5963,	0.9508 s / batch. (data: 1.27e-04). ETA=5 days, 8:12:07, max mem: 15.3 GB 
[06/12 06:41:07][INFO] visual_prompt:  306: Epoch 3 / 100: avg data time: 5.08e-03, avg batch time: 0.9648, average train loss: 6.6153
[06/12 07:08:48][INFO] visual_prompt:  426: 	Test 100/196. loss: 204.669, 16.2710 s / batch. (data: 2.18e-04)max mem: 15.25483 GB 
[06/12 07:34:42][INFO] visual_prompt:  463: Inference (val):avg data time: 2.34e-04, avg batch time: 16.1755, average loss: 205.1707
[06/12 07:34:42][INFO] visual_prompt:  480: Saved invariances for val_imagenet at output_ce/val_imagenet_invariances.json
[06/12 07:34:42][INFO] visual_prompt:  249: Training 4 / 100 epoch, with learning rate 0.3
[06/12 07:36:57][INFO] visual_prompt:  293: 	Training 100/5004. train loss: 6.6606,	0.9436 s / batch. (data: 2.65e-04). ETA=5 days, 7:11:38, max mem: 15.3 GB 
[06/12 07:38:31][INFO] visual_prompt:  293: 	Training 200/5004. train loss: 6.5875,	0.9428 s / batch. (data: 3.13e-04). ETA=5 days, 7:03:36, max mem: 15.3 GB 
[06/12 07:40:06][INFO] visual_prompt:  293: 	Training 300/5004. train loss: 6.5969,	0.9374 s / batch. (data: 5.94e-04). ETA=5 days, 6:18:30, max mem: 15.3 GB 
[06/12 07:41:40][INFO] visual_prompt:  293: 	Training 400/5004. train loss: 6.6494,	0.9470 s / batch. (data: 3.75e-04). ETA=5 days, 7:34:35, max mem: 15.3 GB 
[06/12 07:43:14][INFO] visual_prompt:  293: 	Training 500/5004. train loss: 6.6368,	0.9469 s / batch. (data: 3.71e-04). ETA=5 days, 7:32:34, max mem: 15.3 GB 
[06/12 07:44:49][INFO] visual_prompt:  293: 	Training 600/5004. train loss: 6.5997,	0.9458 s / batch. (data: 3.58e-04). ETA=5 days, 7:21:39, max mem: 15.3 GB 
[06/12 07:46:23][INFO] visual_prompt:  293: 	Training 700/5004. train loss: 6.5943,	0.9433 s / batch. (data: 3.35e-04). ETA=5 days, 7:00:10, max mem: 15.3 GB 
[06/12 07:47:58][INFO] visual_prompt:  293: 	Training 800/5004. train loss: 6.6094,	0.9466 s / batch. (data: 3.84e-04). ETA=5 days, 7:24:51, max mem: 15.3 GB 
[06/12 07:49:32][INFO] visual_prompt:  293: 	Training 900/5004. train loss: 6.5877,	0.9458 s / batch. (data: 2.58e-04). ETA=5 days, 7:17:03, max mem: 15.3 GB 
[06/12 07:51:07][INFO] visual_prompt:  293: 	Training 1000/5004. train loss: 6.5705,	0.9437 s / batch. (data: 3.13e-04). ETA=5 days, 6:58:20, max mem: 15.3 GB 
[06/12 07:52:41][INFO] visual_prompt:  293: 	Training 1100/5004. train loss: 6.6398,	0.9491 s / batch. (data: 2.87e-04). ETA=5 days, 7:40:23, max mem: 15.3 GB 
[06/12 07:54:16][INFO] visual_prompt:  293: 	Training 1200/5004. train loss: 6.5887,	0.9438 s / batch. (data: 3.85e-04). ETA=5 days, 6:56:03, max mem: 15.3 GB 
[06/12 07:55:50][INFO] visual_prompt:  293: 	Training 1300/5004. train loss: 6.5524,	0.9477 s / batch. (data: 3.08e-04). ETA=5 days, 7:26:23, max mem: 15.3 GB 
[06/12 07:57:25][INFO] visual_prompt:  293: 	Training 1400/5004. train loss: 6.5670,	0.9449 s / batch. (data: 3.77e-04). ETA=5 days, 7:02:07, max mem: 15.3 GB 
[06/12 07:58:59][INFO] visual_prompt:  293: 	Training 1500/5004. train loss: 6.5981,	0.9511 s / batch. (data: 3.09e-04). ETA=5 days, 7:50:40, max mem: 15.3 GB 
[06/12 08:00:34][INFO] visual_prompt:  293: 	Training 1600/5004. train loss: 6.5720,	0.9404 s / batch. (data: 4.58e-04). ETA=5 days, 6:22:33, max mem: 15.3 GB 
[06/12 08:02:08][INFO] visual_prompt:  293: 	Training 1700/5004. train loss: 6.6549,	0.9409 s / batch. (data: 3.58e-04). ETA=5 days, 6:25:05, max mem: 15.3 GB 
[06/12 08:03:43][INFO] visual_prompt:  293: 	Training 1800/5004. train loss: 6.6164,	0.9484 s / batch. (data: 3.96e-04). ETA=5 days, 7:23:52, max mem: 15.3 GB 
[06/12 08:05:17][INFO] visual_prompt:  293: 	Training 1900/5004. train loss: 6.5842,	0.9535 s / batch. (data: 3.51e-04). ETA=5 days, 8:03:10, max mem: 15.3 GB 
[06/12 08:06:52][INFO] visual_prompt:  293: 	Training 2000/5004. train loss: 6.6058,	0.9458 s / batch. (data: 3.14e-04). ETA=5 days, 6:59:35, max mem: 15.3 GB 
[06/12 08:08:26][INFO] visual_prompt:  293: 	Training 2100/5004. train loss: 6.6462,	0.9456 s / batch. (data: 3.56e-04). ETA=5 days, 6:56:47, max mem: 15.3 GB 
[06/12 08:10:00][INFO] visual_prompt:  293: 	Training 2200/5004. train loss: 6.6481,	0.9417 s / batch. (data: 3.76e-04). ETA=5 days, 6:23:33, max mem: 15.3 GB 
[06/12 08:11:35][INFO] visual_prompt:  293: 	Training 2300/5004. train loss: 6.5720,	0.9411 s / batch. (data: 2.70e-04). ETA=5 days, 6:17:02, max mem: 15.3 GB 
[06/12 08:13:09][INFO] visual_prompt:  293: 	Training 2400/5004. train loss: 6.6051,	0.9403 s / batch. (data: 5.02e-04). ETA=5 days, 6:08:59, max mem: 15.3 GB 
[06/12 08:14:44][INFO] visual_prompt:  293: 	Training 2500/5004. train loss: 6.6416,	0.9463 s / batch. (data: 3.00e-04). ETA=5 days, 6:55:36, max mem: 15.3 GB 
[06/12 08:16:18][INFO] visual_prompt:  293: 	Training 2600/5004. train loss: 6.6151,	0.9465 s / batch. (data: 3.02e-04). ETA=5 days, 6:55:39, max mem: 15.3 GB 
[06/12 08:17:53][INFO] visual_prompt:  293: 	Training 2700/5004. train loss: 6.5984,	0.9429 s / batch. (data: 3.77e-04). ETA=5 days, 6:25:35, max mem: 15.3 GB 
[06/12 08:19:28][INFO] visual_prompt:  293: 	Training 2800/5004. train loss: 6.5927,	0.9532 s / batch. (data: 3.37e-04). ETA=5 days, 7:47:01, max mem: 15.3 GB 
[06/12 08:21:02][INFO] visual_prompt:  293: 	Training 2900/5004. train loss: 6.5675,	0.9438 s / batch. (data: 3.89e-04). ETA=5 days, 6:29:11, max mem: 15.3 GB 
[06/12 08:22:37][INFO] visual_prompt:  293: 	Training 3000/5004. train loss: 6.6260,	0.9473 s / batch. (data: 4.23e-04). ETA=5 days, 6:56:17, max mem: 15.3 GB 
[06/12 08:24:11][INFO] visual_prompt:  293: 	Training 3100/5004. train loss: 6.6359,	0.9615 s / batch. (data: 3.67e-04). ETA=5 days, 8:48:38, max mem: 15.3 GB 
[06/12 08:25:46][INFO] visual_prompt:  293: 	Training 3200/5004. train loss: 6.5793,	0.9427 s / batch. (data: 3.15e-04). ETA=5 days, 6:15:57, max mem: 15.3 GB 
[06/12 08:27:20][INFO] visual_prompt:  293: 	Training 3300/5004. train loss: 6.6322,	0.9400 s / batch. (data: 3.21e-04). ETA=5 days, 5:52:51, max mem: 15.3 GB 
[06/12 08:28:55][INFO] visual_prompt:  293: 	Training 3400/5004. train loss: 6.6202,	0.9526 s / batch. (data: 3.28e-04). ETA=5 days, 7:32:37, max mem: 15.3 GB 
[06/12 08:30:29][INFO] visual_prompt:  293: 	Training 3500/5004. train loss: 6.6054,	0.9409 s / batch. (data: 4.42e-04). ETA=5 days, 5:56:59, max mem: 15.3 GB 
[06/12 08:32:04][INFO] visual_prompt:  293: 	Training 3600/5004. train loss: 6.5626,	0.9477 s / batch. (data: 3.01e-04). ETA=5 days, 6:49:59, max mem: 15.3 GB 
[06/12 08:33:38][INFO] visual_prompt:  293: 	Training 3700/5004. train loss: 6.6092,	0.9393 s / batch. (data: 2.96e-04). ETA=5 days, 5:41:02, max mem: 15.3 GB 
[06/12 08:35:13][INFO] visual_prompt:  293: 	Training 3800/5004. train loss: 6.5972,	0.9471 s / batch. (data: 3.69e-04). ETA=5 days, 6:41:41, max mem: 15.3 GB 
[06/12 08:36:47][INFO] visual_prompt:  293: 	Training 3900/5004. train loss: 6.6323,	0.9426 s / batch. (data: 4.85e-04). ETA=5 days, 6:03:55, max mem: 15.3 GB 
[06/12 08:38:22][INFO] visual_prompt:  293: 	Training 4000/5004. train loss: 6.6225,	0.9442 s / batch. (data: 3.73e-04). ETA=5 days, 6:15:39, max mem: 15.3 GB 
[06/12 08:39:56][INFO] visual_prompt:  293: 	Training 4100/5004. train loss: 6.5957,	0.9413 s / batch. (data: 2.59e-04). ETA=5 days, 5:50:58, max mem: 15.3 GB 
[06/12 08:41:30][INFO] visual_prompt:  293: 	Training 4200/5004. train loss: 6.6520,	0.9416 s / batch. (data: 3.31e-04). ETA=5 days, 5:51:50, max mem: 15.3 GB 
[06/12 08:43:05][INFO] visual_prompt:  293: 	Training 4300/5004. train loss: 6.6343,	0.9411 s / batch. (data: 4.67e-04). ETA=5 days, 5:45:51, max mem: 15.3 GB 
[06/12 08:44:39][INFO] visual_prompt:  293: 	Training 4400/5004. train loss: 6.6453,	0.9632 s / batch. (data: 3.23e-04). ETA=5 days, 8:41:04, max mem: 15.3 GB 
[06/12 08:46:14][INFO] visual_prompt:  293: 	Training 4500/5004. train loss: 6.6352,	0.9532 s / batch. (data: 4.17e-04). ETA=5 days, 7:19:19, max mem: 15.3 GB 
[06/12 08:47:48][INFO] visual_prompt:  293: 	Training 4600/5004. train loss: 6.5922,	0.9421 s / batch. (data: 3.79e-04). ETA=5 days, 5:49:12, max mem: 15.3 GB 
[06/12 08:49:22][INFO] visual_prompt:  293: 	Training 4700/5004. train loss: 6.6047,	0.9377 s / batch. (data: 3.72e-04). ETA=5 days, 5:12:18, max mem: 15.3 GB 
[06/12 08:50:56][INFO] visual_prompt:  293: 	Training 4800/5004. train loss: 6.5959,	0.9339 s / batch. (data: 3.31e-04). ETA=5 days, 4:40:38, max mem: 15.3 GB 
[06/12 08:52:30][INFO] visual_prompt:  293: 	Training 4900/5004. train loss: 6.5805,	0.9368 s / batch. (data: 5.18e-04). ETA=5 days, 5:02:01, max mem: 15.3 GB 
[06/12 08:54:04][INFO] visual_prompt:  293: 	Training 5000/5004. train loss: 6.6383,	0.9330 s / batch. (data: 7.92e-05). ETA=5 days, 4:29:43, max mem: 15.3 GB 
[06/12 08:54:09][INFO] visual_prompt:  306: Epoch 4 / 100: avg data time: 4.56e-03, avg batch time: 0.9524, average train loss: 6.6136
[06/12 09:21:32][INFO] visual_prompt:  426: 	Test 100/196. loss: 204.770, 16.1305 s / batch. (data: 2.25e-04)max mem: 15.25483 GB 
[06/12 09:47:27][INFO] visual_prompt:  463: Inference (val):avg data time: 2.37e-04, avg batch time: 16.1169, average loss: 205.3592
[06/12 09:47:27][INFO] visual_prompt:  480: Saved invariances for val_imagenet at output_ce/val_imagenet_invariances.json
[06/12 09:47:28][INFO] visual_prompt:  249: Training 5 / 100 epoch, with learning rate 0.4
[06/12 09:50:02][INFO] visual_prompt:  293: 	Training 100/5004. train loss: 6.5832,	0.9443 s / batch. (data: 3.42e-04). ETA=5 days, 5:58:52, max mem: 15.3 GB 
[06/12 09:51:38][INFO] visual_prompt:  293: 	Training 200/5004. train loss: 6.6140,	0.9663 s / batch. (data: 3.19e-04). ETA=5 days, 8:53:40, max mem: 15.3 GB 
[06/12 09:53:14][INFO] visual_prompt:  293: 	Training 300/5004. train loss: 6.6445,	0.9573 s / batch. (data: 4.98e-03). ETA=5 days, 7:39:55, max mem: 15.3 GB 
[06/12 09:54:50][INFO] visual_prompt:  293: 	Training 400/5004. train loss: 6.6439,	0.9586 s / batch. (data: 3.44e-04). ETA=5 days, 7:48:36, max mem: 15.3 GB 
[06/12 09:56:26][INFO] visual_prompt:  293: 	Training 500/5004. train loss: 6.6150,	0.9543 s / batch. (data: 2.48e-03). ETA=5 days, 7:12:26, max mem: 15.3 GB 
[06/12 09:58:02][INFO] visual_prompt:  293: 	Training 600/5004. train loss: 6.6116,	0.9554 s / batch. (data: 3.44e-04). ETA=5 days, 7:19:55, max mem: 15.3 GB 
[06/12 09:59:38][INFO] visual_prompt:  293: 	Training 700/5004. train loss: 6.5568,	0.9643 s / batch. (data: 3.61e-04). ETA=5 days, 8:29:41, max mem: 15.3 GB 
[06/12 10:01:13][INFO] visual_prompt:  293: 	Training 800/5004. train loss: 6.6026,	0.9662 s / batch. (data: 3.69e-04). ETA=5 days, 8:42:34, max mem: 15.3 GB 
[06/12 10:02:49][INFO] visual_prompt:  293: 	Training 900/5004. train loss: 6.6544,	0.9501 s / batch. (data: 4.17e-04). ETA=5 days, 6:32:29, max mem: 15.3 GB 
[06/12 10:04:25][INFO] visual_prompt:  293: 	Training 1000/5004. train loss: 6.6218,	0.9618 s / batch. (data: 2.85e-04). ETA=5 days, 8:04:29, max mem: 15.3 GB 
[06/12 10:06:01][INFO] visual_prompt:  293: 	Training 1100/5004. train loss: 6.6792,	0.9526 s / batch. (data: 3.27e-04). ETA=5 days, 6:49:49, max mem: 15.3 GB 
[06/12 10:07:37][INFO] visual_prompt:  293: 	Training 1200/5004. train loss: 6.5864,	0.9532 s / batch. (data: 3.04e-04). ETA=5 days, 6:53:01, max mem: 15.3 GB 
[06/12 10:09:13][INFO] visual_prompt:  293: 	Training 1300/5004. train loss: 6.5718,	0.9596 s / batch. (data: 3.58e-04). ETA=5 days, 7:42:08, max mem: 15.3 GB 
[06/12 10:10:49][INFO] visual_prompt:  293: 	Training 1400/5004. train loss: 6.5492,	0.9742 s / batch. (data: 4.00e-04). ETA=5 days, 9:36:57, max mem: 15.3 GB 
[06/12 10:12:25][INFO] visual_prompt:  293: 	Training 1500/5004. train loss: 6.6146,	0.9661 s / batch. (data: 3.37e-04). ETA=5 days, 8:30:51, max mem: 15.3 GB 
[06/12 10:14:01][INFO] visual_prompt:  293: 	Training 1600/5004. train loss: 6.5678,	0.9658 s / batch. (data: 4.02e-04). ETA=5 days, 8:26:50, max mem: 15.3 GB 
[06/12 10:15:37][INFO] visual_prompt:  293: 	Training 1700/5004. train loss: 6.5971,	0.9849 s / batch. (data: 4.33e-04). ETA=5 days, 10:57:26, max mem: 15.3 GB 
[06/12 10:17:14][INFO] visual_prompt:  293: 	Training 1800/5004. train loss: 6.5420,	0.9779 s / batch. (data: 3.44e-04). ETA=5 days, 10:00:15, max mem: 15.3 GB 
[06/12 10:18:49][INFO] visual_prompt:  293: 	Training 1900/5004. train loss: 6.6419,	0.9490 s / batch. (data: 4.13e-04). ETA=5 days, 6:07:59, max mem: 15.3 GB 
[06/12 10:20:25][INFO] visual_prompt:  293: 	Training 2000/5004. train loss: 6.6008,	0.9578 s / batch. (data: 4.69e-04). ETA=5 days, 7:16:58, max mem: 15.3 GB 
[06/12 10:22:01][INFO] visual_prompt:  293: 	Training 2100/5004. train loss: 6.6918,	0.9628 s / batch. (data: 3.56e-04). ETA=5 days, 7:55:12, max mem: 15.3 GB 
[06/12 10:23:37][INFO] visual_prompt:  293: 	Training 2200/5004. train loss: 6.6089,	0.9585 s / batch. (data: 3.51e-04). ETA=5 days, 7:18:51, max mem: 15.3 GB 
[06/12 10:25:13][INFO] visual_prompt:  293: 	Training 2300/5004. train loss: 6.6621,	0.9591 s / batch. (data: 3.19e-04). ETA=5 days, 7:21:52, max mem: 15.3 GB 
[06/12 10:26:50][INFO] visual_prompt:  293: 	Training 2400/5004. train loss: 6.6506,	0.9560 s / batch. (data: 3.01e-04). ETA=5 days, 6:56:11, max mem: 15.3 GB 
[06/12 10:28:26][INFO] visual_prompt:  293: 	Training 2500/5004. train loss: 6.6173,	0.9470 s / batch. (data: 3.50e-04). ETA=5 days, 5:42:56, max mem: 15.3 GB 
[06/12 10:30:02][INFO] visual_prompt:  293: 	Training 2600/5004. train loss: 6.6168,	0.9477 s / batch. (data: 3.75e-04). ETA=5 days, 5:46:50, max mem: 15.3 GB 
[06/12 10:31:38][INFO] visual_prompt:  293: 	Training 2700/5004. train loss: 6.6438,	0.9518 s / batch. (data: 3.27e-04). ETA=5 days, 6:17:54, max mem: 15.3 GB 
[06/12 10:33:14][INFO] visual_prompt:  293: 	Training 2800/5004. train loss: 6.6243,	0.9617 s / batch. (data: 3.55e-04). ETA=5 days, 7:34:45, max mem: 15.3 GB 
[06/12 10:34:50][INFO] visual_prompt:  293: 	Training 2900/5004. train loss: 6.5978,	0.9679 s / batch. (data: 4.84e-04). ETA=5 days, 8:22:24, max mem: 15.3 GB 
[06/12 10:36:26][INFO] visual_prompt:  293: 	Training 3000/5004. train loss: 6.6643,	0.9565 s / batch. (data: 3.09e-04). ETA=5 days, 6:49:56, max mem: 15.3 GB 
[06/12 10:38:02][INFO] visual_prompt:  293: 	Training 3100/5004. train loss: 6.6025,	0.9427 s / batch. (data: 4.39e-04). ETA=5 days, 4:58:58, max mem: 15.3 GB 
[06/12 10:39:38][INFO] visual_prompt:  293: 	Training 3200/5004. train loss: 6.6202,	0.9693 s / batch. (data: 3.06e-04). ETA=5 days, 8:28:53, max mem: 15.3 GB 
[06/12 10:41:14][INFO] visual_prompt:  293: 	Training 3300/5004. train loss: 6.6560,	0.9656 s / batch. (data: 3.51e-04). ETA=5 days, 7:58:04, max mem: 15.3 GB 
[06/12 10:42:50][INFO] visual_prompt:  293: 	Training 3400/5004. train loss: 6.6007,	0.9553 s / batch. (data: 3.32e-04). ETA=5 days, 6:34:14, max mem: 15.3 GB 
[06/12 10:44:26][INFO] visual_prompt:  293: 	Training 3500/5004. train loss: 6.5737,	0.9617 s / batch. (data: 3.62e-04). ETA=5 days, 7:23:42, max mem: 15.3 GB 
[06/12 10:46:01][INFO] visual_prompt:  293: 	Training 3600/5004. train loss: 6.6439,	0.9583 s / batch. (data: 2.69e-04). ETA=5 days, 6:55:13, max mem: 15.3 GB 
[06/12 10:47:38][INFO] visual_prompt:  293: 	Training 3700/5004. train loss: 6.5907,	0.9605 s / batch. (data: 3.42e-04). ETA=5 days, 7:10:55, max mem: 15.3 GB 
[06/12 10:49:13][INFO] visual_prompt:  293: 	Training 3800/5004. train loss: 6.6703,	0.9607 s / batch. (data: 4.00e-04). ETA=5 days, 7:10:40, max mem: 15.3 GB 
[06/12 10:50:49][INFO] visual_prompt:  293: 	Training 3900/5004. train loss: 6.6573,	0.9459 s / batch. (data: 3.68e-04). ETA=5 days, 5:12:01, max mem: 15.3 GB 
[06/12 10:52:25][INFO] visual_prompt:  293: 	Training 4000/5004. train loss: 6.6095,	0.9603 s / batch. (data: 4.84e-04). ETA=5 days, 7:04:25, max mem: 15.3 GB 
[06/12 10:54:01][INFO] visual_prompt:  293: 	Training 4100/5004. train loss: 6.5564,	0.9612 s / batch. (data: 3.56e-04). ETA=5 days, 7:10:15, max mem: 15.3 GB 
[06/12 10:55:37][INFO] visual_prompt:  293: 	Training 4200/5004. train loss: 6.6611,	0.9626 s / batch. (data: 3.44e-04). ETA=5 days, 7:19:41, max mem: 15.3 GB 
[06/12 10:57:13][INFO] visual_prompt:  293: 	Training 4300/5004. train loss: 6.6484,	0.9657 s / batch. (data: 4.16e-04). ETA=5 days, 7:42:29, max mem: 15.3 GB 
[06/12 10:58:49][INFO] visual_prompt:  293: 	Training 4400/5004. train loss: 6.6364,	0.9701 s / batch. (data: 3.58e-04). ETA=5 days, 8:15:28, max mem: 15.3 GB 
[06/12 11:00:25][INFO] visual_prompt:  293: 	Training 4500/5004. train loss: 6.6532,	0.9579 s / batch. (data: 3.05e-04). ETA=5 days, 6:37:43, max mem: 15.3 GB 
[06/12 11:02:01][INFO] visual_prompt:  293: 	Training 4600/5004. train loss: 6.6566,	0.9417 s / batch. (data: 3.62e-04). ETA=5 days, 4:27:27, max mem: 15.3 GB 
[06/12 11:03:37][INFO] visual_prompt:  293: 	Training 4700/5004. train loss: 6.5773,	0.9593 s / batch. (data: 3.60e-04). ETA=5 days, 6:45:32, max mem: 15.3 GB 
[06/12 11:05:13][INFO] visual_prompt:  293: 	Training 4800/5004. train loss: 6.6601,	0.9615 s / batch. (data: 4.69e-04). ETA=5 days, 7:01:01, max mem: 15.3 GB 
[06/12 11:06:49][INFO] visual_prompt:  293: 	Training 4900/5004. train loss: 6.5596,	0.9647 s / batch. (data: 5.00e-04). ETA=5 days, 7:24:52, max mem: 15.3 GB 
[06/12 11:08:25][INFO] visual_prompt:  293: 	Training 5000/5004. train loss: 6.6471,	0.9350 s / batch. (data: 1.50e-04). ETA=5 days, 3:28:27, max mem: 15.3 GB 
[06/12 11:08:30][INFO] visual_prompt:  306: Epoch 5 / 100: avg data time: 5.92e-03, avg batch time: 0.9714, average train loss: 6.6203
[06/12 11:36:24][INFO] visual_prompt:  426: 	Test 100/196. loss: 204.980, 16.3004 s / batch. (data: 7.70e-05)max mem: 15.25483 GB 
