[06/16 14:16:00][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/16 14:16:00][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,4,5,6
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/16 14:16:00][INFO] visual_prompt:   99: Command line arguments: None
[06/16 14:16:00][INFO] visual_prompt:  108: Training with config:
[06/16 14:16:00][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': False,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls-reinit+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_shallow_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 1.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'mse',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'USE_CLS_TOKEN': True,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/16 14:16:04][INFO] visual_prompt:   56: Total Parameters: 93879568	 Gradient Parameters: 8081680
[06/16 14:16:04][INFO] visual_prompt:   58: tuned percent:8.609
[06/16 14:16:04][INFO] visual_prompt:   44: Device used for model: 0
[06/16 14:16:04][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/16 14:16:04][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/16 14:16:07][INFO] visual_prompt:  158: Number of images: 1281167
[06/16 14:16:07][INFO] visual_prompt:  159: Number of classes: 1000
[06/16 14:16:07][INFO] visual_prompt:   78: Loading validation data...
[06/16 14:16:07][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/16 14:16:07][INFO] visual_prompt:  158: Number of images: 50000
[06/16 14:16:07][INFO] visual_prompt:  159: Number of classes: 1000
[06/16 14:16:07][INFO] visual_prompt:   81: Loading test data...
[06/16 14:16:07][INFO] visual_prompt:   83: ...no test data is constructed
[06/16 14:16:07][INFO] visual_prompt:  111: Constructing models...
[06/16 14:16:07][INFO] visual_prompt:  114: Setting up Evalutator...
[06/16 14:16:07][INFO] visual_prompt:  116: Setting up Trainer...
[06/16 14:16:07][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/16 14:16:07][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/16 14:16:07][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/16 14:16:07][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/16 14:16:07][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/16 14:16:07][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/16 14:16:07][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/16 14:16:07][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/16 14:16:07][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/16 14:16:07][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/16 14:16:07][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/16 14:16:07][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/16 14:16:07][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/16 14:16:07][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/16 14:16:07][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/16 14:16:07][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/16 14:16:07][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/16 14:16:07][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/16 14:16:07][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/16 14:16:07][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/16 14:16:07][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/16 14:16:07][INFO] visual_prompt:  238: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/16 14:16:07][INFO] visual_prompt:  257: Training 1 / 100 epoch, with learning rate 0.0
[06/16 14:18:21][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0734,	1.1302 s / batch. (data: 3.20e-04). ETA=6 days, 13:04:11, max mem: 15.0 GB 
[06/16 14:20:14][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.1250,	1.1269 s / batch. (data: 3.21e-04). ETA=6 days, 12:34:11, max mem: 15.0 GB 
[06/16 14:22:07][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0914,	1.1265 s / batch. (data: 3.32e-04). ETA=6 days, 12:29:32, max mem: 15.0 GB 
[06/16 14:23:59][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0316,	1.1253 s / batch. (data: 3.04e-04). ETA=6 days, 12:17:45, max mem: 15.0 GB 
[06/16 14:25:52][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0729,	1.1281 s / batch. (data: 3.32e-04). ETA=6 days, 12:38:45, max mem: 15.0 GB 
[06/16 14:27:45][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0634,	1.1278 s / batch. (data: 3.44e-04). ETA=6 days, 12:34:29, max mem: 15.0 GB 
[06/16 14:29:37][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0779,	1.1236 s / batch. (data: 3.02e-04). ETA=6 days, 11:57:31, max mem: 15.0 GB 
[06/16 14:31:30][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0401,	1.1277 s / batch. (data: 3.02e-04). ETA=6 days, 12:30:10, max mem: 15.0 GB 
[06/16 14:33:23][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.1133,	1.1285 s / batch. (data: 3.24e-04). ETA=6 days, 12:34:49, max mem: 15.0 GB 
[06/16 14:35:15][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0666,	1.1244 s / batch. (data: 5.71e-04). ETA=6 days, 11:58:23, max mem: 15.0 GB 
[06/16 14:37:08][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0319,	1.1273 s / batch. (data: 3.12e-04). ETA=6 days, 12:20:56, max mem: 15.0 GB 
[06/16 14:39:01][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0587,	1.1247 s / batch. (data: 5.08e-04). ETA=6 days, 11:57:44, max mem: 15.0 GB 
[06/16 14:40:53][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0336,	1.1272 s / batch. (data: 3.14e-04). ETA=6 days, 12:16:13, max mem: 15.0 GB 
[06/16 14:42:46][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0617,	1.1304 s / batch. (data: 4.51e-04). ETA=6 days, 12:41:07, max mem: 15.0 GB 
[06/16 14:44:39][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.1178,	1.1254 s / batch. (data: 3.32e-04). ETA=6 days, 11:57:34, max mem: 15.0 GB 
[06/16 14:46:32][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0902,	1.1283 s / batch. (data: 2.69e-04). ETA=6 days, 12:19:43, max mem: 15.0 GB 
[06/16 14:48:24][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.1262,	1.1283 s / batch. (data: 2.86e-04). ETA=6 days, 12:17:52, max mem: 15.0 GB 
[06/16 14:50:17][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0619,	1.1312 s / batch. (data: 3.51e-04). ETA=6 days, 12:40:06, max mem: 15.0 GB 
[06/16 14:52:10][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0867,	1.1276 s / batch. (data: 3.04e-04). ETA=6 days, 12:08:20, max mem: 15.0 GB 
[06/16 14:54:03][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0386,	1.1252 s / batch. (data: 2.96e-04). ETA=6 days, 11:46:19, max mem: 15.0 GB 
[06/16 14:55:55][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0417,	1.1323 s / batch. (data: 3.49e-04). ETA=6 days, 12:43:48, max mem: 15.0 GB 
[06/16 14:57:48][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0553,	1.1257 s / batch. (data: 3.54e-04). ETA=6 days, 11:47:08, max mem: 15.0 GB 
[06/16 14:59:41][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.1135,	1.1233 s / batch. (data: 2.65e-04). ETA=6 days, 11:25:25, max mem: 15.0 GB 
[06/16 15:01:34][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0568,	1.1237 s / batch. (data: 2.90e-04). ETA=6 days, 11:26:20, max mem: 15.0 GB 
[06/16 15:03:26][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0817,	1.1260 s / batch. (data: 3.14e-04). ETA=6 days, 11:44:03, max mem: 15.0 GB 
[06/16 15:05:19][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0979,	1.1285 s / batch. (data: 3.43e-04). ETA=6 days, 12:02:36, max mem: 15.0 GB 
[06/16 15:07:12][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0984,	1.1271 s / batch. (data: 3.96e-04). ETA=6 days, 11:48:53, max mem: 15.0 GB 
[06/16 15:09:04][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0891,	1.1301 s / batch. (data: 3.66e-04). ETA=6 days, 12:12:34, max mem: 15.0 GB 
[06/16 15:10:57][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.1219,	1.1265 s / batch. (data: 3.02e-04). ETA=6 days, 11:40:27, max mem: 15.0 GB 
[06/16 15:12:49][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0532,	1.1227 s / batch. (data: 3.10e-04). ETA=6 days, 11:07:18, max mem: 15.0 GB 
[06/16 15:14:42][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0426,	1.1273 s / batch. (data: 3.35e-04). ETA=6 days, 11:43:50, max mem: 15.0 GB 
[06/16 15:16:35][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0336,	1.1280 s / batch. (data: 3.66e-04). ETA=6 days, 11:47:17, max mem: 15.0 GB 
[06/16 15:18:28][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0608,	1.1288 s / batch. (data: 3.32e-04). ETA=6 days, 11:52:09, max mem: 15.0 GB 
[06/16 15:20:20][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0734,	1.1298 s / batch. (data: 4.37e-04). ETA=6 days, 11:58:38, max mem: 15.0 GB 
[06/16 15:22:13][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.1121,	1.1284 s / batch. (data: 4.61e-04). ETA=6 days, 11:44:52, max mem: 15.0 GB 
[06/16 15:24:06][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0920,	1.1342 s / batch. (data: 5.73e-04). ETA=6 days, 12:31:33, max mem: 15.0 GB 
[06/16 15:25:59][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0825,	1.1273 s / batch. (data: 3.82e-04). ETA=6 days, 11:32:09, max mem: 15.0 GB 
[06/16 15:27:52][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0649,	1.1231 s / batch. (data: 3.46e-04). ETA=6 days, 10:55:30, max mem: 15.0 GB 
[06/16 15:29:44][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.1234,	1.1283 s / batch. (data: 2.99e-04). ETA=6 days, 11:36:49, max mem: 15.0 GB 
[06/16 15:31:37][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.1161,	1.1270 s / batch. (data: 3.47e-04). ETA=6 days, 11:24:24, max mem: 15.0 GB 
[06/16 15:33:30][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0585,	1.1253 s / batch. (data: 3.80e-04). ETA=6 days, 11:08:30, max mem: 15.0 GB 
[06/16 15:35:22][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.1192,	1.1270 s / batch. (data: 3.40e-04). ETA=6 days, 11:20:31, max mem: 15.0 GB 
[06/16 15:37:15][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0890,	1.1238 s / batch. (data: 3.61e-04). ETA=6 days, 10:52:06, max mem: 15.0 GB 
[06/16 15:39:08][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0712,	1.1292 s / batch. (data: 3.35e-04). ETA=6 days, 11:34:28, max mem: 15.0 GB 
[06/16 15:41:01][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0347,	1.1300 s / batch. (data: 3.15e-04). ETA=6 days, 11:39:32, max mem: 15.0 GB 
[06/16 15:42:53][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0558,	1.1253 s / batch. (data: 2.39e-04). ETA=6 days, 10:58:50, max mem: 15.0 GB 
[06/16 15:44:46][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0797,	1.1285 s / batch. (data: 3.55e-04). ETA=6 days, 11:22:52, max mem: 15.0 GB 
[06/16 15:46:39][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0522,	1.1248 s / batch. (data: 2.81e-04). ETA=6 days, 10:50:26, max mem: 15.0 GB 
[06/16 15:48:32][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0824,	1.1242 s / batch. (data: 3.43e-04). ETA=6 days, 10:44:14, max mem: 15.0 GB 
[06/16 15:50:24][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0347,	1.1240 s / batch. (data: 1.33e-04). ETA=6 days, 10:40:48, max mem: 15.0 GB 
[06/16 15:50:30][INFO] visual_prompt:  319: Epoch 1 / 100: avg data time: 4.36e-03, avg batch time: 1.1314, average train loss: 0.0737
[06/16 15:59:21][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.071, 5.1491 s / batch. (data: 7.08e-05)max mem: 14.95011 GB 
[06/16 16:07:33][INFO] visual_prompt:  476: Inference (val):avg data time: 1.44e-04, avg batch time: 5.1192, average loss: 0.0732
[06/16 16:07:33][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_shallow_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/16 16:07:33][INFO] visual_prompt:  257: Training 2 / 100 epoch, with learning rate 0.1
[06/16 16:10:08][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0787,	1.1271 s / batch. (data: 2.81e-04). ETA=6 days, 11:03:46, max mem: 15.0 GB 
[06/16 16:12:01][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0730,	1.1267 s / batch. (data: 2.93e-04). ETA=6 days, 10:58:44, max mem: 15.0 GB 
[06/16 16:13:54][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0673,	1.1235 s / batch. (data: 3.00e-04). ETA=6 days, 10:30:24, max mem: 15.0 GB 
[06/16 16:15:46][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0840,	1.1265 s / batch. (data: 3.21e-04). ETA=6 days, 10:53:41, max mem: 15.0 GB 
[06/16 16:17:39][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0778,	1.1266 s / batch. (data: 3.22e-04). ETA=6 days, 10:52:33, max mem: 15.0 GB 
[06/16 16:19:32][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0480,	1.1222 s / batch. (data: 3.68e-04). ETA=6 days, 10:14:18, max mem: 15.0 GB 
[06/16 16:21:24][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.1030,	1.1258 s / batch. (data: 3.57e-04). ETA=6 days, 10:42:12, max mem: 15.0 GB 
[06/16 16:23:17][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.1068,	1.1241 s / batch. (data: 3.10e-04). ETA=6 days, 10:26:22, max mem: 15.0 GB 
[06/16 16:25:09][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.1120,	1.1243 s / batch. (data: 3.17e-04). ETA=6 days, 10:25:38, max mem: 15.0 GB 
[06/16 16:27:02][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0778,	1.1247 s / batch. (data: 3.18e-04). ETA=6 days, 10:27:21, max mem: 15.0 GB 
[06/16 16:28:55][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0499,	1.1265 s / batch. (data: 3.37e-04). ETA=6 days, 10:40:27, max mem: 15.0 GB 
[06/16 16:30:48][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0222,	1.1240 s / batch. (data: 3.40e-04). ETA=6 days, 10:17:58, max mem: 15.0 GB 
[06/16 16:32:41][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.1076,	1.1329 s / batch. (data: 3.18e-04). ETA=6 days, 11:29:10, max mem: 15.0 GB 
[06/16 16:34:33][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0757,	1.1309 s / batch. (data: 2.99e-04). ETA=6 days, 11:10:37, max mem: 15.0 GB 
[06/16 16:36:26][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.1109,	1.1315 s / batch. (data: 3.20e-04). ETA=6 days, 11:13:45, max mem: 15.0 GB 
[06/16 16:38:19][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0347,	1.1314 s / batch. (data: 3.08e-04). ETA=6 days, 11:11:02, max mem: 15.0 GB 
[06/16 16:40:12][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0348,	1.1354 s / batch. (data: 3.10e-04). ETA=6 days, 11:42:10, max mem: 15.0 GB 
[06/16 16:42:05][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0479,	1.1303 s / batch. (data: 2.78e-04). ETA=6 days, 10:58:52, max mem: 15.0 GB 
[06/16 16:43:58][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0338,	1.1295 s / batch. (data: 3.10e-04). ETA=6 days, 10:50:06, max mem: 15.0 GB 
[06/16 16:45:51][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0187,	1.1354 s / batch. (data: 3.70e-04). ETA=6 days, 11:36:49, max mem: 15.0 GB 
[06/16 16:47:44][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0154,	1.1259 s / batch. (data: 4.20e-04). ETA=6 days, 10:16:51, max mem: 15.0 GB 
[06/16 16:49:36][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0519,	1.1255 s / batch. (data: 3.00e-04). ETA=6 days, 10:11:22, max mem: 15.0 GB 
[06/16 16:51:29][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0202,	1.1276 s / batch. (data: 2.80e-04). ETA=6 days, 10:26:36, max mem: 15.0 GB 
[06/16 16:53:22][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0476,	1.1295 s / batch. (data: 2.99e-04). ETA=6 days, 10:40:20, max mem: 15.0 GB 
[06/16 16:55:15][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0113,	1.1298 s / batch. (data: 2.68e-04). ETA=6 days, 10:41:16, max mem: 15.0 GB 
[06/16 16:57:08][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0399,	1.1261 s / batch. (data: 3.04e-04). ETA=6 days, 10:09:08, max mem: 15.0 GB 
[06/16 16:59:00][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0312,	1.1309 s / batch. (data: 2.99e-04). ETA=6 days, 10:46:27, max mem: 15.0 GB 
[06/16 17:00:53][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0267,	1.1283 s / batch. (data: 3.34e-04). ETA=6 days, 10:22:52, max mem: 15.0 GB 
[06/16 17:02:46][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0069,	1.1305 s / batch. (data: 3.24e-04). ETA=6 days, 10:39:36, max mem: 15.0 GB 
[06/16 17:04:39][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0382,	1.1270 s / batch. (data: 4.41e-04). ETA=6 days, 10:08:37, max mem: 15.0 GB 
[06/16 17:06:31][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0199,	1.1267 s / batch. (data: 3.08e-04). ETA=6 days, 10:04:45, max mem: 15.0 GB 
[06/16 17:08:24][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0665,	1.1278 s / batch. (data: 3.46e-04). ETA=6 days, 10:11:42, max mem: 15.0 GB 
[06/16 17:10:17][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0342,	1.1295 s / batch. (data: 2.78e-04). ETA=6 days, 10:23:48, max mem: 15.0 GB 
[06/16 17:12:10][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0805,	1.1259 s / batch. (data: 3.28e-04). ETA=6 days, 9:51:57, max mem: 15.0 GB 
[06/16 17:14:03][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0434,	1.1276 s / batch. (data: 3.23e-04). ETA=6 days, 10:04:21, max mem: 15.0 GB 
[06/16 17:15:56][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0040,	1.1253 s / batch. (data: 3.38e-04). ETA=6 days, 9:43:34, max mem: 15.0 GB 
[06/16 17:17:48][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0415,	1.1196 s / batch. (data: 3.36e-04). ETA=6 days, 8:55:01, max mem: 15.0 GB 
[06/16 17:19:41][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0078,	1.1276 s / batch. (data: 2.65e-04). ETA=6 days, 9:58:49, max mem: 15.0 GB 
[06/16 17:21:34][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0451,	1.1274 s / batch. (data: 4.23e-04). ETA=6 days, 9:54:48, max mem: 15.0 GB 
[06/16 17:23:27][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0063,	1.1300 s / batch. (data: 3.44e-04). ETA=6 days, 10:14:34, max mem: 15.0 GB 
[06/16 17:25:19][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0187,	1.1279 s / batch. (data: 2.92e-04). ETA=6 days, 9:55:26, max mem: 15.0 GB 
[06/16 17:27:12][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0037,	1.1273 s / batch. (data: 3.29e-04). ETA=6 days, 9:48:25, max mem: 15.0 GB 
[06/16 17:29:05][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0409,	1.1266 s / batch. (data: 3.29e-04). ETA=6 days, 9:41:07, max mem: 15.0 GB 
[06/16 17:30:58][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0066,	1.1317 s / batch. (data: 3.86e-04). ETA=6 days, 10:20:52, max mem: 15.0 GB 
[06/16 17:32:50][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0461,	1.1282 s / batch. (data: 3.43e-04). ETA=6 days, 9:50:07, max mem: 15.0 GB 
[06/16 17:34:43][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0367,	1.1272 s / batch. (data: 4.24e-04). ETA=6 days, 9:40:19, max mem: 15.0 GB 
[06/16 17:36:36][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0384,	1.1304 s / batch. (data: 2.94e-04). ETA=6 days, 10:05:05, max mem: 15.0 GB 
[06/16 17:38:29][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0362,	1.1256 s / batch. (data: 3.82e-04). ETA=6 days, 9:23:10, max mem: 15.0 GB 
[06/16 17:40:22][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0180,	1.1238 s / batch. (data: 3.62e-04). ETA=6 days, 9:07:18, max mem: 15.0 GB 
[06/16 17:42:14][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0120,	1.1289 s / batch. (data: 1.17e-04). ETA=6 days, 9:47:02, max mem: 15.0 GB 
[06/16 17:42:20][INFO] visual_prompt:  319: Epoch 2 / 100: avg data time: 4.28e-03, avg batch time: 1.1363, average train loss: 0.0439
[06/16 17:51:09][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.011, 5.1201 s / batch. (data: 2.13e-04)max mem: 14.95011 GB 
[06/16 17:59:20][INFO] visual_prompt:  476: Inference (val):avg data time: 1.50e-04, avg batch time: 5.1088, average loss: 0.0118
[06/16 17:59:20][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_shallow_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/16 17:59:20][INFO] visual_prompt:  257: Training 3 / 100 epoch, with learning rate 0.2
[06/16 18:01:56][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0099,	1.1228 s / batch. (data: 2.86e-04). ETA=6 days, 8:55:04, max mem: 15.0 GB 
[06/16 18:03:49][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0611,	1.1245 s / batch. (data: 3.32e-04). ETA=6 days, 9:06:59, max mem: 15.0 GB 
[06/16 18:05:42][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0134,	1.1267 s / batch. (data: 3.28e-04). ETA=6 days, 9:22:44, max mem: 15.0 GB 
[06/16 18:07:35][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0670,	1.1270 s / batch. (data: 3.27e-04). ETA=6 days, 9:23:46, max mem: 15.0 GB 
[06/16 18:09:27][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0470,	1.1201 s / batch. (data: 3.22e-04). ETA=6 days, 8:25:52, max mem: 15.0 GB 
[06/16 18:11:20][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0185,	1.1286 s / batch. (data: 3.30e-04). ETA=6 days, 9:33:13, max mem: 15.0 GB 
[06/16 18:13:13][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0465,	1.1255 s / batch. (data: 2.46e-04). ETA=6 days, 9:05:36, max mem: 15.0 GB 
[06/16 18:15:06][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0487,	1.1270 s / batch. (data: 3.48e-04). ETA=6 days, 9:16:20, max mem: 15.0 GB 
[06/16 18:16:58][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0204,	1.1296 s / batch. (data: 3.04e-04). ETA=6 days, 9:35:13, max mem: 15.0 GB 
[06/16 18:18:51][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0087,	1.1276 s / batch. (data: 3.24e-04). ETA=6 days, 9:17:28, max mem: 15.0 GB 
[06/16 18:20:44][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0051,	1.1314 s / batch. (data: 3.35e-04). ETA=6 days, 9:46:36, max mem: 15.0 GB 
[06/16 18:22:37][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0188,	1.1307 s / batch. (data: 3.16e-04). ETA=6 days, 9:39:12, max mem: 15.0 GB 
[06/16 18:24:30][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0483,	1.1266 s / batch. (data: 3.12e-04). ETA=6 days, 9:03:42, max mem: 15.0 GB 
[06/16 18:26:23][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0228,	1.1334 s / batch. (data: 3.52e-04). ETA=6 days, 9:57:24, max mem: 15.0 GB 
[06/16 18:28:16][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0193,	1.1258 s / batch. (data: 3.32e-04). ETA=6 days, 8:52:55, max mem: 15.0 GB 
[06/16 18:30:09][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0060,	1.1318 s / batch. (data: 3.14e-04). ETA=6 days, 9:39:55, max mem: 15.0 GB 
[06/16 18:32:02][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0046,	1.1314 s / batch. (data: 3.02e-04). ETA=6 days, 9:34:52, max mem: 15.0 GB 
[06/16 18:33:55][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0052,	1.1253 s / batch. (data: 3.95e-04). ETA=6 days, 8:43:26, max mem: 15.0 GB 
[06/16 18:35:48][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0080,	1.1263 s / batch. (data: 2.75e-04). ETA=6 days, 8:49:55, max mem: 15.0 GB 
[06/16 18:37:40][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0071,	1.1277 s / batch. (data: 3.05e-04). ETA=6 days, 8:59:04, max mem: 15.0 GB 
[06/16 18:39:33][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0500,	1.1260 s / batch. (data: 3.05e-04). ETA=6 days, 8:43:37, max mem: 15.0 GB 
[06/16 18:41:26][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0486,	1.1263 s / batch. (data: 3.15e-04). ETA=6 days, 8:43:52, max mem: 15.0 GB 
[06/16 18:43:18][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0453,	1.1230 s / batch. (data: 2.60e-04). ETA=6 days, 8:15:09, max mem: 15.0 GB 
[06/16 18:45:11][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0115,	1.1261 s / batch. (data: 4.18e-04). ETA=6 days, 8:39:11, max mem: 15.0 GB 
[06/16 18:47:04][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0101,	1.1254 s / batch. (data: 3.20e-04). ETA=6 days, 8:31:07, max mem: 15.0 GB 
[06/16 18:48:56][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0114,	1.1305 s / batch. (data: 4.77e-04). ETA=6 days, 9:10:39, max mem: 15.0 GB 
[06/16 18:50:49][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0079,	1.1271 s / batch. (data: 3.30e-04). ETA=6 days, 8:41:10, max mem: 15.0 GB 
[06/16 18:52:42][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0141,	1.1284 s / batch. (data: 3.10e-04). ETA=6 days, 8:49:59, max mem: 15.0 GB 
[06/16 18:54:35][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0062,	1.1249 s / batch. (data: 2.80e-04). ETA=6 days, 8:19:36, max mem: 15.0 GB 
[06/16 18:56:28][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0061,	1.1290 s / batch. (data: 3.62e-04). ETA=6 days, 8:50:52, max mem: 15.0 GB 
[06/16 18:58:21][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0070,	1.1241 s / batch. (data: 3.58e-04). ETA=6 days, 8:09:42, max mem: 15.0 GB 
[06/16 19:00:13][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0054,	1.1295 s / batch. (data: 3.19e-04). ETA=6 days, 8:51:43, max mem: 15.0 GB 
[06/16 19:02:06][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0251,	1.1319 s / batch. (data: 3.60e-04). ETA=6 days, 9:09:04, max mem: 15.0 GB 
[06/16 19:03:59][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0078,	1.1254 s / batch. (data: 2.99e-04). ETA=6 days, 8:14:10, max mem: 15.0 GB 
[06/16 19:05:52][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0158,	1.1325 s / batch. (data: 2.78e-04). ETA=6 days, 9:10:13, max mem: 15.0 GB 
[06/16 19:07:45][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0082,	1.1290 s / batch. (data: 3.63e-04). ETA=6 days, 8:40:02, max mem: 15.0 GB 
[06/16 19:09:38][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0079,	1.1226 s / batch. (data: 3.33e-04). ETA=6 days, 7:46:11, max mem: 15.0 GB 
[06/16 19:11:30][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0708,	1.1273 s / batch. (data: 3.94e-04). ETA=6 days, 8:22:16, max mem: 15.0 GB 
[06/16 19:13:23][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0234,	1.1262 s / batch. (data: 3.75e-04). ETA=6 days, 8:11:26, max mem: 15.0 GB 
[06/16 19:15:16][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0057,	1.1405 s / batch. (data: 3.33e-04). ETA=6 days, 10:05:46, max mem: 15.0 GB 
[06/16 19:17:09][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0611,	1.1287 s / batch. (data: 4.70e-04). ETA=6 days, 8:27:56, max mem: 15.0 GB 
[06/16 19:19:01][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0062,	1.1282 s / batch. (data: 3.15e-04). ETA=6 days, 8:21:50, max mem: 15.0 GB 
[06/16 19:20:54][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0511,	1.1251 s / batch. (data: 3.73e-04). ETA=6 days, 7:55:10, max mem: 15.0 GB 
[06/16 19:22:47][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0295,	1.1246 s / batch. (data: 3.95e-04). ETA=6 days, 7:49:09, max mem: 15.0 GB 
[06/16 19:24:39][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0137,	1.1227 s / batch. (data: 2.53e-04). ETA=6 days, 7:31:45, max mem: 15.0 GB 
[06/16 19:26:32][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0066,	1.1287 s / batch. (data: 2.56e-04). ETA=6 days, 8:18:35, max mem: 15.0 GB 
[06/16 19:28:25][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0130,	1.1314 s / batch. (data: 4.30e-04). ETA=6 days, 8:38:21, max mem: 15.0 GB 
[06/16 19:30:18][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0123,	1.1323 s / batch. (data: 4.03e-04). ETA=6 days, 8:44:15, max mem: 15.0 GB 
[06/16 19:32:11][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0139,	1.1289 s / batch. (data: 4.78e-04). ETA=6 days, 8:14:51, max mem: 15.0 GB 
[06/16 19:34:04][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0078,	1.1297 s / batch. (data: 1.21e-04). ETA=6 days, 8:19:25, max mem: 15.0 GB 
[06/16 19:34:09][INFO] visual_prompt:  319: Epoch 3 / 100: avg data time: 4.32e-03, avg batch time: 1.1367, average train loss: 0.0245
[06/16 19:42:58][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.017, 5.1243 s / batch. (data: 2.00e-04)max mem: 14.95011 GB 
[06/16 19:51:07][INFO] visual_prompt:  476: Inference (val):avg data time: 1.46e-04, avg batch time: 5.0987, average loss: 0.0168
[06/16 19:51:07][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_shallow_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/16 19:51:07][INFO] visual_prompt:  257: Training 4 / 100 epoch, with learning rate 0.3
[06/16 19:53:46][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0086,	1.1290 s / batch. (data: 2.81e-04). ETA=6 days, 8:11:37, max mem: 15.0 GB 
[06/16 19:55:38][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0272,	1.1247 s / batch. (data: 2.44e-04). ETA=6 days, 7:35:11, max mem: 15.0 GB 
[06/16 19:57:31][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0370,	1.1284 s / batch. (data: 3.08e-04). ETA=6 days, 8:02:44, max mem: 15.0 GB 
[06/16 19:59:24][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0201,	1.1321 s / batch. (data: 4.46e-04). ETA=6 days, 8:30:45, max mem: 15.0 GB 
[06/16 20:01:17][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0328,	1.1269 s / batch. (data: 3.85e-04). ETA=6 days, 7:46:56, max mem: 15.0 GB 
[06/16 20:03:09][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0142,	1.1295 s / batch. (data: 3.09e-04). ETA=6 days, 8:06:26, max mem: 15.0 GB 
[06/16 20:05:02][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0450,	1.1331 s / batch. (data: 3.26e-04). ETA=6 days, 8:33:38, max mem: 15.0 GB 
[06/16 20:06:55][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0383,	1.1302 s / batch. (data: 3.72e-04). ETA=6 days, 8:08:06, max mem: 15.0 GB 
[06/16 20:08:48][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0076,	1.1288 s / batch. (data: 2.66e-04). ETA=6 days, 7:54:47, max mem: 15.0 GB 
[06/16 20:10:41][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0050,	1.1283 s / batch. (data: 3.65e-04). ETA=6 days, 7:48:34, max mem: 15.0 GB 
[06/16 20:12:34][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0082,	1.1279 s / batch. (data: 3.43e-04). ETA=6 days, 7:43:57, max mem: 15.0 GB 
[06/16 20:14:27][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0144,	1.1280 s / batch. (data: 3.77e-04). ETA=6 days, 7:42:45, max mem: 15.0 GB 
[06/16 20:16:20][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0455,	1.1224 s / batch. (data: 3.18e-04). ETA=6 days, 6:55:29, max mem: 15.0 GB 
[06/16 20:18:13][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0078,	1.1270 s / batch. (data: 4.11e-04). ETA=6 days, 7:30:58, max mem: 15.0 GB 
[06/16 20:20:06][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0068,	1.1269 s / batch. (data: 2.23e-04). ETA=6 days, 7:28:23, max mem: 15.0 GB 
[06/16 20:21:59][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0706,	1.1268 s / batch. (data: 2.88e-04). ETA=6 days, 7:25:21, max mem: 15.0 GB 
[06/16 20:23:51][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0364,	1.1276 s / batch. (data: 3.20e-04). ETA=6 days, 7:29:49, max mem: 15.0 GB 
[06/16 20:25:44][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0098,	1.1234 s / batch. (data: 3.29e-04). ETA=6 days, 6:54:46, max mem: 15.0 GB 
[06/16 20:27:37][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0486,	1.1241 s / batch. (data: 3.70e-04). ETA=6 days, 6:58:04, max mem: 15.0 GB 
[06/16 20:29:29][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.1154,	1.1244 s / batch. (data: 2.80e-04). ETA=6 days, 6:58:19, max mem: 15.0 GB 
[06/16 20:31:22][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0193,	1.1285 s / batch. (data: 2.57e-04). ETA=6 days, 7:29:37, max mem: 15.0 GB 
[06/16 20:33:15][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0422,	1.1260 s / batch. (data: 2.91e-04). ETA=6 days, 7:07:40, max mem: 15.0 GB 
[06/16 20:35:08][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0480,	1.1295 s / batch. (data: 3.20e-04). ETA=6 days, 7:34:02, max mem: 15.0 GB 
[06/16 20:37:00][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0066,	1.1279 s / batch. (data: 3.63e-04). ETA=6 days, 7:19:42, max mem: 15.0 GB 
[06/16 20:38:53][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0071,	1.1271 s / batch. (data: 2.67e-04). ETA=6 days, 7:11:25, max mem: 15.0 GB 
[06/16 20:40:46][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0453,	1.1288 s / batch. (data: 3.47e-04). ETA=6 days, 7:22:53, max mem: 15.0 GB 
[06/16 20:42:39][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0061,	1.1283 s / batch. (data: 2.98e-04). ETA=6 days, 7:16:48, max mem: 15.0 GB 
[06/16 20:44:32][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0320,	1.1252 s / batch. (data: 2.92e-04). ETA=6 days, 6:49:51, max mem: 15.0 GB 
[06/16 20:46:24][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0249,	1.1252 s / batch. (data: 2.66e-04). ETA=6 days, 6:48:30, max mem: 15.0 GB 
[06/16 20:48:17][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0524,	1.1227 s / batch. (data: 3.38e-04). ETA=6 days, 6:26:25, max mem: 15.0 GB 
[06/16 20:50:10][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0059,	1.1288 s / batch. (data: 3.24e-04). ETA=6 days, 7:13:41, max mem: 15.0 GB 
[06/16 20:52:03][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0305,	1.1311 s / batch. (data: 3.67e-04). ETA=6 days, 7:29:38, max mem: 15.0 GB 
[06/16 20:53:56][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0216,	1.1337 s / batch. (data: 3.77e-04). ETA=6 days, 7:49:26, max mem: 15.0 GB 
[06/16 20:55:49][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0108,	1.1251 s / batch. (data: 3.16e-04). ETA=6 days, 6:38:10, max mem: 15.0 GB 
[06/16 20:57:42][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0083,	1.1287 s / batch. (data: 2.87e-04). ETA=6 days, 7:04:47, max mem: 15.0 GB 
[06/16 20:59:34][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0394,	1.1199 s / batch. (data: 3.35e-04). ETA=6 days, 5:52:19, max mem: 15.0 GB 
[06/16 21:01:27][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0081,	1.1335 s / batch. (data: 3.06e-04). ETA=6 days, 7:39:53, max mem: 15.0 GB 
[06/16 21:03:20][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0265,	1.1261 s / batch. (data: 2.79e-04). ETA=6 days, 6:38:42, max mem: 15.0 GB 
[06/16 21:05:12][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0435,	1.1291 s / batch. (data: 3.51e-04). ETA=6 days, 7:00:46, max mem: 15.0 GB 
[06/16 21:07:05][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0497,	1.1276 s / batch. (data: 3.17e-04). ETA=6 days, 6:47:09, max mem: 15.0 GB 
[06/16 21:08:58][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0726,	1.1273 s / batch. (data: 3.15e-04). ETA=6 days, 6:42:13, max mem: 15.0 GB 
[06/16 21:10:51][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0510,	1.1263 s / batch. (data: 4.12e-04). ETA=6 days, 6:32:53, max mem: 15.0 GB 
[06/16 21:12:44][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0206,	1.1315 s / batch. (data: 3.31e-04). ETA=6 days, 7:12:41, max mem: 15.0 GB 
[06/16 21:14:37][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0236,	1.1271 s / batch. (data: 2.91e-04). ETA=6 days, 6:35:35, max mem: 15.0 GB 
[06/16 21:16:30][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0074,	1.1317 s / batch. (data: 2.48e-04). ETA=6 days, 7:10:27, max mem: 15.0 GB 
[06/16 21:18:22][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0185,	1.1259 s / batch. (data: 3.19e-04). ETA=6 days, 6:22:13, max mem: 15.0 GB 
[06/16 21:20:15][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0420,	1.1260 s / batch. (data: 3.08e-04). ETA=6 days, 6:21:08, max mem: 15.0 GB 
[06/16 21:22:08][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0752,	1.1254 s / batch. (data: 3.10e-04). ETA=6 days, 6:13:59, max mem: 15.0 GB 
[06/16 21:24:01][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0425,	1.1269 s / batch. (data: 3.02e-04). ETA=6 days, 6:24:45, max mem: 15.0 GB 
[06/16 21:25:54][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0278,	1.1265 s / batch. (data: 1.17e-04). ETA=6 days, 6:19:08, max mem: 15.0 GB 
[06/16 21:25:59][INFO] visual_prompt:  319: Epoch 4 / 100: avg data time: 4.31e-03, avg batch time: 1.1373, average train loss: 0.0279
[06/16 21:34:48][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.025, 5.1054 s / batch. (data: 9.63e-05)max mem: 14.95011 GB 
[06/16 21:42:57][INFO] visual_prompt:  476: Inference (val):avg data time: 1.43e-04, avg batch time: 5.0976, average loss: 0.0243
[06/16 21:42:57][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_shallow_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/16 21:42:57][INFO] visual_prompt:  257: Training 5 / 100 epoch, with learning rate 0.4
[06/16 21:45:37][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0079,	1.1297 s / batch. (data: 3.32e-04). ETA=6 days, 6:42:46, max mem: 15.0 GB 
[06/16 21:47:30][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0271,	1.1232 s / batch. (data: 2.85e-04). ETA=6 days, 5:49:02, max mem: 15.0 GB 
[06/16 21:49:23][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0091,	1.1256 s / batch. (data: 3.40e-04). ETA=6 days, 6:06:07, max mem: 15.0 GB 
[06/16 21:51:16][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0222,	1.1279 s / batch. (data: 2.46e-04). ETA=6 days, 6:23:05, max mem: 15.0 GB 
[06/16 21:53:09][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0129,	1.1275 s / batch. (data: 2.73e-04). ETA=6 days, 6:17:31, max mem: 15.0 GB 
[06/16 21:55:01][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0673,	1.1301 s / batch. (data: 3.90e-04). ETA=6 days, 6:36:56, max mem: 15.0 GB 
[06/16 21:56:54][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0468,	1.1319 s / batch. (data: 4.47e-04). ETA=6 days, 6:48:54, max mem: 15.0 GB 
[06/16 21:58:48][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0471,	1.1342 s / batch. (data: 3.38e-04). ETA=6 days, 7:05:33, max mem: 15.0 GB 
[06/16 22:00:41][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0223,	1.1336 s / batch. (data: 3.24e-04). ETA=6 days, 6:58:51, max mem: 15.0 GB 
[06/16 22:02:34][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0120,	1.1326 s / batch. (data: 3.65e-04). ETA=6 days, 6:49:18, max mem: 15.0 GB 
[06/16 22:04:27][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0334,	1.1326 s / batch. (data: 3.71e-04). ETA=6 days, 6:47:11, max mem: 15.0 GB 
[06/16 22:06:20][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0458,	1.1266 s / batch. (data: 2.89e-04). ETA=6 days, 5:57:38, max mem: 15.0 GB 
[06/16 22:08:13][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0259,	1.1282 s / batch. (data: 3.90e-04). ETA=6 days, 6:07:59, max mem: 15.0 GB 
[06/16 22:10:06][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0266,	1.1295 s / batch. (data: 3.32e-04). ETA=6 days, 6:16:55, max mem: 15.0 GB 
[06/16 22:11:59][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0105,	1.1292 s / batch. (data: 3.29e-04). ETA=6 days, 6:12:42, max mem: 15.0 GB 
[06/16 22:13:52][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0474,	1.1253 s / batch. (data: 3.14e-04). ETA=6 days, 5:39:42, max mem: 15.0 GB 
[06/16 22:15:45][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0249,	1.1248 s / batch. (data: 3.02e-04). ETA=6 days, 5:33:25, max mem: 15.0 GB 
[06/16 22:17:37][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0209,	1.1225 s / batch. (data: 3.12e-04). ETA=6 days, 5:13:18, max mem: 15.0 GB 
[06/16 22:19:30][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0475,	1.1246 s / batch. (data: 3.08e-04). ETA=6 days, 5:28:42, max mem: 15.0 GB 
[06/16 22:21:22][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0180,	1.1288 s / batch. (data: 3.84e-04). ETA=6 days, 6:00:03, max mem: 15.0 GB 
[06/16 22:23:15][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0462,	1.1245 s / batch. (data: 4.44e-04). ETA=6 days, 5:24:00, max mem: 15.0 GB 
[06/16 22:25:08][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0075,	1.1280 s / batch. (data: 3.29e-04). ETA=6 days, 5:49:31, max mem: 15.0 GB 
[06/16 22:27:00][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0287,	1.1281 s / batch. (data: 3.44e-04). ETA=6 days, 5:48:28, max mem: 15.0 GB 
[06/16 22:28:53][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0409,	1.1224 s / batch. (data: 2.84e-04). ETA=6 days, 5:01:08, max mem: 15.0 GB 
[06/16 22:30:45][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0082,	1.1293 s / batch. (data: 3.54e-04). ETA=6 days, 5:54:34, max mem: 15.0 GB 
[06/16 22:32:38][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0438,	1.1240 s / batch. (data: 3.73e-04). ETA=6 days, 5:10:49, max mem: 15.0 GB 
[06/16 22:34:31][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0106,	1.1278 s / batch. (data: 2.97e-04). ETA=6 days, 5:38:58, max mem: 15.0 GB 
[06/16 22:36:24][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0518,	1.1296 s / batch. (data: 3.22e-04). ETA=6 days, 5:51:30, max mem: 15.0 GB 
[06/16 22:38:17][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0210,	1.1288 s / batch. (data: 3.79e-04). ETA=6 days, 5:43:24, max mem: 15.0 GB 
[06/16 22:40:09][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0204,	1.1284 s / batch. (data: 3.44e-04). ETA=6 days, 5:38:05, max mem: 15.0 GB 
[06/16 22:42:02][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0453,	1.1266 s / batch. (data: 3.30e-04). ETA=6 days, 5:21:47, max mem: 15.0 GB 
[06/16 22:43:55][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0314,	1.1296 s / batch. (data: 3.25e-04). ETA=6 days, 5:44:10, max mem: 15.0 GB 
[06/16 22:45:48][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0288,	1.1265 s / batch. (data: 2.95e-04). ETA=6 days, 5:17:04, max mem: 15.0 GB 
[06/16 22:47:40][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0093,	1.1251 s / batch. (data: 3.45e-04). ETA=6 days, 5:04:22, max mem: 15.0 GB 
[06/16 22:49:33][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0510,	1.1243 s / batch. (data: 3.49e-04). ETA=6 days, 4:56:20, max mem: 15.0 GB 
[06/16 22:51:26][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0196,	1.1290 s / batch. (data: 3.08e-04). ETA=6 days, 5:31:12, max mem: 15.0 GB 
[06/16 22:53:18][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0223,	1.1284 s / batch. (data: 3.10e-04). ETA=6 days, 5:25:00, max mem: 15.0 GB 
[06/16 22:55:11][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0185,	1.1235 s / batch. (data: 4.83e-04). ETA=6 days, 4:43:42, max mem: 15.0 GB 
[06/16 22:57:04][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0449,	1.1228 s / batch. (data: 3.08e-04). ETA=6 days, 4:36:15, max mem: 15.0 GB 
[06/16 22:58:56][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0451,	1.1275 s / batch. (data: 3.18e-04). ETA=6 days, 5:11:49, max mem: 15.0 GB 
[06/16 23:00:49][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0188,	1.1287 s / batch. (data: 3.55e-04). ETA=6 days, 5:19:48, max mem: 15.0 GB 
[06/16 23:02:41][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0096,	1.1228 s / batch. (data: 3.14e-04). ETA=6 days, 4:30:42, max mem: 15.0 GB 
