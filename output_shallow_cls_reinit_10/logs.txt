[06/16 23:04:53][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/16 23:04:53][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,4,5,6
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/16 23:04:53][INFO] visual_prompt:   99: Command line arguments: None
[06/16 23:04:53][INFO] visual_prompt:  108: Training with config:
[06/16 23:04:53][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': False,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls-reinit+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_shallow_cls_reinit_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 1.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'mse',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'USE_CLS_TOKEN': True,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/16 23:04:57][INFO] visual_prompt:   56: Total Parameters: 98103568	 Gradient Parameters: 12305680
[06/16 23:04:57][INFO] visual_prompt:   58: tuned percent:12.544
[06/16 23:04:57][INFO] visual_prompt:   44: Device used for model: 0
[06/16 23:04:57][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/16 23:04:57][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/16 23:05:00][INFO] visual_prompt:  158: Number of images: 1281167
[06/16 23:05:00][INFO] visual_prompt:  159: Number of classes: 1000
[06/16 23:05:00][INFO] visual_prompt:   78: Loading validation data...
[06/16 23:05:00][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/16 23:05:00][INFO] visual_prompt:  158: Number of images: 50000
[06/16 23:05:00][INFO] visual_prompt:  159: Number of classes: 1000
[06/16 23:05:00][INFO] visual_prompt:   81: Loading test data...
[06/16 23:05:00][INFO] visual_prompt:   83: ...no test data is constructed
[06/16 23:05:00][INFO] visual_prompt:  111: Constructing models...
[06/16 23:05:00][INFO] visual_prompt:  114: Setting up Evalutator...
[06/16 23:05:00][INFO] visual_prompt:  116: Setting up Trainer...
[06/16 23:05:00][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.deep_prompt_embeddings: True
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/16 23:05:00][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/16 23:05:00][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/16 23:05:00][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/16 23:05:00][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/16 23:05:00][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/16 23:05:00][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/16 23:05:00][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/16 23:05:00][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/16 23:05:00][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/16 23:05:00][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/16 23:05:00][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/16 23:05:00][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/16 23:05:00][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/16 23:05:00][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/16 23:05:00][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/16 23:05:00][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/16 23:05:00][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/16 23:05:00][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/16 23:05:00][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/16 23:05:00][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/16 23:05:00][INFO] visual_prompt:  238: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/16 23:05:00][INFO] visual_prompt:  257: Training 1 / 100 epoch, with learning rate 0.0
[06/16 23:07:19][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0292,	1.1746 s / batch. (data: 2.96e-04). ETA=6 days, 19:14:36, max mem: 15.1 GB 
[06/16 23:09:16][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0256,	1.1724 s / batch. (data: 3.84e-04). ETA=6 days, 18:53:54, max mem: 15.1 GB 
[06/16 23:11:13][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0211,	1.1785 s / batch. (data: 3.97e-04). ETA=6 days, 19:42:45, max mem: 15.1 GB 
[06/16 23:13:11][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0280,	1.1766 s / batch. (data: 3.02e-04). ETA=6 days, 19:25:08, max mem: 15.1 GB 
[06/16 23:15:08][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0204,	1.1736 s / batch. (data: 3.17e-04). ETA=6 days, 18:58:07, max mem: 15.1 GB 
[06/16 23:17:06][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0193,	1.1734 s / batch. (data: 3.02e-04). ETA=6 days, 18:54:20, max mem: 15.1 GB 
[06/16 23:19:03][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0201,	1.1738 s / batch. (data: 3.39e-04). ETA=6 days, 18:56:09, max mem: 15.1 GB 
[06/16 23:21:00][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0201,	1.1756 s / batch. (data: 3.21e-04). ETA=6 days, 19:09:05, max mem: 15.1 GB 
[06/16 23:22:58][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0268,	1.1737 s / batch. (data: 3.24e-04). ETA=6 days, 18:51:26, max mem: 15.1 GB 
[06/16 23:24:55][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0257,	1.1707 s / batch. (data: 3.36e-04). ETA=6 days, 18:24:06, max mem: 15.1 GB 
[06/16 23:26:52][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0212,	1.1685 s / batch. (data: 3.61e-04). ETA=6 days, 18:03:41, max mem: 15.1 GB 
[06/16 23:28:50][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0214,	1.1745 s / batch. (data: 2.80e-04). ETA=6 days, 18:51:30, max mem: 15.1 GB 
[06/16 23:30:47][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0246,	1.1748 s / batch. (data: 3.39e-04). ETA=6 days, 18:52:22, max mem: 15.1 GB 
[06/16 23:32:45][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0198,	1.1836 s / batch. (data: 3.72e-04). ETA=6 days, 20:03:35, max mem: 15.1 GB 
[06/16 23:34:42][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0197,	1.1734 s / batch. (data: 3.21e-04). ETA=6 days, 18:37:05, max mem: 15.1 GB 
[06/16 23:36:40][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0254,	1.1742 s / batch. (data: 3.29e-04). ETA=6 days, 18:41:20, max mem: 15.1 GB 
[06/19 11:59:02][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/19 11:59:02][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,4,5,6
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/19 11:59:02][INFO] visual_prompt:   99: Command line arguments: None
[06/19 11:59:02][INFO] visual_prompt:  108: Training with config:
[06/19 11:59:02][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': False,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': 512,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls-reinit+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_shallow_cls_reinit_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 1.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'mse',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'USE_CLS_TOKEN': True,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/19 11:59:06][INFO] visual_prompt:   56: Total Parameters: 94145552	 Gradient Parameters: 8347664
[06/19 11:59:06][INFO] visual_prompt:   58: tuned percent:8.867
[06/19 11:59:06][INFO] visual_prompt:   44: Device used for model: 0
[06/19 11:59:06][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/19 11:59:06][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/19 11:59:09][INFO] visual_prompt:  158: Number of images: 1281167
[06/19 11:59:09][INFO] visual_prompt:  159: Number of classes: 1000
[06/19 11:59:09][INFO] visual_prompt:   78: Loading validation data...
[06/19 11:59:09][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/19 11:59:09][INFO] visual_prompt:  158: Number of images: 50000
[06/19 11:59:09][INFO] visual_prompt:  159: Number of classes: 1000
[06/19 11:59:09][INFO] visual_prompt:   81: Loading test data...
[06/19 11:59:09][INFO] visual_prompt:   83: ...no test data is constructed
[06/19 11:59:09][INFO] visual_prompt:  111: Constructing models...
[06/19 11:59:09][INFO] visual_prompt:  114: Setting up Evalutator...
[06/19 11:59:09][INFO] visual_prompt:  116: Setting up Trainer...
[06/19 11:59:09][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.prompt_proj.weight: True
[06/19 11:59:09][INFO] visual_prompt:   59: module.enc.transformer.prompt_proj.bias: True
[06/19 11:59:09][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/19 11:59:09][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/19 11:59:09][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/19 11:59:09][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/19 11:59:09][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/19 11:59:09][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/19 11:59:09][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/19 11:59:09][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/19 11:59:09][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/19 11:59:09][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/19 11:59:09][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/19 11:59:09][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/19 11:59:09][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/19 11:59:09][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/19 11:59:09][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/19 11:59:09][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/19 11:59:09][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/19 11:59:09][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/19 11:59:09][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/19 11:59:09][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/19 11:59:09][INFO] visual_prompt:  238: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/19 11:59:09][INFO] visual_prompt:  257: Training 1 / 100 epoch, with learning rate 0.0
[06/19 12:05:02][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0504,	3.3406 s / batch. (data: 4.52e-04). ETA=19 days, 8:14:57, max mem: 15.0 GB 
[06/19 12:10:28][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0774,	3.3741 s / batch. (data: 2.99e-04). ETA=19 days, 12:48:55, max mem: 15.0 GB 
[06/19 12:14:30][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.1026,	3.4131 s / batch. (data: 4.56e-04). ETA=19 days, 18:08:00, max mem: 15.0 GB 
[06/19 12:20:06][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0591,	3.1911 s / batch. (data: 3.20e-04). ETA=18 days, 11:12:07, max mem: 15.0 GB 
[06/19 12:25:41][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0958,	3.4607 s / batch. (data: 3.38e-04). ETA=20 days, 0:33:48, max mem: 15.0 GB 
[06/19 12:31:13][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.1130,	3.2636 s / batch. (data: 2.88e-04). ETA=18 days, 21:05:32, max mem: 15.0 GB 
[06/19 12:36:43][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0797,	3.1763 s / batch. (data: 4.01e-04). ETA=18 days, 8:53:07, max mem: 15.0 GB 
[06/19 12:42:14][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0696,	3.3522 s / batch. (data: 2.89e-04). ETA=19 days, 9:12:54, max mem: 15.0 GB 
[06/19 12:47:49][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0529,	3.3461 s / batch. (data: 3.18e-04). ETA=19 days, 8:16:04, max mem: 15.0 GB 
[06/19 12:53:22][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0934,	3.0736 s / batch. (data: 3.26e-04). ETA=17 days, 18:22:16, max mem: 15.0 GB 
[06/19 12:58:55][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0491,	2.9796 s / batch. (data: 2.62e-04). ETA=17 days, 5:15:26, max mem: 15.0 GB 
[06/19 13:04:29][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0310,	3.3454 s / batch. (data: 5.27e-04). ETA=19 days, 7:53:18, max mem: 15.0 GB 
[06/19 13:10:02][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0456,	3.3725 s / batch. (data: 3.21e-04). ETA=19 days, 11:33:14, max mem: 15.0 GB 
[06/19 13:15:37][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0791,	3.3709 s / batch. (data: 2.82e-04). ETA=19 days, 11:14:56, max mem: 15.0 GB 
[06/19 13:21:10][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0462,	3.3358 s / batch. (data: 2.62e-04). ETA=19 days, 6:16:57, max mem: 15.0 GB 
[06/19 13:26:44][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0354,	3.3304 s / batch. (data: 3.25e-04). ETA=19 days, 5:26:39, max mem: 15.0 GB 
[06/19 13:32:21][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0767,	3.3787 s / batch. (data: 2.89e-04). ETA=19 days, 12:03:02, max mem: 15.0 GB 
[06/19 13:37:53][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0607,	3.2906 s / batch. (data: 3.12e-04). ETA=18 days, 23:45:01, max mem: 15.0 GB 
[06/19 13:43:26][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0372,	3.3647 s / batch. (data: 5.56e-04). ETA=19 days, 9:54:46, max mem: 15.0 GB 
[06/19 13:49:00][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0558,	3.0968 s / batch. (data: 3.16e-04). ETA=17 days, 20:44:10, max mem: 15.0 GB 
[06/19 13:54:35][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0939,	3.4351 s / batch. (data: 3.09e-04). ETA=19 days, 19:28:39, max mem: 15.0 GB 
[06/19 13:59:24][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0694,	3.3684 s / batch. (data: 3.38e-04). ETA=19 days, 10:08:59, max mem: 15.0 GB 
[06/19 14:04:55][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0311,	3.0378 s / batch. (data: 3.10e-04). ETA=17 days, 12:18:28, max mem: 15.0 GB 
[06/19 14:10:27][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0634,	3.3202 s / batch. (data: 3.16e-04). ETA=19 days, 3:18:02, max mem: 15.0 GB 
[06/19 14:16:00][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0437,	3.3409 s / batch. (data: 3.39e-04). ETA=19 days, 6:03:40, max mem: 15.0 GB 
[06/19 14:21:33][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0695,	3.3339 s / batch. (data: 3.73e-04). ETA=19 days, 5:00:10, max mem: 15.0 GB 
[06/19 14:27:04][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0888,	3.3917 s / batch. (data: 3.25e-04). ETA=19 days, 12:54:21, max mem: 15.0 GB 
[06/19 14:32:35][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0481,	3.3408 s / batch. (data: 3.14e-04). ETA=19 days, 5:46:00, max mem: 15.0 GB 
[06/19 14:38:06][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0424,	3.3218 s / batch. (data: 3.28e-04). ETA=19 days, 3:03:22, max mem: 15.0 GB 
[06/19 14:43:38][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0305,	2.9739 s / batch. (data: 2.85e-04). ETA=17 days, 2:53:39, max mem: 15.0 GB 
[06/19 14:49:10][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0999,	3.3187 s / batch. (data: 3.08e-04). ETA=19 days, 2:26:46, max mem: 15.0 GB 
[06/19 14:54:39][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0503,	3.3162 s / batch. (data: 3.01e-04). ETA=19 days, 2:00:01, max mem: 15.0 GB 
[06/19 15:00:07][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0741,	3.3424 s / batch. (data: 2.92e-04). ETA=19 days, 5:31:48, max mem: 15.0 GB 
[06/19 15:05:33][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0321,	2.8890 s / batch. (data: 3.89e-04). ETA=16 days, 14:50:50, max mem: 15.0 GB 
[06/19 15:10:59][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.1145,	3.2419 s / batch. (data: 4.19e-04). ETA=18 days, 15:28:31, max mem: 15.0 GB 
[06/19 15:16:26][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0541,	3.4094 s / batch. (data: 3.91e-04). ETA=19 days, 14:29:53, max mem: 15.0 GB 
[06/19 15:21:56][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.1175,	3.3056 s / batch. (data: 3.92e-04). ETA=19 days, 0:05:00, max mem: 15.0 GB 
[06/19 15:27:21][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0453,	3.2012 s / batch. (data: 3.27e-04). ETA=18 days, 9:35:28, max mem: 15.0 GB 
[06/19 15:32:48][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0987,	3.3943 s / batch. (data: 3.63e-04). ETA=19 days, 12:08:14, max mem: 15.0 GB 
[06/19 15:38:19][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.1174,	3.3401 s / batch. (data: 4.68e-04). ETA=19 days, 4:33:52, max mem: 15.0 GB 
[06/19 15:43:49][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0335,	3.3467 s / batch. (data: 3.33e-04). ETA=19 days, 5:22:32, max mem: 15.0 GB 
[06/19 15:49:16][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0661,	3.4042 s / batch. (data: 3.65e-04). ETA=19 days, 13:12:53, max mem: 15.0 GB 
[06/19 15:54:42][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0911,	3.0986 s / batch. (data: 3.40e-04). ETA=17 days, 19:00:05, max mem: 15.0 GB 
[06/19 16:00:09][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.1256,	3.2482 s / batch. (data: 3.40e-04). ETA=18 days, 15:31:59, max mem: 15.0 GB 
[06/19 16:05:34][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0807,	3.2196 s / batch. (data: 3.09e-04). ETA=18 days, 11:30:05, max mem: 15.0 GB 
[06/19 16:10:59][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0922,	3.1972 s / batch. (data: 3.28e-04). ETA=18 days, 8:19:35, max mem: 15.0 GB 
[06/19 16:16:25][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0452,	3.2544 s / batch. (data: 3.76e-04). ETA=18 days, 16:06:30, max mem: 15.0 GB 
[06/19 16:21:51][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0329,	3.3308 s / batch. (data: 3.07e-04). ETA=19 days, 2:32:37, max mem: 15.0 GB 
[06/19 16:27:17][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0302,	3.3076 s / batch. (data: 2.95e-04). ETA=18 days, 23:15:34, max mem: 15.0 GB 
[06/19 16:32:43][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0607,	3.4017 s / batch. (data: 2.63e-04). ETA=19 days, 12:06:55, max mem: 15.0 GB 
[06/19 16:32:57][INFO] visual_prompt:  319: Epoch 1 / 100: avg data time: 4.46e-03, avg batch time: 3.2828, average train loss: 0.0737
[06/19 16:41:51][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.071, 5.1369 s / batch. (data: 2.16e-04)max mem: 14.95420 GB 
[06/19 16:50:03][INFO] visual_prompt:  476: Inference (val):avg data time: 1.46e-04, avg batch time: 5.1294, average loss: 0.0733
[06/19 16:50:03][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_shallow_cls_reinit_10/val_imagenet_invariances.json
[06/19 16:50:03][INFO] visual_prompt:  257: Training 2 / 100 epoch, with learning rate 0.1
[06/19 17:29:16][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0732,	3.1986 s / batch. (data: 3.13e-04). ETA=18 days, 8:03:52, max mem: 15.0 GB 
[06/19 17:34:39][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0592,	2.9665 s / batch. (data: 4.54e-04). ETA=17 days, 0:03:31, max mem: 15.0 GB 
[06/19 17:40:07][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0471,	3.3662 s / batch. (data: 3.11e-04). ETA=19 days, 6:56:26, max mem: 15.0 GB 
[06/19 17:45:32][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0555,	3.3433 s / batch. (data: 4.25e-04). ETA=19 days, 3:41:45, max mem: 15.0 GB 
[06/19 17:51:00][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.1205,	3.2544 s / batch. (data: 3.77e-04). ETA=18 days, 15:23:03, max mem: 15.0 GB 
[06/19 17:56:24][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0479,	3.2105 s / batch. (data: 3.36e-04). ETA=18 days, 9:15:30, max mem: 15.0 GB 
[06/19 18:01:51][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0699,	3.2284 s / batch. (data: 4.39e-04). ETA=18 days, 11:37:52, max mem: 15.0 GB 
[06/19 18:07:15][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0492,	3.2171 s / batch. (data: 4.68e-04). ETA=18 days, 9:59:39, max mem: 15.0 GB 
[06/19 18:12:41][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0229,	3.2266 s / batch. (data: 3.46e-04). ETA=18 days, 11:12:06, max mem: 15.0 GB 
[06/19 18:18:07][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0463,	3.1867 s / batch. (data: 3.13e-04). ETA=18 days, 5:37:50, max mem: 15.0 GB 
[06/19 18:23:32][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0857,	3.3365 s / batch. (data: 3.09e-04). ETA=19 days, 2:07:21, max mem: 15.0 GB 
[06/19 18:28:59][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0539,	2.9157 s / batch. (data: 3.46e-04). ETA=16 days, 16:15:20, max mem: 15.0 GB 
[06/19 18:34:24][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0763,	3.2448 s / batch. (data: 3.04e-04). ETA=18 days, 13:20:51, max mem: 15.0 GB 
[06/19 18:39:50][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0499,	3.3103 s / batch. (data: 2.97e-04). ETA=18 days, 22:14:25, max mem: 15.0 GB 
[06/19 18:45:15][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0230,	3.1974 s / batch. (data: 3.64e-04). ETA=18 days, 6:39:34, max mem: 15.0 GB 
[06/19 18:50:38][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0207,	3.2258 s / batch. (data: 3.07e-04). ETA=18 days, 10:28:21, max mem: 15.0 GB 
[06/19 18:56:04][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0601,	3.1740 s / batch. (data: 4.13e-04). ETA=18 days, 3:16:10, max mem: 15.0 GB 
[06/19 19:01:30][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0934,	3.1866 s / batch. (data: 3.10e-04). ETA=18 days, 4:54:46, max mem: 15.0 GB 
[06/19 19:06:55][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0508,	3.3035 s / batch. (data: 3.36e-04). ETA=18 days, 20:51:25, max mem: 15.0 GB 
[06/19 19:12:19][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0532,	3.1870 s / batch. (data: 3.32e-04). ETA=18 days, 4:47:22, max mem: 15.0 GB 
[06/19 19:17:44][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0232,	3.3090 s / batch. (data: 4.34e-04). ETA=18 days, 21:25:23, max mem: 15.0 GB 
[06/19 19:23:09][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0563,	3.1682 s / batch. (data: 2.86e-04). ETA=18 days, 2:02:21, max mem: 15.0 GB 
[06/19 19:28:36][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0352,	3.3452 s / batch. (data: 3.51e-04). ETA=19 days, 2:11:21, max mem: 15.0 GB 
[06/19 19:34:02][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0638,	3.3337 s / batch. (data: 3.19e-04). ETA=19 days, 0:31:31, max mem: 15.0 GB 
[06/19 19:39:29][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0487,	3.3443 s / batch. (data: 4.99e-04). ETA=19 days, 1:53:24, max mem: 15.0 GB 
[06/19 19:44:52][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0537,	3.2502 s / batch. (data: 3.54e-04). ETA=18 days, 12:54:38, max mem: 15.0 GB 
[06/19 19:50:19][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0878,	3.2333 s / batch. (data: 3.67e-04). ETA=18 days, 10:30:28, max mem: 15.0 GB 
[06/19 19:55:43][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0299,	2.8704 s / batch. (data: 3.04e-04). ETA=16 days, 8:46:11, max mem: 15.0 GB 
[06/19 20:01:08][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0377,	3.2333 s / batch. (data: 3.13e-04). ETA=18 days, 10:20:10, max mem: 15.0 GB 
[06/19 20:06:36][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0215,	3.2525 s / batch. (data: 3.71e-04). ETA=18 days, 12:52:10, max mem: 15.0 GB 
[06/19 20:11:59][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0713,	3.3003 s / batch. (data: 2.91e-04). ETA=18 days, 19:18:43, max mem: 15.0 GB 
[06/19 20:17:26][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0218,	3.2579 s / batch. (data: 4.76e-04). ETA=18 days, 13:25:18, max mem: 15.0 GB 
[06/19 20:22:48][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0230,	3.1952 s / batch. (data: 4.08e-04). ETA=18 days, 4:46:06, max mem: 15.0 GB 
[06/19 20:28:13][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0719,	3.2629 s / batch. (data: 3.18e-04). ETA=18 days, 13:55:27, max mem: 15.0 GB 
[06/19 20:33:39][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0368,	3.3086 s / batch. (data: 3.46e-04). ETA=18 days, 20:05:10, max mem: 15.0 GB 
[06/19 20:39:06][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0822,	3.3742 s / batch. (data: 3.68e-04). ETA=19 days, 4:57:19, max mem: 15.0 GB 
[06/19 20:44:32][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0593,	3.1876 s / batch. (data: 3.12e-04). ETA=18 days, 3:22:04, max mem: 15.0 GB 
[06/19 20:49:59][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0435,	3.3449 s / batch. (data: 3.62e-04). ETA=19 days, 0:45:20, max mem: 15.0 GB 
[06/19 20:55:24][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0580,	3.3366 s / batch. (data: 3.02e-04). ETA=18 days, 23:32:06, max mem: 15.0 GB 
[06/19 21:00:51][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0534,	3.2265 s / batch. (data: 2.53e-04). ETA=18 days, 8:24:33, max mem: 15.0 GB 
[06/19 21:06:15][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0352,	3.3084 s / batch. (data: 3.04e-04). ETA=18 days, 19:29:48, max mem: 15.0 GB 
[06/19 21:11:41][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.1068,	3.3863 s / batch. (data: 3.19e-04). ETA=19 days, 6:02:24, max mem: 15.0 GB 
[06/19 21:17:07][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0902,	3.2618 s / batch. (data: 2.82e-04). ETA=18 days, 12:57:15, max mem: 15.0 GB 
[06/19 21:22:33][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0310,	3.2650 s / batch. (data: 3.17e-04). ETA=18 days, 13:18:43, max mem: 15.0 GB 
[06/19 21:28:01][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.1058,	3.2168 s / batch. (data: 5.15e-04). ETA=18 days, 6:38:27, max mem: 15.0 GB 
[06/19 21:33:27][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0987,	3.3102 s / batch. (data: 2.65e-04). ETA=18 days, 19:16:53, max mem: 15.0 GB 
[06/19 21:38:50][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0758,	3.3084 s / batch. (data: 4.82e-04). ETA=18 days, 18:56:39, max mem: 15.0 GB 
[06/19 21:44:17][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0974,	3.2857 s / batch. (data: 2.98e-04). ETA=18 days, 15:46:05, max mem: 15.0 GB 
[06/19 21:49:43][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0195,	3.2992 s / batch. (data: 3.32e-04). ETA=18 days, 17:30:56, max mem: 15.0 GB 
[06/19 21:55:11][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0390,	3.3417 s / batch. (data: 1.45e-04). ETA=18 days, 23:12:16, max mem: 15.0 GB 
[06/19 21:55:25][INFO] visual_prompt:  319: Epoch 2 / 100: avg data time: 4.44e-03, avg batch time: 3.6613, average train loss: 0.0608
[06/19 22:04:13][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.058, 5.1156 s / batch. (data: 6.18e-05)max mem: 14.95420 GB 
[06/19 22:12:24][INFO] visual_prompt:  476: Inference (val):avg data time: 1.62e-04, avg batch time: 5.1014, average loss: 0.0593
[06/19 22:12:24][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_shallow_cls_reinit_10/val_imagenet_invariances.json
[06/19 22:12:24][INFO] visual_prompt:  257: Training 3 / 100 epoch, with learning rate 0.2
[06/19 22:51:25][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0770,	3.3728 s / batch. (data: 3.28e-04). ETA=19 days, 3:20:39, max mem: 15.0 GB 
[06/19 22:56:43][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0244,	3.2222 s / batch. (data: 3.16e-04). ETA=18 days, 6:44:38, max mem: 15.0 GB 
[06/19 23:02:03][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0398,	3.3170 s / batch. (data: 2.95e-04). ETA=18 days, 19:33:33, max mem: 15.0 GB 
[06/19 23:07:23][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0527,	3.2278 s / batch. (data: 3.30e-04). ETA=18 days, 7:20:13, max mem: 15.0 GB 
[06/19 23:12:43][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0339,	3.2437 s / batch. (data: 2.65e-04). ETA=18 days, 9:24:35, max mem: 15.0 GB 
[06/19 23:18:03][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0212,	3.1956 s / batch. (data: 4.55e-04). ETA=18 days, 2:46:13, max mem: 15.0 GB 
[06/19 23:23:25][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0385,	3.1008 s / batch. (data: 3.20e-04). ETA=17 days, 13:47:04, max mem: 15.0 GB 
[06/19 23:28:46][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0574,	3.3231 s / batch. (data: 4.12e-04). ETA=18 days, 19:55:41, max mem: 15.0 GB 
[06/19 23:34:08][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0784,	3.1723 s / batch. (data: 3.31e-04). ETA=17 days, 23:20:34, max mem: 15.0 GB 
[06/19 23:39:28][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0998,	3.1975 s / batch. (data: 3.45e-04). ETA=18 days, 2:40:41, max mem: 15.0 GB 
[06/19 23:44:49][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0435,	3.3542 s / batch. (data: 3.02e-04). ETA=18 days, 23:53:26, max mem: 15.0 GB 
[06/19 23:50:08][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0474,	3.1359 s / batch. (data: 5.83e-04). ETA=17 days, 18:07:39, max mem: 15.0 GB 
[06/19 23:55:32][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.1009,	3.1682 s / batch. (data: 3.64e-04). ETA=17 days, 22:25:51, max mem: 15.0 GB 
[06/20 00:00:58][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.1004,	3.2394 s / batch. (data: 3.83e-04). ETA=18 days, 8:00:31, max mem: 15.0 GB 
[06/20 00:06:26][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0339,	3.3426 s / batch. (data: 2.77e-04). ETA=18 days, 21:55:54, max mem: 15.0 GB 
[06/20 00:11:54][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0560,	3.3132 s / batch. (data: 2.96e-04). ETA=18 days, 17:51:12, max mem: 15.0 GB 
[06/20 00:17:20][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0967,	3.3537 s / batch. (data: 3.19e-04). ETA=18 days, 23:15:07, max mem: 15.0 GB 
[06/20 00:22:45][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0982,	3.3452 s / batch. (data: 4.23e-04). ETA=18 days, 22:00:30, max mem: 15.0 GB 
[06/20 00:28:11][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0202,	3.2268 s / batch. (data: 3.68e-04). ETA=18 days, 5:50:45, max mem: 15.0 GB 
[06/20 00:33:39][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0216,	3.2547 s / batch. (data: 5.50e-04). ETA=18 days, 9:33:00, max mem: 15.0 GB 
[06/20 00:39:05][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0837,	3.3933 s / batch. (data: 3.03e-04). ETA=19 days, 4:15:15, max mem: 15.0 GB 
[06/20 00:44:33][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0520,	3.2990 s / batch. (data: 3.94e-04). ETA=18 days, 15:22:37, max mem: 15.0 GB 
[06/20 00:49:58][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0343,	3.2735 s / batch. (data: 2.77e-04). ETA=18 days, 11:49:51, max mem: 15.0 GB 
[06/20 00:55:25][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0984,	3.2215 s / batch. (data: 4.60e-04). ETA=18 days, 4:41:04, max mem: 15.0 GB 
[06/20 01:00:50][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0313,	3.2973 s / batch. (data: 3.54e-04). ETA=18 days, 14:51:53, max mem: 15.0 GB 
[06/20 01:06:17][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0299,	3.2438 s / batch. (data: 3.17e-04). ETA=18 days, 7:31:44, max mem: 15.0 GB 
[06/20 01:11:41][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0391,	3.3689 s / batch. (data: 6.31e-04). ETA=19 days, 0:23:27, max mem: 15.0 GB 
[06/20 01:17:07][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0407,	3.3115 s / batch. (data: 3.11e-04). ETA=18 days, 16:30:51, max mem: 15.0 GB 
[06/20 01:22:34][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0219,	3.2031 s / batch. (data: 3.94e-04). ETA=18 days, 1:44:49, max mem: 15.0 GB 
[06/20 01:28:02][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0336,	3.2017 s / batch. (data: 3.14e-04). ETA=18 days, 1:28:20, max mem: 15.0 GB 
[06/20 01:33:29][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0973,	2.9407 s / batch. (data: 3.43e-04). ETA=16 days, 14:02:37, max mem: 15.0 GB 
[06/20 01:38:54][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0959,	3.2344 s / batch. (data: 2.50e-04). ETA=18 days, 5:42:39, max mem: 15.0 GB 
[06/20 01:44:22][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0953,	3.4172 s / batch. (data: 3.42e-04). ETA=19 days, 6:21:28, max mem: 15.0 GB 
[06/20 01:49:49][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0352,	3.1692 s / batch. (data: 3.72e-04). ETA=17 days, 20:43:05, max mem: 15.0 GB 
[06/20 01:55:15][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0270,	3.3199 s / batch. (data: 2.71e-04). ETA=18 days, 17:00:34, max mem: 15.0 GB 
[06/20 02:00:40][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0200,	3.2130 s / batch. (data: 3.34e-04). ETA=18 days, 2:28:01, max mem: 15.0 GB 
[06/20 02:06:07][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0640,	3.3415 s / batch. (data: 3.19e-04). ETA=18 days, 19:45:04, max mem: 15.0 GB 
[06/20 02:11:33][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0492,	3.2717 s / batch. (data: 3.44e-04). ETA=18 days, 10:12:40, max mem: 15.0 GB 
[06/20 02:16:56][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0804,	3.2784 s / batch. (data: 3.11e-04). ETA=18 days, 11:02:01, max mem: 15.0 GB 
[06/20 02:22:22][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0262,	3.2524 s / batch. (data: 3.45e-04). ETA=18 days, 7:25:37, max mem: 15.0 GB 
[06/20 02:27:46][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0440,	3.1532 s / batch. (data: 3.38e-04). ETA=17 days, 17:55:58, max mem: 15.0 GB 
[06/20 02:33:16][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0478,	3.3364 s / batch. (data: 3.81e-04). ETA=18 days, 18:35:53, max mem: 15.0 GB 
[06/20 02:38:41][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0992,	3.2368 s / batch. (data: 3.00e-04). ETA=18 days, 5:03:24, max mem: 15.0 GB 
[06/20 02:44:08][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0779,	3.2017 s / batch. (data: 3.57e-04). ETA=18 days, 0:13:16, max mem: 15.0 GB 
[06/20 02:49:37][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0991,	3.3786 s / batch. (data: 3.43e-04). ETA=19 days, 0:00:14, max mem: 15.0 GB 
[06/20 02:55:03][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0465,	3.2745 s / batch. (data: 3.05e-04). ETA=18 days, 9:52:03, max mem: 15.0 GB 
[06/20 03:00:28][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0706,	2.8540 s / batch. (data: 3.55e-04). ETA=16 days, 1:02:31, max mem: 15.0 GB 
[06/20 03:05:54][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0336,	3.2858 s / batch. (data: 3.17e-04). ETA=18 days, 11:12:30, max mem: 15.0 GB 
[06/20 03:11:22][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0595,	3.3167 s / batch. (data: 2.93e-04). ETA=18 days, 15:17:23, max mem: 15.0 GB 
[06/20 03:16:49][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0933,	3.3038 s / batch. (data: 1.45e-04). ETA=18 days, 13:27:34, max mem: 15.0 GB 
[06/20 03:17:03][INFO] visual_prompt:  319: Epoch 3 / 100: avg data time: 4.41e-03, avg batch time: 3.6528, average train loss: 0.0592
[06/20 03:25:49][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.056, 5.1036 s / batch. (data: 1.44e-04)max mem: 14.95420 GB 
[06/20 03:34:00][INFO] visual_prompt:  476: Inference (val):avg data time: 1.52e-04, avg batch time: 5.0891, average loss: 0.0582
[06/20 03:34:00][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_shallow_cls_reinit_10/val_imagenet_invariances.json
[06/20 03:34:00][INFO] visual_prompt:  257: Training 4 / 100 epoch, with learning rate 0.3
[06/20 04:13:08][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0549,	3.2007 s / batch. (data: 2.77e-04). ETA=17 days, 23:27:27, max mem: 15.0 GB 
[06/20 04:18:28][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0326,	3.3730 s / batch. (data: 2.95e-04). ETA=18 days, 22:35:15, max mem: 15.0 GB 
[06/20 04:23:51][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0481,	3.3802 s / batch. (data: 3.02e-04). ETA=18 days, 23:27:56, max mem: 15.0 GB 
[06/20 04:29:22][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.1017,	3.2763 s / batch. (data: 2.70e-04). ETA=18 days, 9:22:59, max mem: 15.0 GB 
[06/20 04:34:53][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0345,	3.4114 s / batch. (data: 5.53e-04). ETA=19 days, 3:29:02, max mem: 15.0 GB 
[06/20 04:40:25][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0502,	3.3323 s / batch. (data: 3.11e-04). ETA=18 days, 16:44:13, max mem: 15.0 GB 
[06/20 04:45:58][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0391,	3.3831 s / batch. (data: 4.98e-04). ETA=18 days, 23:28:53, max mem: 15.0 GB 
[06/20 04:51:29][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0213,	3.3433 s / batch. (data: 2.76e-04). ETA=18 days, 18:02:06, max mem: 15.0 GB 
[06/20 04:56:58][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0825,	3.3229 s / batch. (data: 3.85e-04). ETA=18 days, 15:11:52, max mem: 15.0 GB 
[06/20 05:02:29][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0701,	3.3621 s / batch. (data: 4.26e-04). ETA=18 days, 20:22:53, max mem: 15.0 GB 
[06/20 05:08:01][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0207,	3.3465 s / batch. (data: 4.28e-04). ETA=18 days, 18:11:32, max mem: 15.0 GB 
[06/20 05:13:32][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0523,	3.0257 s / batch. (data: 3.28e-04). ETA=16 days, 22:56:41, max mem: 15.0 GB 
[06/20 05:19:04][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0954,	3.3053 s / batch. (data: 4.81e-04). ETA=18 days, 12:27:47, max mem: 15.0 GB 
[06/20 05:24:36][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0265,	3.2909 s / batch. (data: 3.24e-04). ETA=18 days, 10:25:55, max mem: 15.0 GB 
[06/20 05:30:08][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.1023,	3.3647 s / batch. (data: 2.93e-04). ETA=18 days, 20:15:23, max mem: 15.0 GB 
[06/20 05:35:40][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0526,	3.3285 s / batch. (data: 3.24e-04). ETA=18 days, 15:17:50, max mem: 15.0 GB 
[06/20 05:41:12][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0974,	3.3394 s / batch. (data: 3.45e-04). ETA=18 days, 16:40:20, max mem: 15.0 GB 
[06/20 05:46:43][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0929,	2.9893 s / batch. (data: 5.16e-04). ETA=16 days, 17:33:19, max mem: 15.0 GB 
[06/20 05:52:16][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0248,	3.4520 s / batch. (data: 2.89e-04). ETA=19 days, 7:36:46, max mem: 15.0 GB 
[06/20 05:57:48][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0444,	3.3428 s / batch. (data: 3.39e-04). ETA=18 days, 16:51:16, max mem: 15.0 GB 
[06/20 06:03:21][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0609,	3.3315 s / batch. (data: 3.29e-04). ETA=18 days, 15:14:32, max mem: 15.0 GB 
[06/20 06:08:54][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0967,	3.0072 s / batch. (data: 3.33e-04). ETA=16 days, 19:37:40, max mem: 15.0 GB 
[06/20 06:14:26][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0349,	3.3577 s / batch. (data: 3.40e-04). ETA=18 days, 18:34:46, max mem: 15.0 GB 
[06/20 06:19:57][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0621,	3.2412 s / batch. (data: 5.21e-04). ETA=18 days, 2:50:40, max mem: 15.0 GB 
[06/20 06:25:28][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0912,	3.3463 s / batch. (data: 4.11e-04). ETA=18 days, 16:51:22, max mem: 15.0 GB 
[06/20 06:30:59][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0617,	3.4308 s / batch. (data: 4.74e-04). ETA=19 days, 4:05:30, max mem: 15.0 GB 
[06/20 06:36:32][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0487,	3.3615 s / batch. (data: 3.38e-04). ETA=18 days, 18:42:17, max mem: 15.0 GB 
[06/20 06:42:03][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0617,	3.3420 s / batch. (data: 5.42e-04). ETA=18 days, 15:59:56, max mem: 15.0 GB 
[06/20 06:47:32][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0648,	2.9916 s / batch. (data: 3.03e-04). ETA=16 days, 16:56:29, max mem: 15.0 GB 
[06/20 06:53:04][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0973,	3.3525 s / batch. (data: 4.54e-04). ETA=18 days, 17:13:41, max mem: 15.0 GB 
[06/20 06:55:36][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0525,	1.1254 s / batch. (data: 3.01e-04). ETA=6 days, 6:45:50, max mem: 15.0 GB 
[06/20 06:57:29][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0983,	1.1308 s / batch. (data: 3.09e-04). ETA=6 days, 7:27:29, max mem: 15.0 GB 
[06/20 06:59:21][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0920,	1.1294 s / batch. (data: 3.33e-04). ETA=6 days, 7:14:12, max mem: 15.0 GB 
[06/20 07:01:14][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0938,	1.1299 s / batch. (data: 3.23e-04). ETA=6 days, 7:16:59, max mem: 15.0 GB 
[06/20 07:03:07][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0923,	1.1255 s / batch. (data: 3.15e-04). ETA=6 days, 6:39:27, max mem: 15.0 GB 
[06/20 07:05:00][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0818,	1.1264 s / batch. (data: 3.23e-04). ETA=6 days, 6:44:51, max mem: 15.0 GB 
[06/20 07:06:53][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0223,	1.1369 s / batch. (data: 3.14e-04). ETA=6 days, 8:07:02, max mem: 15.0 GB 
[06/20 07:08:46][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0576,	1.1281 s / batch. (data: 3.26e-04). ETA=6 days, 6:54:35, max mem: 15.0 GB 
[06/20 07:10:39][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0627,	1.1310 s / batch. (data: 3.37e-04). ETA=6 days, 7:16:00, max mem: 15.0 GB 
[06/20 07:12:32][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0491,	1.1301 s / batch. (data: 3.57e-04). ETA=6 days, 7:06:51, max mem: 15.0 GB 
[06/20 07:14:25][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0523,	1.1250 s / batch. (data: 3.28e-04). ETA=6 days, 6:24:26, max mem: 15.0 GB 
[06/20 07:16:17][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.1008,	1.1294 s / batch. (data: 3.43e-04). ETA=6 days, 6:57:23, max mem: 15.0 GB 
[06/20 07:18:10][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0538,	1.1300 s / batch. (data: 3.19e-04). ETA=6 days, 7:00:43, max mem: 15.0 GB 
[06/20 07:20:03][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.1028,	1.1300 s / batch. (data: 3.15e-04). ETA=6 days, 6:58:59, max mem: 15.0 GB 
[06/20 07:21:55][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0320,	1.1289 s / batch. (data: 3.12e-04). ETA=6 days, 6:48:13, max mem: 15.0 GB 
[06/20 07:23:48][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0675,	1.1269 s / batch. (data: 3.20e-04). ETA=6 days, 6:29:47, max mem: 15.0 GB 
[06/20 07:25:41][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.1039,	1.1252 s / batch. (data: 3.53e-04). ETA=6 days, 6:14:18, max mem: 15.0 GB 
[06/20 07:27:34][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0398,	1.1257 s / batch. (data: 4.29e-04). ETA=6 days, 6:16:37, max mem: 15.0 GB 
[06/20 07:29:27][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0460,	1.1271 s / batch. (data: 3.41e-04). ETA=6 days, 6:25:53, max mem: 15.0 GB 
[06/20 07:31:20][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0678,	1.1264 s / batch. (data: 1.19e-04). ETA=6 days, 6:18:06, max mem: 15.0 GB 
[06/20 07:31:25][INFO] visual_prompt:  319: Epoch 4 / 100: avg data time: 4.32e-03, avg batch time: 2.8465, average train loss: 0.0585
[06/20 07:40:10][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.056, 5.0801 s / batch. (data: 1.47e-04)max mem: 14.95420 GB 
[06/20 07:48:16][INFO] visual_prompt:  476: Inference (val):avg data time: 1.58e-04, avg batch time: 5.0633, average loss: 0.0578
[06/20 07:48:16][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_shallow_cls_reinit_10/val_imagenet_invariances.json
[06/20 07:48:16][INFO] visual_prompt:  257: Training 5 / 100 epoch, with learning rate 0.4
[06/20 07:50:53][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0952,	1.1261 s / batch. (data: 3.68e-04). ETA=6 days, 6:14:07, max mem: 15.0 GB 
[06/20 07:52:46][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0687,	1.1264 s / batch. (data: 3.49e-04). ETA=6 days, 6:14:57, max mem: 15.0 GB 
[06/20 07:54:38][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0321,	1.1255 s / batch. (data: 3.35e-04). ETA=6 days, 6:05:57, max mem: 15.0 GB 
[06/20 07:56:31][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0686,	1.1300 s / batch. (data: 3.48e-04). ETA=6 days, 6:39:18, max mem: 15.0 GB 
[06/20 07:58:24][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0722,	1.1301 s / batch. (data: 3.40e-04). ETA=6 days, 6:38:59, max mem: 15.0 GB 
[06/20 08:00:17][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0625,	1.1365 s / batch. (data: 3.57e-04). ETA=6 days, 7:28:13, max mem: 15.0 GB 
[06/20 08:02:10][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0539,	1.1325 s / batch. (data: 3.76e-04). ETA=6 days, 6:54:01, max mem: 15.0 GB 
[06/20 08:04:03][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0997,	1.1336 s / batch. (data: 3.02e-04). ETA=6 days, 7:00:48, max mem: 15.0 GB 
[06/20 08:05:57][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0713,	1.1314 s / batch. (data: 3.47e-04). ETA=6 days, 6:41:38, max mem: 15.0 GB 
[06/20 08:07:50][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0977,	1.1332 s / batch. (data: 3.75e-04). ETA=6 days, 6:53:52, max mem: 15.0 GB 
[06/20 08:09:43][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0725,	1.1230 s / batch. (data: 3.66e-04). ETA=6 days, 5:30:38, max mem: 15.0 GB 
[06/20 08:11:36][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0532,	1.1302 s / batch. (data: 3.36e-04). ETA=6 days, 6:26:19, max mem: 15.0 GB 
[06/20 08:13:29][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0819,	1.1276 s / batch. (data: 4.26e-04). ETA=6 days, 6:03:48, max mem: 15.0 GB 
[06/20 08:15:21][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0953,	1.1322 s / batch. (data: 4.17e-04). ETA=6 days, 6:38:26, max mem: 15.0 GB 
[06/20 08:17:14][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0493,	1.1252 s / batch. (data: 4.08e-04). ETA=6 days, 5:40:18, max mem: 15.0 GB 
[06/20 08:19:07][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0557,	1.1264 s / batch. (data: 3.16e-04). ETA=6 days, 5:48:04, max mem: 15.0 GB 
[06/20 08:21:00][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0270,	1.1304 s / batch. (data: 2.81e-04). ETA=6 days, 6:18:33, max mem: 15.0 GB 
[06/20 08:22:53][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0349,	1.1267 s / batch. (data: 4.21e-04). ETA=6 days, 5:47:24, max mem: 15.0 GB 
[06/20 08:24:45][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.1038,	1.1292 s / batch. (data: 3.52e-04). ETA=6 days, 6:05:11, max mem: 15.0 GB 
[06/20 08:26:38][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0569,	1.1243 s / batch. (data: 5.23e-04). ETA=6 days, 5:24:30, max mem: 15.0 GB 
[06/20 08:28:31][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0584,	1.1280 s / batch. (data: 3.51e-04). ETA=6 days, 5:51:35, max mem: 15.0 GB 
[06/20 08:30:24][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0513,	1.1208 s / batch. (data: 3.78e-04). ETA=6 days, 4:52:20, max mem: 15.0 GB 
[06/20 08:32:16][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0528,	1.1244 s / batch. (data: 4.09e-04). ETA=6 days, 5:19:16, max mem: 15.0 GB 
[06/20 08:34:09][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0750,	1.1246 s / batch. (data: 3.19e-04). ETA=6 days, 5:18:57, max mem: 15.0 GB 
[06/20 08:36:02][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0671,	1.1288 s / batch. (data: 2.95e-04). ETA=6 days, 5:50:13, max mem: 15.0 GB 
[06/20 08:37:55][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0438,	1.1275 s / batch. (data: 3.31e-04). ETA=6 days, 5:38:14, max mem: 15.0 GB 
[06/20 08:39:47][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0922,	1.1316 s / batch. (data: 3.22e-04). ETA=6 days, 6:09:01, max mem: 15.0 GB 
[06/20 08:41:40][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0556,	1.1312 s / batch. (data: 2.87e-04). ETA=6 days, 6:04:01, max mem: 15.0 GB 
[06/20 08:43:33][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0552,	1.1259 s / batch. (data: 3.44e-04). ETA=6 days, 5:20:21, max mem: 15.0 GB 
[06/20 08:45:26][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0322,	1.1300 s / batch. (data: 3.31e-04). ETA=6 days, 5:50:32, max mem: 15.0 GB 
[06/20 08:47:18][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0820,	1.1270 s / batch. (data: 3.21e-04). ETA=6 days, 5:24:42, max mem: 15.0 GB 
[06/20 08:49:11][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0621,	1.1228 s / batch. (data: 3.35e-04). ETA=6 days, 4:49:51, max mem: 15.0 GB 
[06/20 08:51:04][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0823,	1.1331 s / batch. (data: 3.73e-04). ETA=6 days, 6:09:48, max mem: 15.0 GB 
[06/20 08:52:57][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0933,	1.1311 s / batch. (data: 3.74e-04). ETA=6 days, 5:52:09, max mem: 15.0 GB 
[06/20 08:54:50][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0509,	1.1239 s / batch. (data: 4.03e-04). ETA=6 days, 4:52:57, max mem: 15.0 GB 
[06/20 08:56:42][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0465,	1.1234 s / batch. (data: 5.17e-04). ETA=6 days, 4:46:36, max mem: 15.0 GB 
[06/20 08:58:35][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0546,	1.1217 s / batch. (data: 3.96e-04). ETA=6 days, 4:31:16, max mem: 15.0 GB 
[06/20 09:00:27][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0750,	1.1227 s / batch. (data: 3.34e-04). ETA=6 days, 4:37:55, max mem: 15.0 GB 
[06/20 09:02:20][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0367,	1.1237 s / batch. (data: 2.83e-04). ETA=6 days, 4:43:22, max mem: 15.0 GB 
[06/20 09:04:12][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0997,	1.1219 s / batch. (data: 3.30e-04). ETA=6 days, 4:27:17, max mem: 15.0 GB 
[06/20 09:06:05][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0214,	1.1237 s / batch. (data: 2.99e-04). ETA=6 days, 4:39:42, max mem: 15.0 GB 
[06/20 09:07:57][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0255,	1.1246 s / batch. (data: 3.41e-04). ETA=6 days, 4:45:29, max mem: 15.0 GB 
[06/20 09:09:49][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0367,	1.1283 s / batch. (data: 2.89e-04). ETA=6 days, 5:13:08, max mem: 15.0 GB 
[06/20 09:11:42][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.1034,	1.1240 s / batch. (data: 3.01e-04). ETA=6 days, 4:36:58, max mem: 15.0 GB 
[06/20 09:13:34][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0349,	1.1205 s / batch. (data: 2.93e-04). ETA=6 days, 4:06:59, max mem: 15.0 GB 
[06/20 09:15:27][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0470,	1.1214 s / batch. (data: 2.93e-04). ETA=6 days, 4:12:40, max mem: 15.0 GB 
[06/20 09:17:19][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0433,	1.1233 s / batch. (data: 2.35e-04). ETA=6 days, 4:25:34, max mem: 15.0 GB 
[06/20 09:19:11][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0220,	1.1217 s / batch. (data: 2.88e-04). ETA=6 days, 4:10:43, max mem: 15.0 GB 
[06/20 09:21:04][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0514,	1.1231 s / batch. (data: 2.91e-04). ETA=6 days, 4:20:39, max mem: 15.0 GB 
[06/20 09:22:56][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0988,	1.1182 s / batch. (data: 7.49e-05). ETA=6 days, 3:39:18, max mem: 15.0 GB 
[06/20 09:23:02][INFO] visual_prompt:  319: Epoch 5 / 100: avg data time: 4.10e-03, avg batch time: 1.1361, average train loss: 0.0589
[06/20 09:31:42][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.058, 5.0625 s / batch. (data: 1.28e-04)max mem: 14.95420 GB 
[06/20 09:39:46][INFO] visual_prompt:  476: Inference (val):avg data time: 1.43e-04, avg batch time: 5.0400, average loss: 0.0589
[06/20 09:39:46][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_shallow_cls_reinit_10/val_imagenet_invariances.json
[06/20 09:39:46][INFO] visual_prompt:  257: Training 6 / 100 epoch, with learning rate 0.5
[06/20 09:42:24][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0716,	1.1217 s / batch. (data: 3.18e-04). ETA=6 days, 4:05:04, max mem: 15.0 GB 
[06/20 09:44:16][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0414,	1.1284 s / batch. (data: 3.02e-04). ETA=6 days, 4:56:42, max mem: 15.0 GB 
[06/20 09:46:09][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0699,	1.1230 s / batch. (data: 2.40e-04). ETA=6 days, 4:11:48, max mem: 15.0 GB 
[06/20 09:48:01][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0289,	1.1208 s / batch. (data: 2.91e-04). ETA=6 days, 3:52:26, max mem: 15.0 GB 
[06/20 09:49:53][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0337,	1.1215 s / batch. (data: 2.88e-04). ETA=6 days, 3:56:20, max mem: 15.0 GB 
[06/20 09:51:46][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0291,	1.1219 s / batch. (data: 3.13e-04). ETA=6 days, 3:57:41, max mem: 15.0 GB 
[06/20 09:53:38][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0550,	1.1233 s / batch. (data: 3.24e-04). ETA=6 days, 4:06:36, max mem: 15.0 GB 
[06/20 09:55:30][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0450,	1.1231 s / batch. (data: 3.08e-04). ETA=6 days, 4:03:11, max mem: 15.0 GB 
[06/20 09:57:23][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0742,	1.1202 s / batch. (data: 2.68e-04). ETA=6 days, 3:38:47, max mem: 15.0 GB 
[06/20 09:59:15][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0380,	1.1224 s / batch. (data: 3.27e-04). ETA=6 days, 3:54:17, max mem: 15.0 GB 
[06/20 10:01:07][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0485,	1.1232 s / batch. (data: 3.39e-04). ETA=6 days, 3:58:35, max mem: 15.0 GB 
[06/20 10:02:59][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0778,	1.1211 s / batch. (data: 3.10e-04). ETA=6 days, 3:39:57, max mem: 15.0 GB 
[06/20 10:04:52][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0724,	1.1224 s / batch. (data: 3.02e-04). ETA=6 days, 3:48:19, max mem: 15.0 GB 
[06/20 10:06:44][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0762,	1.1245 s / batch. (data: 3.18e-04). ETA=6 days, 4:03:00, max mem: 15.0 GB 
[06/20 10:08:36][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0463,	1.1254 s / batch. (data: 3.07e-04). ETA=6 days, 4:08:26, max mem: 15.0 GB 
[06/20 10:10:29][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0881,	1.1246 s / batch. (data: 3.70e-04). ETA=6 days, 4:00:08, max mem: 15.0 GB 
[06/20 10:12:21][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0592,	1.1218 s / batch. (data: 3.09e-04). ETA=6 days, 3:36:32, max mem: 15.0 GB 
[06/20 10:14:14][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0732,	1.1190 s / batch. (data: 3.19e-04). ETA=6 days, 3:12:31, max mem: 15.0 GB 
[06/20 10:16:06][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0853,	1.1226 s / batch. (data: 3.09e-04). ETA=6 days, 3:38:54, max mem: 15.0 GB 
[06/20 10:17:58][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.1047,	1.1224 s / batch. (data: 3.14e-04). ETA=6 days, 3:35:01, max mem: 15.0 GB 
[06/20 10:19:51][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.1014,	1.1243 s / batch. (data: 3.14e-04). ETA=6 days, 3:48:29, max mem: 15.0 GB 
[06/20 10:21:44][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0805,	1.1277 s / batch. (data: 3.28e-04). ETA=6 days, 4:13:37, max mem: 15.0 GB 
[06/20 10:23:36][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0834,	1.1291 s / batch. (data: 2.63e-04). ETA=6 days, 4:22:22, max mem: 15.0 GB 
[06/20 10:25:29][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0448,	1.1240 s / batch. (data: 2.63e-04). ETA=6 days, 3:40:08, max mem: 15.0 GB 
[06/20 10:27:22][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0202,	1.1280 s / batch. (data: 3.27e-04). ETA=6 days, 4:10:05, max mem: 15.0 GB 
[06/20 10:29:15][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0953,	1.1276 s / batch. (data: 2.58e-04). ETA=6 days, 4:05:02, max mem: 15.0 GB 
[06/20 10:31:08][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0481,	1.1258 s / batch. (data: 3.05e-04). ETA=6 days, 3:49:24, max mem: 15.0 GB 
[06/20 10:33:00][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0761,	1.1271 s / batch. (data: 3.98e-04). ETA=6 days, 3:57:32, max mem: 15.0 GB 
[06/20 10:34:53][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0241,	1.1265 s / batch. (data: 3.25e-04). ETA=6 days, 3:51:04, max mem: 15.0 GB 
[06/20 10:36:46][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0231,	1.1253 s / batch. (data: 3.33e-04). ETA=6 days, 3:39:31, max mem: 15.0 GB 
[06/20 10:38:39][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0306,	1.1302 s / batch. (data: 3.34e-04). ETA=6 days, 4:15:59, max mem: 15.0 GB 
[06/20 10:40:31][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0972,	1.1286 s / batch. (data: 3.70e-04). ETA=6 days, 4:01:23, max mem: 15.0 GB 
[06/20 10:42:24][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0827,	1.1313 s / batch. (data: 3.41e-04). ETA=6 days, 4:20:55, max mem: 15.0 GB 
[06/20 10:44:17][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0467,	1.1348 s / batch. (data: 3.96e-04). ETA=6 days, 4:46:27, max mem: 15.0 GB 
[06/20 10:46:10][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0582,	1.1293 s / batch. (data: 2.95e-04). ETA=6 days, 4:01:12, max mem: 15.0 GB 
[06/20 10:48:03][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0271,	1.1226 s / batch. (data: 3.64e-04). ETA=6 days, 3:07:03, max mem: 15.0 GB 
[06/20 10:49:55][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0979,	1.1294 s / batch. (data: 2.98e-04). ETA=6 days, 3:58:39, max mem: 15.0 GB 
[06/20 10:51:48][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0689,	1.1261 s / batch. (data: 4.10e-04). ETA=6 days, 3:30:26, max mem: 15.0 GB 
[06/20 10:53:41][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0807,	1.1303 s / batch. (data: 3.16e-04). ETA=6 days, 4:02:12, max mem: 15.0 GB 
[06/20 10:55:34][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0578,	1.1281 s / batch. (data: 3.25e-04). ETA=6 days, 3:42:31, max mem: 15.0 GB 
[06/20 10:57:26][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0955,	1.1252 s / batch. (data: 3.33e-04). ETA=6 days, 3:17:56, max mem: 15.0 GB 
[06/20 10:59:19][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0366,	1.1263 s / batch. (data: 3.81e-04). ETA=6 days, 3:24:31, max mem: 15.0 GB 
[06/20 11:01:12][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.1027,	1.1224 s / batch. (data: 3.51e-04). ETA=6 days, 2:52:30, max mem: 15.0 GB 
[06/20 11:03:05][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0224,	1.1323 s / batch. (data: 4.61e-04). ETA=6 days, 4:08:17, max mem: 15.0 GB 
[06/20 11:04:58][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0690,	1.1242 s / batch. (data: 3.68e-04). ETA=6 days, 3:02:40, max mem: 15.0 GB 
[06/20 11:06:50][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0457,	1.1284 s / batch. (data: 4.93e-04). ETA=6 days, 3:33:28, max mem: 15.0 GB 
[06/20 11:08:43][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0441,	1.1235 s / batch. (data: 4.50e-04). ETA=6 days, 2:53:30, max mem: 15.0 GB 
[06/20 11:10:36][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0349,	1.1286 s / batch. (data: 3.14e-04). ETA=6 days, 3:31:44, max mem: 15.0 GB 
[06/20 11:12:29][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.1011,	1.1291 s / batch. (data: 3.79e-04). ETA=6 days, 3:33:22, max mem: 15.0 GB 
[06/20 11:14:21][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0534,	1.1289 s / batch. (data: 1.69e-04). ETA=6 days, 3:30:01, max mem: 15.0 GB 
[06/20 11:14:27][INFO] visual_prompt:  319: Epoch 6 / 100: avg data time: 3.92e-03, avg batch time: 1.1350, average train loss: 0.0588
[06/20 11:21:54][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/20 11:21:54][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,4,5,6
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/20 11:21:54][INFO] visual_prompt:   99: Command line arguments: None
[06/20 11:21:54][INFO] visual_prompt:  108: Training with config:
[06/20 11:21:54][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': False,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': 512,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls-reinit+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_shallow_cls_reinit_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 1.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'mse',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'USE_CLS_TOKEN': True,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/20 11:21:58][INFO] visual_prompt:   56: Total Parameters: 96961552	 Gradient Parameters: 11163664
[06/20 11:21:58][INFO] visual_prompt:   58: tuned percent:11.513
[06/20 11:21:59][INFO] visual_prompt:   44: Device used for model: 0
[06/20 11:21:59][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/20 11:21:59][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/20 11:22:02][INFO] visual_prompt:  158: Number of images: 1281167
[06/20 11:22:02][INFO] visual_prompt:  159: Number of classes: 1000
[06/20 11:22:02][INFO] visual_prompt:   78: Loading validation data...
[06/20 11:22:02][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/20 11:22:02][INFO] visual_prompt:  158: Number of images: 50000
[06/20 11:22:02][INFO] visual_prompt:  159: Number of classes: 1000
[06/20 11:22:02][INFO] visual_prompt:   81: Loading test data...
[06/20 11:22:02][INFO] visual_prompt:   83: ...no test data is constructed
[06/20 11:22:02][INFO] visual_prompt:  111: Constructing models...
[06/20 11:22:02][INFO] visual_prompt:  114: Setting up Evalutator...
[06/20 11:22:02][INFO] visual_prompt:  116: Setting up Trainer...
[06/20 11:22:02][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.deep_prompt_embeddings: True
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.prompt_proj.weight: True
[06/20 11:22:02][INFO] visual_prompt:   59: module.enc.transformer.prompt_proj.bias: True
[06/20 11:22:02][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/20 11:22:02][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/20 11:22:02][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/20 11:22:02][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/20 11:22:02][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/20 11:22:02][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/20 11:22:02][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/20 11:22:02][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/20 11:22:02][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/20 11:22:02][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/20 11:22:02][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/20 11:22:02][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/20 11:22:02][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/20 11:22:02][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/20 11:22:02][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/20 11:22:02][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/20 11:22:02][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/20 11:22:02][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/20 11:22:02][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/20 11:22:02][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/20 11:22:02][INFO] visual_prompt:  238: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/20 11:22:02][INFO] visual_prompt:  257: Training 1 / 100 epoch, with learning rate 0.0
[06/20 11:24:24][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0210,	1.1841 s / batch. (data: 3.52e-04). ETA=6 days, 20:33:04, max mem: 15.2 GB 
[06/20 11:26:23][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0242,	1.1884 s / batch. (data: 3.73e-04). ETA=6 days, 21:06:54, max mem: 15.2 GB 
[06/20 11:28:21][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0273,	1.1881 s / batch. (data: 2.88e-04). ETA=6 days, 21:02:45, max mem: 15.2 GB 
[06/20 11:30:20][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0222,	1.1880 s / batch. (data: 3.35e-04). ETA=6 days, 20:59:56, max mem: 15.2 GB 
[06/20 11:32:19][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0230,	1.1859 s / batch. (data: 3.63e-04). ETA=6 days, 20:40:55, max mem: 15.2 GB 
[06/20 11:34:17][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0246,	1.1805 s / batch. (data: 3.30e-04). ETA=6 days, 19:53:38, max mem: 15.2 GB 
[06/20 11:36:16][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0226,	1.1845 s / batch. (data: 3.56e-04). ETA=6 days, 20:24:47, max mem: 15.2 GB 
[06/20 11:38:14][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0166,	1.1815 s / batch. (data: 3.52e-04). ETA=6 days, 19:58:13, max mem: 15.2 GB 
[06/20 11:40:13][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0214,	1.1843 s / batch. (data: 3.18e-04). ETA=6 days, 20:18:53, max mem: 15.2 GB 
[06/20 11:42:11][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0243,	1.1852 s / batch. (data: 3.15e-04). ETA=6 days, 20:24:50, max mem: 15.2 GB 
[06/20 11:44:10][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0212,	1.1842 s / batch. (data: 3.63e-04). ETA=6 days, 20:14:24, max mem: 15.2 GB 
[06/20 11:46:08][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0275,	1.1820 s / batch. (data: 3.02e-04). ETA=6 days, 19:53:52, max mem: 15.2 GB 
[06/20 11:48:07][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0241,	1.1858 s / batch. (data: 3.35e-04). ETA=6 days, 20:23:38, max mem: 15.2 GB 
[06/20 11:50:05][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0167,	1.1924 s / batch. (data: 2.92e-04). ETA=6 days, 21:16:48, max mem: 15.2 GB 
[06/20 11:52:04][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0201,	1.1816 s / batch. (data: 4.22e-04). ETA=6 days, 19:45:04, max mem: 15.2 GB 
[06/20 11:54:02][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0215,	1.1877 s / batch. (data: 3.52e-04). ETA=6 days, 20:33:43, max mem: 15.2 GB 
[06/20 11:56:01][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0267,	1.1978 s / batch. (data: 3.06e-04). ETA=6 days, 21:55:51, max mem: 15.2 GB 
[06/20 11:57:59][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0213,	1.1825 s / batch. (data: 3.43e-04). ETA=6 days, 19:46:21, max mem: 15.2 GB 
[06/20 11:59:58][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0211,	1.1819 s / batch. (data: 2.92e-04). ETA=6 days, 19:39:31, max mem: 15.2 GB 
[06/20 12:01:56][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0235,	1.1824 s / batch. (data: 2.93e-04). ETA=6 days, 19:42:05, max mem: 15.2 GB 
[06/20 12:03:55][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0229,	1.1805 s / batch. (data: 3.06e-04). ETA=6 days, 19:24:24, max mem: 15.2 GB 
[06/20 12:05:53][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0213,	1.1875 s / batch. (data: 2.91e-04). ETA=6 days, 20:20:36, max mem: 15.2 GB 
[06/20 12:07:52][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0266,	1.1878 s / batch. (data: 2.98e-04). ETA=6 days, 20:20:51, max mem: 15.2 GB 
[06/20 12:09:50][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0211,	1.1815 s / batch. (data: 3.40e-04). ETA=6 days, 19:26:47, max mem: 15.2 GB 
[06/20 12:11:49][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0232,	1.1880 s / batch. (data: 3.14e-04). ETA=6 days, 20:18:19, max mem: 15.2 GB 
[06/20 12:13:47][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0230,	1.1858 s / batch. (data: 2.84e-04). ETA=6 days, 19:58:29, max mem: 15.2 GB 
[06/20 12:15:46][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0260,	1.1911 s / batch. (data: 3.02e-04). ETA=6 days, 20:39:56, max mem: 15.2 GB 
[06/20 12:17:44][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0240,	1.1905 s / batch. (data: 2.69e-04). ETA=6 days, 20:32:55, max mem: 15.2 GB 
[06/20 12:19:43][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0165,	1.1902 s / batch. (data: 3.01e-04). ETA=6 days, 20:28:33, max mem: 15.2 GB 
[06/20 12:21:41][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0222,	1.1838 s / batch. (data: 3.71e-04). ETA=6 days, 19:34:03, max mem: 15.2 GB 
[06/20 12:23:40][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0211,	1.1830 s / batch. (data: 3.20e-04). ETA=6 days, 19:25:04, max mem: 15.2 GB 
[06/20 12:25:38][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0236,	1.1821 s / batch. (data: 3.04e-04). ETA=6 days, 19:15:26, max mem: 15.2 GB 
[06/20 12:27:37][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0271,	1.1866 s / batch. (data: 3.44e-04). ETA=6 days, 19:51:05, max mem: 15.2 GB 
[06/20 12:29:35][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0170,	1.1807 s / batch. (data: 2.95e-04). ETA=6 days, 18:59:56, max mem: 15.2 GB 
[06/20 12:31:34][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0211,	1.1837 s / batch. (data: 2.90e-04). ETA=6 days, 19:23:07, max mem: 15.2 GB 
[06/20 12:33:32][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0257,	1.1863 s / batch. (data: 3.49e-04). ETA=6 days, 19:42:36, max mem: 15.2 GB 
[06/20 12:35:31][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0264,	1.1797 s / batch. (data: 3.10e-04). ETA=6 days, 18:45:59, max mem: 15.2 GB 
[06/20 12:37:29][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0172,	1.1802 s / batch. (data: 3.17e-04). ETA=6 days, 18:48:27, max mem: 15.2 GB 
[06/20 12:39:28][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0201,	1.1900 s / batch. (data: 2.87e-04). ETA=6 days, 20:07:18, max mem: 15.2 GB 
[06/20 12:41:26][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0262,	1.1799 s / batch. (data: 3.58e-04). ETA=6 days, 18:41:51, max mem: 15.2 GB 
[06/20 12:43:25][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0212,	1.1812 s / batch. (data: 4.00e-04). ETA=6 days, 18:50:11, max mem: 15.2 GB 
[06/20 12:45:23][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0169,	1.1866 s / batch. (data: 3.01e-04). ETA=6 days, 19:33:28, max mem: 15.2 GB 
[06/20 12:47:22][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0243,	1.1876 s / batch. (data: 2.92e-04). ETA=6 days, 19:39:51, max mem: 15.2 GB 
[06/20 12:49:20][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0229,	1.1825 s / batch. (data: 3.20e-04). ETA=6 days, 18:55:02, max mem: 15.2 GB 
[06/20 12:51:19][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0206,	1.1852 s / batch. (data: 2.99e-04). ETA=6 days, 19:15:20, max mem: 15.2 GB 
[06/20 12:53:17][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0234,	1.1824 s / batch. (data: 3.21e-04). ETA=6 days, 18:50:37, max mem: 15.2 GB 
[06/20 12:55:16][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0231,	1.1839 s / batch. (data: 2.61e-04). ETA=6 days, 19:01:02, max mem: 15.2 GB 
[06/20 12:57:14][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0226,	1.1825 s / batch. (data: 3.23e-04). ETA=6 days, 18:47:22, max mem: 15.2 GB 
[06/20 12:59:12][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0235,	1.1832 s / batch. (data: 3.09e-04). ETA=6 days, 18:51:35, max mem: 15.2 GB 
[06/20 13:01:11][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0232,	1.1803 s / batch. (data: 1.37e-04). ETA=6 days, 18:25:01, max mem: 15.2 GB 
[06/20 13:01:16][INFO] visual_prompt:  319: Epoch 1 / 100: avg data time: 4.60e-03, avg batch time: 1.1898, average train loss: 0.0224
[06/20 13:10:20][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.012, 5.2390 s / batch. (data: 2.85e-04)max mem: 15.18835 GB 
[06/20 13:18:41][INFO] visual_prompt:  476: Inference (val):avg data time: 1.58e-04, avg batch time: 5.2309, average loss: 0.0120
[06/20 13:18:42][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_shallow_cls_reinit_10/val_imagenet_invariances.json
[06/20 13:18:42][INFO] visual_prompt:  257: Training 2 / 100 epoch, with learning rate 0.1
[06/20 13:21:27][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0045,	1.1814 s / batch. (data: 3.51e-04). ETA=6 days, 18:32:13, max mem: 15.2 GB 
[06/20 13:23:26][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0029,	1.1805 s / batch. (data: 3.76e-04). ETA=6 days, 18:22:45, max mem: 15.2 GB 
[06/20 13:25:24][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0034,	1.1826 s / batch. (data: 3.27e-04). ETA=6 days, 18:38:06, max mem: 15.2 GB 
[06/20 13:27:22][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0021,	1.1771 s / batch. (data: 3.81e-04). ETA=6 days, 17:50:39, max mem: 15.2 GB 
[06/20 13:29:21][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0018,	1.1844 s / batch. (data: 6.11e-04). ETA=6 days, 18:48:55, max mem: 15.2 GB 
[06/20 13:31:19][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0013,	1.1810 s / batch. (data: 2.74e-04). ETA=6 days, 18:19:16, max mem: 15.2 GB 
[06/20 13:33:17][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0013,	1.1818 s / batch. (data: 2.95e-04). ETA=6 days, 18:24:13, max mem: 15.2 GB 
[06/20 13:35:15][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0018,	1.1759 s / batch. (data: 3.09e-04). ETA=6 days, 17:33:05, max mem: 15.2 GB 
[06/20 13:37:14][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0018,	1.1780 s / batch. (data: 2.97e-04). ETA=6 days, 17:48:17, max mem: 15.2 GB 
[06/20 13:39:12][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0012,	1.1920 s / batch. (data: 4.08e-04). ETA=6 days, 19:42:13, max mem: 15.2 GB 
[06/20 13:41:10][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0018,	1.1814 s / batch. (data: 2.84e-04). ETA=6 days, 18:12:45, max mem: 15.2 GB 
[06/20 13:43:08][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0014,	1.1767 s / batch. (data: 2.87e-04). ETA=6 days, 17:31:42, max mem: 15.2 GB 
[06/20 13:45:06][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0016,	1.1785 s / batch. (data: 2.24e-04). ETA=6 days, 17:45:15, max mem: 15.2 GB 
[06/20 13:47:04][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0017,	1.1781 s / batch. (data: 3.10e-04). ETA=6 days, 17:39:38, max mem: 15.2 GB 
[06/20 13:49:02][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0014,	1.1774 s / batch. (data: 3.26e-04). ETA=6 days, 17:32:16, max mem: 15.2 GB 
[06/20 13:51:01][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0013,	1.1840 s / batch. (data: 2.93e-04). ETA=6 days, 18:23:55, max mem: 15.2 GB 
[06/20 13:52:59][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0012,	1.1792 s / batch. (data: 3.09e-04). ETA=6 days, 17:42:33, max mem: 15.2 GB 
[06/20 13:54:57][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0013,	1.1853 s / batch. (data: 3.16e-04). ETA=6 days, 18:30:44, max mem: 15.2 GB 
[06/20 13:56:55][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0015,	1.1801 s / batch. (data: 3.66e-04). ETA=6 days, 17:46:21, max mem: 15.2 GB 
[06/20 13:58:53][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0013,	1.1794 s / batch. (data: 2.88e-04). ETA=6 days, 17:38:37, max mem: 15.2 GB 
[06/20 14:00:51][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0013,	1.1798 s / batch. (data: 3.50e-04). ETA=6 days, 17:39:51, max mem: 15.2 GB 
[06/20 14:02:49][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0013,	1.1811 s / batch. (data: 2.59e-04). ETA=6 days, 17:48:42, max mem: 15.2 GB 
[06/20 14:04:47][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0010,	1.1770 s / batch. (data: 2.18e-04). ETA=6 days, 17:12:46, max mem: 15.2 GB 
[06/20 14:06:45][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0011,	1.1792 s / batch. (data: 3.02e-04). ETA=6 days, 17:29:13, max mem: 15.2 GB 
[06/20 14:08:43][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0014,	1.1820 s / batch. (data: 3.26e-04). ETA=6 days, 17:50:18, max mem: 15.2 GB 
[06/20 14:10:41][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0013,	1.1778 s / batch. (data: 2.29e-04). ETA=6 days, 17:13:48, max mem: 15.2 GB 
[06/20 14:12:39][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0016,	1.1865 s / batch. (data: 2.88e-04). ETA=6 days, 18:22:45, max mem: 15.2 GB 
[06/20 14:14:37][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0010,	1.1835 s / batch. (data: 2.90e-04). ETA=6 days, 17:56:11, max mem: 15.2 GB 
[06/20 14:16:35][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0013,	1.1771 s / batch. (data: 2.53e-04). ETA=6 days, 17:02:06, max mem: 15.2 GB 
[06/20 14:18:33][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0016,	1.1844 s / batch. (data: 2.42e-04). ETA=6 days, 18:00:14, max mem: 15.2 GB 
[06/20 14:20:31][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0012,	1.1837 s / batch. (data: 2.77e-04). ETA=6 days, 17:52:13, max mem: 15.2 GB 
[06/20 14:22:29][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0010,	1.1835 s / batch. (data: 2.88e-04). ETA=6 days, 17:48:41, max mem: 15.2 GB 
[06/20 14:24:27][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0012,	1.1763 s / batch. (data: 3.01e-04). ETA=6 days, 16:47:33, max mem: 15.2 GB 
[06/20 14:26:25][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0017,	1.1768 s / batch. (data: 3.06e-04). ETA=6 days, 16:49:51, max mem: 15.2 GB 
[06/20 14:28:23][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0010,	1.1845 s / batch. (data: 2.01e-04). ETA=6 days, 17:51:00, max mem: 15.2 GB 
[06/20 14:30:20][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0012,	1.1749 s / batch. (data: 2.93e-04). ETA=6 days, 16:30:04, max mem: 15.2 GB 
[06/20 14:32:18][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0012,	1.1796 s / batch. (data: 2.23e-04). ETA=6 days, 17:06:25, max mem: 15.2 GB 
[06/20 14:34:16][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0011,	1.1785 s / batch. (data: 2.69e-04). ETA=6 days, 16:55:43, max mem: 15.2 GB 
[06/20 14:36:14][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0012,	1.1768 s / batch. (data: 3.14e-04). ETA=6 days, 16:39:42, max mem: 15.2 GB 
[06/20 14:38:11][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0017,	1.1775 s / batch. (data: 3.11e-04). ETA=6 days, 16:43:55, max mem: 15.2 GB 
[06/20 14:40:09][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0012,	1.1752 s / batch. (data: 2.64e-04). ETA=6 days, 16:22:41, max mem: 15.2 GB 
[06/20 14:42:07][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0016,	1.1772 s / batch. (data: 2.67e-04). ETA=6 days, 16:36:53, max mem: 15.2 GB 
[06/20 14:44:05][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0014,	1.1826 s / batch. (data: 2.54e-04). ETA=6 days, 17:19:12, max mem: 15.2 GB 
[06/20 14:46:02][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0011,	1.1786 s / batch. (data: 2.79e-04). ETA=6 days, 16:44:31, max mem: 15.2 GB 
[06/20 14:48:00][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0011,	1.1765 s / batch. (data: 3.03e-04). ETA=6 days, 16:25:26, max mem: 15.2 GB 
[06/20 14:49:58][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0013,	1.1785 s / batch. (data: 2.85e-04). ETA=6 days, 16:39:56, max mem: 15.2 GB 
[06/20 14:51:55][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0015,	1.1806 s / batch. (data: 2.92e-04). ETA=6 days, 16:54:57, max mem: 15.2 GB 
[06/20 14:53:54][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0012,	1.1844 s / batch. (data: 3.12e-04). ETA=6 days, 17:24:20, max mem: 15.2 GB 
[06/20 14:55:52][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0016,	1.1861 s / batch. (data: 3.46e-04). ETA=6 days, 17:36:24, max mem: 15.2 GB 
[06/20 14:57:50][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0012,	1.1734 s / batch. (data: 1.04e-04). ETA=6 days, 15:50:25, max mem: 15.2 GB 
[06/20 14:57:56][INFO] visual_prompt:  319: Epoch 2 / 100: avg data time: 4.56e-03, avg batch time: 1.1897, average train loss: 0.0016
[06/20 15:06:51][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.001, 5.1952 s / batch. (data: 1.75e-04)max mem: 15.18835 GB 
[06/20 15:15:09][INFO] visual_prompt:  476: Inference (val):avg data time: 1.42e-04, avg batch time: 5.1802, average loss: 0.0008
[06/20 15:15:09][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_shallow_cls_reinit_10/val_imagenet_invariances.json
[06/20 15:15:09][INFO] visual_prompt:  257: Training 3 / 100 epoch, with learning rate 0.2
[06/20 15:17:51][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0019,	1.1797 s / batch. (data: 2.63e-04). ETA=6 days, 16:40:19, max mem: 15.2 GB 
[06/20 15:19:49][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0010,	1.1803 s / batch. (data: 3.28e-04). ETA=6 days, 16:42:36, max mem: 15.2 GB 
[06/20 15:21:48][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0012,	1.1786 s / batch. (data: 3.13e-04). ETA=6 days, 16:26:38, max mem: 15.2 GB 
[06/20 15:23:46][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0010,	1.1850 s / batch. (data: 3.07e-04). ETA=6 days, 17:17:16, max mem: 15.2 GB 
[06/20 15:25:44][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0011,	1.1781 s / batch. (data: 3.16e-04). ETA=6 days, 16:18:44, max mem: 15.2 GB 
[06/20 15:27:42][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0015,	1.1816 s / batch. (data: 3.20e-04). ETA=6 days, 16:45:22, max mem: 15.2 GB 
[06/20 15:29:40][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0012,	1.1913 s / batch. (data: 2.78e-04). ETA=6 days, 18:02:32, max mem: 15.2 GB 
[06/20 15:31:38][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0012,	1.1795 s / batch. (data: 2.70e-04). ETA=6 days, 16:24:56, max mem: 15.2 GB 
[06/20 15:33:36][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0015,	1.1802 s / batch. (data: 2.95e-04). ETA=6 days, 16:28:05, max mem: 15.2 GB 
[06/20 15:35:34][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0012,	1.1782 s / batch. (data: 3.88e-04). ETA=6 days, 16:10:11, max mem: 15.2 GB 
[06/20 15:37:32][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0014,	1.1824 s / batch. (data: 2.90e-04). ETA=6 days, 16:42:14, max mem: 15.2 GB 
[06/20 15:39:30][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0019,	1.1798 s / batch. (data: 2.72e-04). ETA=6 days, 16:19:08, max mem: 15.2 GB 
[06/20 15:41:28][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0013,	1.1805 s / batch. (data: 2.93e-04). ETA=6 days, 16:22:43, max mem: 15.2 GB 
[06/20 15:43:26][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0011,	1.1775 s / batch. (data: 2.94e-04). ETA=6 days, 15:56:35, max mem: 15.2 GB 
[06/20 15:45:24][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0012,	1.1781 s / batch. (data: 2.98e-04). ETA=6 days, 15:59:30, max mem: 15.2 GB 
[06/20 15:47:22][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0013,	1.1741 s / batch. (data: 2.93e-04). ETA=6 days, 15:25:14, max mem: 15.2 GB 
[06/20 15:49:19][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0012,	1.1755 s / batch. (data: 3.46e-04). ETA=6 days, 15:34:01, max mem: 15.2 GB 
[06/20 15:51:17][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0012,	1.1799 s / batch. (data: 3.18e-04). ETA=6 days, 16:08:04, max mem: 15.2 GB 
[06/20 15:53:15][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0013,	1.1884 s / batch. (data: 3.50e-04). ETA=6 days, 17:15:28, max mem: 15.2 GB 
[06/20 15:55:14][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0013,	1.1952 s / batch. (data: 3.83e-04). ETA=6 days, 18:08:45, max mem: 15.2 GB 
[06/20 15:57:13][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0011,	1.1847 s / batch. (data: 4.83e-04). ETA=6 days, 16:41:06, max mem: 15.2 GB 
[06/20 15:59:11][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0018,	1.1845 s / batch. (data: 3.65e-04). ETA=6 days, 16:37:58, max mem: 15.2 GB 
[06/20 16:01:10][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0017,	1.1907 s / batch. (data: 3.06e-04). ETA=6 days, 17:25:53, max mem: 15.2 GB 
[06/20 16:03:08][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0016,	1.1866 s / batch. (data: 3.45e-04). ETA=6 days, 16:51:03, max mem: 15.2 GB 
[06/20 16:05:06][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0011,	1.1793 s / batch. (data: 3.28e-04). ETA=6 days, 15:49:25, max mem: 15.2 GB 
[06/20 16:07:05][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0013,	1.1870 s / batch. (data: 4.58e-04). ETA=6 days, 16:49:56, max mem: 15.2 GB 
[06/20 16:09:03][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0012,	1.1817 s / batch. (data: 3.45e-04). ETA=6 days, 16:04:47, max mem: 15.2 GB 
[06/20 16:11:02][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0014,	1.1859 s / batch. (data: 3.36e-04). ETA=6 days, 16:37:37, max mem: 15.2 GB 
[06/20 16:13:00][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0013,	1.1844 s / batch. (data: 3.42e-04). ETA=6 days, 16:22:49, max mem: 15.2 GB 
[06/20 16:14:59][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0012,	1.1833 s / batch. (data: 3.55e-04). ETA=6 days, 16:11:48, max mem: 15.2 GB 
[06/20 16:16:57][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0013,	1.1822 s / batch. (data: 2.96e-04). ETA=6 days, 16:01:23, max mem: 15.2 GB 
[06/20 16:18:56][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0018,	1.1854 s / batch. (data: 3.10e-04). ETA=6 days, 16:25:37, max mem: 15.2 GB 
[06/20 16:20:54][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0012,	1.1883 s / batch. (data: 3.11e-04). ETA=6 days, 16:47:08, max mem: 15.2 GB 
[06/20 16:22:52][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0012,	1.1882 s / batch. (data: 3.33e-04). ETA=6 days, 16:43:42, max mem: 15.2 GB 
[06/20 16:24:51][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0014,	1.1842 s / batch. (data: 3.42e-04). ETA=6 days, 16:09:54, max mem: 15.2 GB 
[06/20 16:26:49][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0014,	1.1816 s / batch. (data: 2.90e-04). ETA=6 days, 15:46:48, max mem: 15.2 GB 
[06/20 16:28:48][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0012,	1.1829 s / batch. (data: 3.64e-04). ETA=6 days, 15:54:52, max mem: 15.2 GB 
[06/20 16:30:46][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0012,	1.1825 s / batch. (data: 2.93e-04). ETA=6 days, 15:50:10, max mem: 15.2 GB 
[06/20 16:32:44][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0011,	1.1804 s / batch. (data: 3.49e-04). ETA=6 days, 15:31:05, max mem: 15.2 GB 
[06/20 16:34:43][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0019,	1.1804 s / batch. (data: 3.52e-04). ETA=6 days, 15:28:41, max mem: 15.2 GB 
[06/20 16:36:41][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0012,	1.1864 s / batch. (data: 3.28e-04). ETA=6 days, 16:15:40, max mem: 15.2 GB 
[06/20 16:38:40][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0012,	1.1901 s / batch. (data: 3.12e-04). ETA=6 days, 16:43:47, max mem: 15.2 GB 
[06/20 16:40:38][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0015,	1.1879 s / batch. (data: 2.73e-04). ETA=6 days, 16:23:53, max mem: 15.2 GB 
[06/20 16:42:37][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0012,	1.1771 s / batch. (data: 3.20e-04). ETA=6 days, 14:54:02, max mem: 15.2 GB 
[06/20 16:44:35][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0022,	1.1860 s / batch. (data: 3.10e-04). ETA=6 days, 16:04:22, max mem: 15.2 GB 
[06/20 16:46:33][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0015,	1.1807 s / batch. (data: 3.75e-04). ETA=6 days, 15:19:23, max mem: 15.2 GB 
[06/20 16:48:32][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0012,	1.1854 s / batch. (data: 3.38e-04). ETA=6 days, 15:55:17, max mem: 15.2 GB 
[06/20 16:50:30][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0013,	1.1840 s / batch. (data: 3.89e-04). ETA=6 days, 15:42:04, max mem: 15.2 GB 
[06/20 16:52:29][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0013,	1.1955 s / batch. (data: 3.62e-04). ETA=6 days, 17:13:35, max mem: 15.2 GB 
[06/20 16:54:28][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0017,	1.1806 s / batch. (data: 1.14e-04). ETA=6 days, 15:11:07, max mem: 15.2 GB 
[06/20 16:54:34][INFO] visual_prompt:  319: Epoch 3 / 100: avg data time: 4.02e-03, avg batch time: 1.1918, average train loss: 0.0014
[06/20 17:03:29][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.001, 5.1856 s / batch. (data: 1.31e-04)max mem: 15.18835 GB 
[06/20 17:11:47][INFO] visual_prompt:  476: Inference (val):avg data time: 1.51e-04, avg batch time: 5.1727, average loss: 0.0008
[06/20 17:11:47][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_shallow_cls_reinit_10/val_imagenet_invariances.json
[06/20 17:11:47][INFO] visual_prompt:  257: Training 4 / 100 epoch, with learning rate 0.3
[06/20 17:14:30][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0015,	1.1834 s / batch. (data: 3.81e-04). ETA=6 days, 15:31:37, max mem: 15.2 GB 
[06/20 17:16:29][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0012,	1.1888 s / batch. (data: 3.45e-04). ETA=6 days, 16:12:53, max mem: 15.2 GB 
[06/20 17:18:27][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0015,	1.1765 s / batch. (data: 3.19e-04). ETA=6 days, 14:32:08, max mem: 15.2 GB 
[06/20 17:20:25][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0019,	1.1824 s / batch. (data: 2.99e-04). ETA=6 days, 15:17:51, max mem: 15.2 GB 
[06/20 17:22:23][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0012,	1.1760 s / batch. (data: 2.91e-04). ETA=6 days, 14:24:07, max mem: 15.2 GB 
[06/20 17:24:20][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0014,	1.1795 s / batch. (data: 3.32e-04). ETA=6 days, 14:50:26, max mem: 15.2 GB 
[06/20 17:26:18][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0011,	1.1740 s / batch. (data: 4.02e-04). ETA=6 days, 14:03:45, max mem: 15.2 GB 
[06/20 17:28:16][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0013,	1.1725 s / batch. (data: 3.97e-04). ETA=6 days, 13:49:57, max mem: 15.2 GB 
[06/20 17:30:14][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0013,	1.1799 s / batch. (data: 3.62e-04). ETA=6 days, 14:47:26, max mem: 15.2 GB 
[06/20 17:32:12][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0017,	1.1807 s / batch. (data: 3.89e-04). ETA=6 days, 14:52:18, max mem: 15.2 GB 
[06/20 17:34:11][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0014,	1.1869 s / batch. (data: 3.26e-04). ETA=6 days, 15:39:51, max mem: 15.2 GB 
[06/20 17:36:09][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0013,	1.1812 s / batch. (data: 3.02e-04). ETA=6 days, 14:51:53, max mem: 15.2 GB 
[06/20 17:38:07][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0013,	1.1821 s / batch. (data: 3.48e-04). ETA=6 days, 14:57:04, max mem: 15.2 GB 
[06/20 17:40:06][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0017,	1.1851 s / batch. (data: 3.89e-04). ETA=6 days, 15:19:32, max mem: 15.2 GB 
[06/20 17:42:04][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0013,	1.1852 s / batch. (data: 2.55e-04). ETA=6 days, 15:18:34, max mem: 15.2 GB 
[06/20 17:44:03][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0012,	1.1858 s / batch. (data: 3.31e-04). ETA=6 days, 15:21:36, max mem: 15.2 GB 
[06/20 17:46:01][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0015,	1.1843 s / batch. (data: 3.00e-04). ETA=6 days, 15:07:26, max mem: 15.2 GB 
[06/20 17:48:00][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0012,	1.1833 s / batch. (data: 2.95e-04). ETA=6 days, 14:57:09, max mem: 15.2 GB 
[06/20 17:49:58][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0010,	1.1818 s / batch. (data: 3.45e-04). ETA=6 days, 14:43:27, max mem: 15.2 GB 
[06/20 17:51:56][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0016,	1.1816 s / batch. (data: 3.11e-04). ETA=6 days, 14:39:36, max mem: 15.2 GB 
[06/20 17:53:55][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0013,	1.1858 s / batch. (data: 3.76e-04). ETA=6 days, 15:11:40, max mem: 15.2 GB 
[06/20 17:55:53][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0016,	1.1802 s / batch. (data: 2.74e-04). ETA=6 days, 14:24:22, max mem: 15.2 GB 
[06/20 17:57:51][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0014,	1.1870 s / batch. (data: 3.43e-04). ETA=6 days, 15:17:21, max mem: 15.2 GB 
[06/20 17:59:49][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0012,	1.1870 s / batch. (data: 2.99e-04). ETA=6 days, 15:14:57, max mem: 15.2 GB 
[06/20 18:01:48][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0012,	1.1784 s / batch. (data: 3.06e-04). ETA=6 days, 14:03:44, max mem: 15.2 GB 
[06/20 18:03:46][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0016,	1.1788 s / batch. (data: 3.51e-04). ETA=6 days, 14:05:32, max mem: 15.2 GB 
[06/20 18:05:44][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0014,	1.1843 s / batch. (data: 3.17e-04). ETA=6 days, 14:47:29, max mem: 15.2 GB 
[06/20 18:07:42][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0014,	1.1793 s / batch. (data: 3.18e-04). ETA=6 days, 14:05:23, max mem: 15.2 GB 
[06/20 18:09:40][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0014,	1.1865 s / batch. (data: 3.14e-04). ETA=6 days, 15:01:09, max mem: 15.2 GB 
[06/20 18:11:39][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0018,	1.1807 s / batch. (data: 3.54e-04). ETA=6 days, 14:12:46, max mem: 15.2 GB 
[06/20 18:13:37][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0015,	1.1858 s / batch. (data: 2.88e-04). ETA=6 days, 14:52:00, max mem: 15.2 GB 
[06/20 18:15:35][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0018,	1.1803 s / batch. (data: 3.17e-04). ETA=6 days, 14:05:15, max mem: 15.2 GB 
[06/20 18:17:33][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0012,	1.1837 s / batch. (data: 3.28e-04). ETA=6 days, 14:30:57, max mem: 15.2 GB 
[06/20 18:19:32][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0013,	1.1848 s / batch. (data: 3.07e-04). ETA=6 days, 14:37:30, max mem: 15.2 GB 
[06/20 18:21:30][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0011,	1.1862 s / batch. (data: 2.97e-04). ETA=6 days, 14:47:06, max mem: 15.2 GB 
[06/20 18:23:28][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0012,	1.1933 s / batch. (data: 2.80e-04). ETA=6 days, 15:42:07, max mem: 15.2 GB 
[06/20 18:25:26][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0014,	1.1831 s / batch. (data: 4.87e-04). ETA=6 days, 14:18:10, max mem: 15.2 GB 
[06/20 18:27:25][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0012,	1.1830 s / batch. (data: 3.01e-04). ETA=6 days, 14:15:32, max mem: 15.2 GB 
[06/20 18:29:23][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0012,	1.1813 s / batch. (data: 3.44e-04). ETA=6 days, 13:59:33, max mem: 15.2 GB 
[06/20 18:31:21][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0012,	1.1784 s / batch. (data: 5.98e-04). ETA=6 days, 13:34:44, max mem: 15.2 GB 
[06/20 18:33:19][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0019,	1.1818 s / batch. (data: 3.88e-04). ETA=6 days, 13:59:34, max mem: 15.2 GB 
[06/20 18:35:17][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0013,	1.1786 s / batch. (data: 4.11e-04). ETA=6 days, 13:31:53, max mem: 15.2 GB 
[06/20 18:37:15][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0017,	1.1838 s / batch. (data: 3.55e-04). ETA=6 days, 14:11:45, max mem: 15.2 GB 
[06/20 18:39:14][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0014,	1.1806 s / batch. (data: 3.18e-04). ETA=6 days, 13:44:02, max mem: 15.2 GB 
[06/20 18:41:12][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0011,	1.1761 s / batch. (data: 3.12e-04). ETA=6 days, 13:06:12, max mem: 15.2 GB 
[06/20 18:43:10][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0011,	1.1825 s / batch. (data: 2.78e-04). ETA=6 days, 13:55:40, max mem: 15.2 GB 
[06/20 18:45:08][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0013,	1.1783 s / batch. (data: 3.22e-04). ETA=6 days, 13:19:32, max mem: 15.2 GB 
[06/20 18:47:06][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0018,	1.1813 s / batch. (data: 3.09e-04). ETA=6 days, 13:42:03, max mem: 15.2 GB 
[06/20 18:49:04][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0014,	1.1771 s / batch. (data: 2.95e-04). ETA=6 days, 13:06:39, max mem: 15.2 GB 
[06/20 18:51:02][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0012,	1.1783 s / batch. (data: 2.04e-04). ETA=6 days, 13:14:05, max mem: 15.2 GB 
[06/20 18:51:08][INFO] visual_prompt:  319: Epoch 4 / 100: avg data time: 4.44e-03, avg batch time: 1.1911, average train loss: 0.0014
[06/20 19:00:05][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.001, 5.1876 s / batch. (data: 2.62e-04)max mem: 15.18835 GB 
[06/20 19:08:21][INFO] visual_prompt:  476: Inference (val):avg data time: 1.63e-04, avg batch time: 5.1727, average loss: 0.0009
[06/20 19:08:21][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_shallow_cls_reinit_10/val_imagenet_invariances.json
[06/20 19:08:22][INFO] visual_prompt:  257: Training 5 / 100 epoch, with learning rate 0.4
[06/20 19:11:06][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0012,	1.1800 s / batch. (data: 5.36e-04). ETA=6 days, 13:25:56, max mem: 15.2 GB 
[06/20 19:13:04][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0019,	1.1816 s / batch. (data: 3.19e-04). ETA=6 days, 13:36:10, max mem: 15.2 GB 
[06/20 19:15:02][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0013,	1.1800 s / batch. (data: 3.08e-04). ETA=6 days, 13:21:52, max mem: 15.2 GB 
[06/20 19:17:01][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0016,	1.1816 s / batch. (data: 3.49e-04). ETA=6 days, 13:32:14, max mem: 15.2 GB 
[06/20 19:18:59][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0018,	1.1798 s / batch. (data: 3.71e-04). ETA=6 days, 13:16:03, max mem: 15.2 GB 
[06/20 19:20:57][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0015,	1.1830 s / batch. (data: 2.81e-04). ETA=6 days, 13:39:31, max mem: 15.2 GB 
[06/20 19:22:56][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0015,	1.1817 s / batch. (data: 3.14e-04). ETA=6 days, 13:27:05, max mem: 15.2 GB 
[06/20 19:24:54][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0017,	1.1879 s / batch. (data: 3.29e-04). ETA=6 days, 14:14:43, max mem: 15.2 GB 
[06/20 19:26:53][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0012,	1.1865 s / batch. (data: 3.42e-04). ETA=6 days, 14:01:31, max mem: 15.2 GB 
[06/20 19:28:51][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0013,	1.1790 s / batch. (data: 5.03e-04). ETA=6 days, 12:59:38, max mem: 15.2 GB 
[06/20 19:30:49][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0013,	1.1788 s / batch. (data: 7.35e-04). ETA=6 days, 12:56:20, max mem: 15.2 GB 
[06/20 19:32:47][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0014,	1.1822 s / batch. (data: 3.22e-04). ETA=6 days, 13:21:21, max mem: 15.2 GB 
[06/20 19:34:46][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0015,	1.1846 s / batch. (data: 2.68e-04). ETA=6 days, 13:38:42, max mem: 15.2 GB 
[06/20 19:36:44][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0012,	1.1814 s / batch. (data: 3.00e-04). ETA=6 days, 13:11:01, max mem: 15.2 GB 
[06/20 19:38:42][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0016,	1.1856 s / batch. (data: 3.08e-04). ETA=6 days, 13:42:39, max mem: 15.2 GB 
[06/20 19:40:41][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0015,	1.1855 s / batch. (data: 3.12e-04). ETA=6 days, 13:40:16, max mem: 15.2 GB 
[06/20 19:42:39][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0012,	1.1827 s / batch. (data: 3.67e-04). ETA=6 days, 13:15:48, max mem: 15.2 GB 
[06/20 19:44:37][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0014,	1.1852 s / batch. (data: 3.34e-04). ETA=6 days, 13:33:31, max mem: 15.2 GB 
[06/20 19:46:35][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0011,	1.1844 s / batch. (data: 3.05e-04). ETA=6 days, 13:24:58, max mem: 15.2 GB 
[06/20 19:48:34][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0012,	1.1853 s / batch. (data: 3.07e-04). ETA=6 days, 13:30:39, max mem: 15.2 GB 
[06/20 19:50:32][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0016,	1.1836 s / batch. (data: 3.33e-04). ETA=6 days, 13:14:49, max mem: 15.2 GB 
[06/20 19:52:30][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0012,	1.1799 s / batch. (data: 3.26e-04). ETA=6 days, 12:43:06, max mem: 15.2 GB 
[06/20 19:54:29][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0012,	1.1820 s / batch. (data: 3.59e-04). ETA=6 days, 12:58:14, max mem: 15.2 GB 
[06/20 19:56:27][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0017,	1.1778 s / batch. (data: 2.88e-04). ETA=6 days, 12:22:36, max mem: 15.2 GB 
[06/20 19:58:25][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0012,	1.1818 s / batch. (data: 3.70e-04). ETA=6 days, 12:52:48, max mem: 15.2 GB 
[06/20 20:00:24][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0013,	1.1838 s / batch. (data: 4.22e-04). ETA=6 days, 13:06:18, max mem: 15.2 GB 
[06/20 20:02:22][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0015,	1.1953 s / batch. (data: 3.67e-04). ETA=6 days, 14:35:52, max mem: 15.2 GB 
[06/20 20:04:20][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0015,	1.1826 s / batch. (data: 3.14e-04). ETA=6 days, 12:53:08, max mem: 15.2 GB 
[06/20 20:06:18][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0012,	1.1840 s / batch. (data: 3.34e-04). ETA=6 days, 13:02:27, max mem: 15.2 GB 
[06/20 20:08:17][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0017,	1.1831 s / batch. (data: 4.22e-04). ETA=6 days, 12:53:21, max mem: 15.2 GB 
[06/20 20:10:15][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0019,	1.1877 s / batch. (data: 2.85e-04). ETA=6 days, 13:27:35, max mem: 15.2 GB 
[06/20 20:12:14][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0016,	1.1822 s / batch. (data: 2.92e-04). ETA=6 days, 12:42:16, max mem: 15.2 GB 
[06/20 20:14:12][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0011,	1.1845 s / batch. (data: 3.80e-04). ETA=6 days, 12:58:43, max mem: 15.2 GB 
[06/20 20:16:10][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0014,	1.1840 s / batch. (data: 3.12e-04). ETA=6 days, 12:52:44, max mem: 15.2 GB 
[06/20 20:18:09][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0017,	1.1820 s / batch. (data: 3.21e-04). ETA=6 days, 12:34:50, max mem: 15.2 GB 
[06/20 20:20:07][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0013,	1.1841 s / batch. (data: 2.92e-04). ETA=6 days, 12:49:25, max mem: 15.2 GB 
[06/20 20:22:05][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0020,	1.1798 s / batch. (data: 3.03e-04). ETA=6 days, 12:12:51, max mem: 15.2 GB 
[06/20 20:24:03][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0015,	1.1796 s / batch. (data: 3.04e-04). ETA=6 days, 12:09:47, max mem: 15.2 GB 
[06/20 20:26:01][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0014,	1.1775 s / batch. (data: 3.71e-04). ETA=6 days, 11:51:23, max mem: 15.2 GB 
[06/20 20:27:59][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0013,	1.1829 s / batch. (data: 3.27e-04). ETA=6 days, 12:31:58, max mem: 15.2 GB 
[06/20 20:29:57][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0014,	1.1790 s / batch. (data: 2.84e-04). ETA=6 days, 11:59:13, max mem: 15.2 GB 
[06/20 20:31:55][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0019,	1.1801 s / batch. (data: 3.51e-04). ETA=6 days, 12:05:32, max mem: 15.2 GB 
[06/20 20:33:53][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0015,	1.1768 s / batch. (data: 2.43e-04). ETA=6 days, 11:37:44, max mem: 15.2 GB 
[06/20 20:35:51][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0016,	1.1813 s / batch. (data: 2.99e-04). ETA=6 days, 12:11:03, max mem: 15.2 GB 
[06/20 20:37:49][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0012,	1.1796 s / batch. (data: 3.19e-04). ETA=6 days, 11:55:32, max mem: 15.2 GB 
[06/20 20:39:47][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0011,	1.1819 s / batch. (data: 3.74e-04). ETA=6 days, 12:12:07, max mem: 15.2 GB 
[06/20 20:41:45][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0020,	1.1782 s / batch. (data: 3.22e-04). ETA=6 days, 11:40:31, max mem: 15.2 GB 
[06/20 20:43:43][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0010,	1.1816 s / batch. (data: 3.09e-04). ETA=6 days, 12:05:26, max mem: 15.2 GB 
[06/20 20:45:40][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0013,	1.1766 s / batch. (data: 3.97e-04). ETA=6 days, 11:24:17, max mem: 15.2 GB 
[06/20 20:47:38][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0012,	1.1733 s / batch. (data: 1.08e-04). ETA=6 days, 10:56:29, max mem: 15.2 GB 
[06/20 20:47:44][INFO] visual_prompt:  319: Epoch 5 / 100: avg data time: 4.51e-03, avg batch time: 1.1914, average train loss: 0.0014
[06/20 20:56:37][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.001, 5.1594 s / batch. (data: 3.19e-05)max mem: 15.18835 GB 
[06/20 21:04:52][INFO] visual_prompt:  476: Inference (val):avg data time: 1.47e-04, avg batch time: 5.1526, average loss: 0.0009
[06/20 21:04:52][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_shallow_cls_reinit_10/val_imagenet_invariances.json
[06/20 21:04:52][INFO] visual_prompt:  257: Training 6 / 100 epoch, with learning rate 0.5
[06/20 21:07:35][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0012,	1.1790 s / batch. (data: 3.15e-04). ETA=6 days, 11:39:23, max mem: 15.2 GB 
[06/20 21:09:33][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0013,	1.1832 s / batch. (data: 3.40e-04). ETA=6 days, 12:10:13, max mem: 15.2 GB 
[06/20 21:11:31][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0019,	1.1779 s / batch. (data: 2.71e-04). ETA=6 days, 11:26:35, max mem: 15.2 GB 
[06/20 21:13:29][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0015,	1.1793 s / batch. (data: 3.33e-04). ETA=6 days, 11:35:24, max mem: 15.2 GB 
[06/20 21:15:27][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0011,	1.1812 s / batch. (data: 2.99e-04). ETA=6 days, 11:48:54, max mem: 15.2 GB 
[06/20 21:17:25][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0011,	1.1763 s / batch. (data: 3.19e-04). ETA=6 days, 11:08:26, max mem: 15.2 GB 
[06/20 21:19:23][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0012,	1.1778 s / batch. (data: 3.36e-04). ETA=6 days, 11:17:36, max mem: 15.2 GB 
[06/20 21:21:21][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0011,	1.1755 s / batch. (data: 3.18e-04). ETA=6 days, 10:57:43, max mem: 15.2 GB 
[06/20 21:23:19][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0015,	1.1768 s / batch. (data: 3.02e-04). ETA=6 days, 11:06:27, max mem: 15.2 GB 
[06/20 21:25:17][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0019,	1.1760 s / batch. (data: 3.04e-04). ETA=6 days, 10:57:36, max mem: 15.2 GB 
[06/20 21:27:14][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0013,	1.1777 s / batch. (data: 3.23e-04). ETA=6 days, 11:09:14, max mem: 15.2 GB 
[06/20 21:29:12][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0012,	1.1826 s / batch. (data: 2.98e-04). ETA=6 days, 11:45:59, max mem: 15.2 GB 
[06/20 21:31:10][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0019,	1.1812 s / batch. (data: 2.71e-04). ETA=6 days, 11:33:21, max mem: 15.2 GB 
[06/20 21:33:08][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0015,	1.1811 s / batch. (data: 3.01e-04). ETA=6 days, 11:30:05, max mem: 15.2 GB 
[06/20 21:35:06][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0013,	1.1797 s / batch. (data: 3.06e-04). ETA=6 days, 11:17:14, max mem: 15.2 GB 
[06/20 21:37:04][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0013,	1.1782 s / batch. (data: 2.85e-04). ETA=6 days, 11:03:42, max mem: 15.2 GB 
[06/20 21:39:02][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0013,	1.1795 s / batch. (data: 2.74e-04). ETA=6 days, 11:11:58, max mem: 15.2 GB 
[06/20 21:41:00][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0014,	1.1786 s / batch. (data: 3.04e-04). ETA=6 days, 11:02:28, max mem: 15.2 GB 
[06/20 21:42:58][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0013,	1.1764 s / batch. (data: 2.98e-04). ETA=6 days, 10:43:07, max mem: 15.2 GB 
[06/20 21:44:56][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0012,	1.1764 s / batch. (data: 2.87e-04). ETA=6 days, 10:41:09, max mem: 15.2 GB 
[06/20 21:46:54][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0016,	1.1774 s / batch. (data: 2.99e-04). ETA=6 days, 10:47:31, max mem: 15.2 GB 
[06/20 21:48:51][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0012,	1.1761 s / batch. (data: 3.01e-04). ETA=6 days, 10:35:14, max mem: 15.2 GB 
[06/20 21:50:49][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0015,	1.1800 s / batch. (data: 2.88e-04). ETA=6 days, 11:03:54, max mem: 15.2 GB 
[06/20 21:52:47][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0013,	1.1787 s / batch. (data: 3.14e-04). ETA=6 days, 10:51:52, max mem: 15.2 GB 
[06/20 21:54:45][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0016,	1.1786 s / batch. (data: 2.55e-04). ETA=6 days, 10:48:47, max mem: 15.2 GB 
[06/20 21:56:43][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0016,	1.1774 s / batch. (data: 3.76e-04). ETA=6 days, 10:37:46, max mem: 15.2 GB 
[06/20 21:58:41][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0012,	1.1842 s / batch. (data: 3.82e-04). ETA=6 days, 11:28:50, max mem: 15.2 GB 
[06/20 22:00:39][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0014,	1.1778 s / batch. (data: 2.80e-04). ETA=6 days, 10:36:44, max mem: 15.2 GB 
[06/20 22:02:37][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0018,	1.1827 s / batch. (data: 3.64e-04). ETA=6 days, 11:13:21, max mem: 15.2 GB 
[06/20 22:04:35][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0011,	1.1826 s / batch. (data: 3.17e-04). ETA=6 days, 11:10:54, max mem: 15.2 GB 
[06/20 22:06:33][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0012,	1.1828 s / batch. (data: 3.52e-04). ETA=6 days, 11:10:19, max mem: 15.2 GB 
[06/20 22:08:31][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0016,	1.1811 s / batch. (data: 2.98e-04). ETA=6 days, 10:54:58, max mem: 15.2 GB 
[06/20 22:10:29][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0016,	1.1811 s / batch. (data: 3.23e-04). ETA=6 days, 10:52:41, max mem: 15.2 GB 
[06/20 22:12:27][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0011,	1.1732 s / batch. (data: 3.63e-04). ETA=6 days, 9:49:04, max mem: 15.2 GB 
[06/20 22:14:25][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0012,	1.1784 s / batch. (data: 2.89e-04). ETA=6 days, 10:27:44, max mem: 15.2 GB 
[06/20 22:16:23][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0016,	1.1770 s / batch. (data: 2.85e-04). ETA=6 days, 10:14:45, max mem: 15.2 GB 
[06/20 22:18:21][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0013,	1.1821 s / batch. (data: 2.99e-04). ETA=6 days, 10:52:38, max mem: 15.2 GB 
[06/20 22:20:19][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0013,	1.1836 s / batch. (data: 3.17e-04). ETA=6 days, 11:02:42, max mem: 15.2 GB 
[06/20 22:22:17][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0014,	1.1808 s / batch. (data: 3.06e-04). ETA=6 days, 10:39:03, max mem: 15.2 GB 
[06/20 22:24:15][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0015,	1.1792 s / batch. (data: 2.96e-04). ETA=6 days, 10:24:03, max mem: 15.2 GB 
[06/20 22:26:13][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0012,	1.1792 s / batch. (data: 3.17e-04). ETA=6 days, 10:22:25, max mem: 15.2 GB 
[06/20 22:28:11][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0013,	1.1801 s / batch. (data: 3.07e-04). ETA=6 days, 10:27:09, max mem: 15.2 GB 
[06/20 22:30:09][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0015,	1.1791 s / batch. (data: 2.80e-04). ETA=6 days, 10:17:10, max mem: 15.2 GB 
[06/20 22:32:07][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0012,	1.1796 s / batch. (data: 2.88e-04). ETA=6 days, 10:19:36, max mem: 15.2 GB 
[06/20 22:34:04][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0014,	1.1796 s / batch. (data: 2.68e-04). ETA=6 days, 10:17:29, max mem: 15.2 GB 
[06/20 22:36:03][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0012,	1.1887 s / batch. (data: 2.81e-04). ETA=6 days, 11:27:19, max mem: 15.2 GB 
[06/20 22:38:01][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0012,	1.1801 s / batch. (data: 3.26e-04). ETA=6 days, 10:17:33, max mem: 15.2 GB 
[06/20 22:39:59][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0013,	1.1776 s / batch. (data: 2.84e-04). ETA=6 days, 9:56:16, max mem: 15.2 GB 
[06/20 22:41:57][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0014,	1.1752 s / batch. (data: 2.92e-04). ETA=6 days, 9:35:16, max mem: 15.2 GB 
[06/20 22:43:54][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0014,	1.1831 s / batch. (data: 1.18e-04). ETA=6 days, 10:34:58, max mem: 15.2 GB 
[06/20 22:44:00][INFO] visual_prompt:  319: Epoch 6 / 100: avg data time: 4.02e-03, avg batch time: 1.1884, average train loss: 0.0014
[06/20 22:52:54][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.001, 5.1663 s / batch. (data: 1.74e-04)max mem: 15.18835 GB 
[06/20 23:01:10][INFO] visual_prompt:  476: Inference (val):avg data time: 1.34e-04, avg batch time: 5.1621, average loss: 0.0007
[06/20 23:01:10][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_shallow_cls_reinit_10/val_imagenet_invariances.json
[06/20 23:01:10][INFO] visual_prompt:  257: Training 7 / 100 epoch, with learning rate 0.6
[06/20 23:03:54][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0013,	1.1877 s / batch. (data: 2.82e-04). ETA=6 days, 11:09:16, max mem: 15.2 GB 
[06/20 23:05:52][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0014,	1.1794 s / batch. (data: 3.92e-04). ETA=6 days, 10:02:00, max mem: 15.2 GB 
[06/20 23:07:50][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0016,	1.1773 s / batch. (data: 3.33e-04). ETA=6 days, 9:43:47, max mem: 15.2 GB 
[06/20 23:09:48][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0012,	1.1847 s / batch. (data: 3.46e-04). ETA=6 days, 10:39:44, max mem: 15.2 GB 
[06/20 23:11:47][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0013,	1.1871 s / batch. (data: 3.54e-04). ETA=6 days, 10:56:33, max mem: 15.2 GB 
[06/20 23:13:45][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0017,	1.1851 s / batch. (data: 3.32e-04). ETA=6 days, 10:39:04, max mem: 15.2 GB 
[06/20 23:15:44][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0012,	1.1894 s / batch. (data: 2.97e-04). ETA=6 days, 11:10:26, max mem: 15.2 GB 
[06/20 23:17:42][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0012,	1.1863 s / batch. (data: 4.55e-04). ETA=6 days, 10:43:56, max mem: 15.2 GB 
[06/20 23:19:41][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0012,	1.1873 s / batch. (data: 3.17e-04). ETA=6 days, 10:50:12, max mem: 15.2 GB 
[06/20 23:21:39][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0026,	1.1869 s / batch. (data: 3.72e-04). ETA=6 days, 10:45:17, max mem: 15.2 GB 
[06/20 23:23:38][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0016,	1.1864 s / batch. (data: 3.02e-04). ETA=6 days, 10:39:27, max mem: 15.2 GB 
[06/20 23:25:36][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0020,	1.1895 s / batch. (data: 3.52e-04). ETA=6 days, 11:01:11, max mem: 15.2 GB 
[06/20 23:27:34][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0014,	1.1883 s / batch. (data: 3.64e-04). ETA=6 days, 10:50:02, max mem: 15.2 GB 
[06/20 23:29:33][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0014,	1.1831 s / batch. (data: 4.12e-04). ETA=6 days, 10:07:19, max mem: 15.2 GB 
[06/20 23:31:31][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0014,	1.1825 s / batch. (data: 2.71e-04). ETA=6 days, 10:00:47, max mem: 15.2 GB 
[06/20 23:33:29][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0017,	1.1849 s / batch. (data: 3.04e-04). ETA=6 days, 10:17:29, max mem: 15.2 GB 
[06/20 23:35:27][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0019,	1.1909 s / batch. (data: 3.45e-04). ETA=6 days, 11:02:31, max mem: 15.2 GB 
[06/20 23:37:26][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0013,	1.1828 s / batch. (data: 3.70e-04). ETA=6 days, 9:57:00, max mem: 15.2 GB 
[06/20 23:39:24][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0015,	1.1866 s / batch. (data: 4.24e-04). ETA=6 days, 10:25:11, max mem: 15.2 GB 
[06/20 23:41:22][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0014,	1.1814 s / batch. (data: 3.32e-04). ETA=6 days, 9:41:59, max mem: 15.2 GB 
[06/20 23:43:21][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0019,	1.1833 s / batch. (data: 2.82e-04). ETA=6 days, 9:55:10, max mem: 15.2 GB 
[06/20 23:45:19][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0014,	1.1828 s / batch. (data: 2.85e-04). ETA=6 days, 9:49:41, max mem: 15.2 GB 
[06/20 23:47:17][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0014,	1.1803 s / batch. (data: 2.64e-04). ETA=6 days, 9:28:03, max mem: 15.2 GB 
[06/20 23:49:16][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0015,	1.1830 s / batch. (data: 3.02e-04). ETA=6 days, 9:47:05, max mem: 15.2 GB 
[06/20 23:51:14][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0014,	1.1822 s / batch. (data: 3.22e-04). ETA=6 days, 9:38:55, max mem: 15.2 GB 
[06/20 23:53:12][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0013,	1.1838 s / batch. (data: 3.09e-04). ETA=6 days, 9:49:24, max mem: 15.2 GB 
[06/20 23:55:11][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0013,	1.1772 s / batch. (data: 3.33e-04). ETA=6 days, 8:55:52, max mem: 15.2 GB 
[06/20 23:57:08][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0019,	1.1780 s / batch. (data: 3.59e-04). ETA=6 days, 9:00:08, max mem: 15.2 GB 
[06/20 23:59:07][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0013,	1.1827 s / batch. (data: 2.69e-04). ETA=6 days, 9:34:49, max mem: 15.2 GB 
[06/21 00:01:05][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0015,	1.1811 s / batch. (data: 3.46e-04). ETA=6 days, 9:20:31, max mem: 15.2 GB 
[06/21 00:03:03][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0014,	1.1833 s / batch. (data: 3.11e-04). ETA=6 days, 9:35:42, max mem: 15.2 GB 
[06/21 00:05:02][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0014,	1.1875 s / batch. (data: 3.55e-04). ETA=6 days, 10:06:21, max mem: 15.2 GB 
[06/21 00:07:00][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0017,	1.1795 s / batch. (data: 3.31e-04). ETA=6 days, 9:01:51, max mem: 15.2 GB 
[06/21 00:08:59][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0014,	1.1888 s / batch. (data: 3.20e-04). ETA=6 days, 10:12:29, max mem: 15.2 GB 
[06/21 00:10:57][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0013,	1.1895 s / batch. (data: 3.01e-04). ETA=6 days, 10:15:40, max mem: 15.2 GB 
[06/21 00:12:56][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0011,	1.1876 s / batch. (data: 3.36e-04). ETA=6 days, 9:58:52, max mem: 15.2 GB 
[06/21 00:14:55][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0019,	1.1878 s / batch. (data: 3.53e-04). ETA=6 days, 9:58:48, max mem: 15.2 GB 
[06/21 00:16:54][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0014,	1.1908 s / batch. (data: 3.50e-04). ETA=6 days, 10:20:05, max mem: 15.2 GB 
[06/21 00:18:53][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0018,	1.1875 s / batch. (data: 3.09e-04). ETA=6 days, 9:52:22, max mem: 15.2 GB 
[06/21 00:20:52][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0012,	1.1816 s / batch. (data: 3.22e-04). ETA=6 days, 9:04:47, max mem: 15.2 GB 
[06/21 00:22:50][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0018,	1.1814 s / batch. (data: 3.19e-04). ETA=6 days, 9:01:13, max mem: 15.2 GB 
[06/21 00:24:49][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0016,	1.1797 s / batch. (data: 3.15e-04). ETA=6 days, 8:45:38, max mem: 15.2 GB 
[06/21 00:26:47][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0014,	1.1791 s / batch. (data: 2.86e-04). ETA=6 days, 8:39:00, max mem: 15.2 GB 
[06/21 00:28:46][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0011,	1.1844 s / batch. (data: 3.74e-04). ETA=6 days, 9:18:18, max mem: 15.2 GB 
[06/21 00:30:44][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0030,	1.1809 s / batch. (data: 2.91e-04). ETA=6 days, 8:48:53, max mem: 15.2 GB 
[06/21 00:32:42][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0017,	1.1825 s / batch. (data: 3.25e-04). ETA=6 days, 8:59:21, max mem: 15.2 GB 
[06/21 00:34:40][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0014,	1.1875 s / batch. (data: 3.69e-04). ETA=6 days, 9:36:20, max mem: 15.2 GB 
[06/21 00:36:39][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0015,	1.1826 s / batch. (data: 3.12e-04). ETA=6 days, 8:56:50, max mem: 15.2 GB 
[06/21 00:38:37][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0014,	1.1813 s / batch. (data: 2.91e-04). ETA=6 days, 8:44:14, max mem: 15.2 GB 
[06/21 00:40:36][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0013,	1.1848 s / batch. (data: 1.14e-04). ETA=6 days, 9:09:17, max mem: 15.2 GB 
[06/21 00:40:42][INFO] visual_prompt:  319: Epoch 7 / 100: avg data time: 4.18e-03, avg batch time: 1.1931, average train loss: 0.0015
[06/21 00:49:35][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.001, 5.1715 s / batch. (data: 3.52e-04)max mem: 15.18835 GB 
[06/21 00:57:52][INFO] visual_prompt:  476: Inference (val):avg data time: 1.51e-04, avg batch time: 5.1577, average loss: 0.0008
[06/21 00:57:52][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_shallow_cls_reinit_10/val_imagenet_invariances.json
[06/21 00:57:52][INFO] visual_prompt:  257: Training 8 / 100 epoch, with learning rate 0.7
[06/21 01:00:33][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0020,	1.1832 s / batch. (data: 3.14e-04). ETA=6 days, 8:54:51, max mem: 15.2 GB 
[06/21 01:02:32][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0017,	1.1888 s / batch. (data: 3.14e-04). ETA=6 days, 9:36:44, max mem: 15.2 GB 
[06/21 01:04:30][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0016,	1.1801 s / batch. (data: 3.45e-04). ETA=6 days, 8:26:48, max mem: 15.2 GB 
[06/21 01:06:28][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0021,	1.1827 s / batch. (data: 4.11e-04). ETA=6 days, 8:45:16, max mem: 15.2 GB 
[06/21 01:08:27][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0014,	1.1896 s / batch. (data: 5.29e-04). ETA=6 days, 9:37:05, max mem: 15.2 GB 
[06/21 01:10:25][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0016,	1.1778 s / batch. (data: 3.89e-04). ETA=6 days, 8:03:41, max mem: 15.2 GB 
[06/21 01:12:23][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0013,	1.1809 s / batch. (data: 3.34e-04). ETA=6 days, 8:25:24, max mem: 15.2 GB 
[06/21 01:14:21][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0023,	1.1947 s / batch. (data: 3.64e-04). ETA=6 days, 10:10:15, max mem: 15.2 GB 
[06/21 01:16:19][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0024,	1.1796 s / batch. (data: 3.31e-04). ETA=6 days, 8:11:35, max mem: 15.2 GB 
[06/21 01:18:18][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0019,	1.1808 s / batch. (data: 3.09e-04). ETA=6 days, 8:18:53, max mem: 15.2 GB 
[06/21 01:20:16][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0021,	1.1785 s / batch. (data: 3.52e-04). ETA=6 days, 7:59:16, max mem: 15.2 GB 
[06/21 01:22:14][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0013,	1.1814 s / batch. (data: 3.76e-04). ETA=6 days, 8:19:14, max mem: 15.2 GB 
[06/21 01:24:13][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0017,	1.1835 s / batch. (data: 3.18e-04). ETA=6 days, 8:34:02, max mem: 15.2 GB 
[06/21 01:26:11][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0014,	1.1887 s / batch. (data: 3.48e-04). ETA=6 days, 9:11:47, max mem: 15.2 GB 
[06/21 01:28:10][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0016,	1.1815 s / batch. (data: 3.81e-04). ETA=6 days, 8:14:44, max mem: 15.2 GB 
[06/21 01:30:08][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0015,	1.1812 s / batch. (data: 3.67e-04). ETA=6 days, 8:10:24, max mem: 15.2 GB 
[06/21 01:32:06][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0013,	1.1796 s / batch. (data: 3.24e-04). ETA=6 days, 7:55:31, max mem: 15.2 GB 
[06/21 01:34:04][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0015,	1.1800 s / batch. (data: 3.05e-04). ETA=6 days, 7:56:47, max mem: 15.2 GB 
[06/21 01:36:02][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0015,	1.1827 s / batch. (data: 3.01e-04). ETA=6 days, 8:15:53, max mem: 15.2 GB 
[06/21 01:38:01][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0014,	1.1863 s / batch. (data: 3.16e-04). ETA=6 days, 8:41:59, max mem: 15.2 GB 
[06/21 01:39:59][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0014,	1.1805 s / batch. (data: 3.60e-04). ETA=6 days, 7:54:31, max mem: 15.2 GB 
[06/21 01:41:57][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0015,	1.1809 s / batch. (data: 3.35e-04). ETA=6 days, 7:56:18, max mem: 15.2 GB 
[06/21 01:43:56][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0017,	1.1779 s / batch. (data: 3.34e-04). ETA=6 days, 7:30:44, max mem: 15.2 GB 
[06/21 01:45:54][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0015,	1.1802 s / batch. (data: 3.49e-04). ETA=6 days, 7:46:47, max mem: 15.2 GB 
[06/21 01:47:52][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0014,	1.1898 s / batch. (data: 3.14e-04). ETA=6 days, 8:58:28, max mem: 15.2 GB 
[06/21 01:49:51][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0019,	1.1826 s / batch. (data: 3.65e-04). ETA=6 days, 8:01:24, max mem: 15.2 GB 
[06/21 01:51:49][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0015,	1.1805 s / batch. (data: 3.38e-04). ETA=6 days, 7:43:26, max mem: 15.2 GB 
[06/21 01:53:47][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0013,	1.1846 s / batch. (data: 3.66e-04). ETA=6 days, 8:12:27, max mem: 15.2 GB 
[06/21 01:55:45][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0014,	1.1763 s / batch. (data: 3.68e-04). ETA=6 days, 7:06:45, max mem: 15.2 GB 
[06/21 01:57:44][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0025,	1.1773 s / batch. (data: 3.29e-04). ETA=6 days, 7:12:09, max mem: 15.2 GB 
[06/21 01:59:41][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0014,	1.1773 s / batch. (data: 3.41e-04). ETA=6 days, 7:10:14, max mem: 15.2 GB 
[06/21 02:01:40][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0014,	1.1856 s / batch. (data: 3.34e-04). ETA=6 days, 8:12:10, max mem: 15.2 GB 
[06/21 02:03:38][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0014,	1.1828 s / batch. (data: 4.03e-04). ETA=6 days, 7:48:37, max mem: 15.2 GB 
[06/21 02:05:36][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0012,	1.1832 s / batch. (data: 3.50e-04). ETA=6 days, 7:49:52, max mem: 15.2 GB 
[06/21 02:07:35][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0013,	1.1878 s / batch. (data: 3.19e-04). ETA=6 days, 8:23:25, max mem: 15.2 GB 
[06/21 02:09:33][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0015,	1.1754 s / batch. (data: 3.82e-04). ETA=6 days, 6:46:14, max mem: 15.2 GB 
[06/21 02:11:31][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0014,	1.1875 s / batch. (data: 3.61e-04). ETA=6 days, 8:16:59, max mem: 15.2 GB 
[06/21 02:13:30][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0014,	1.1800 s / batch. (data: 3.85e-04). ETA=6 days, 7:17:24, max mem: 15.2 GB 
[06/21 02:15:28][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0013,	1.1800 s / batch. (data: 3.65e-04). ETA=6 days, 7:15:23, max mem: 15.2 GB 
[06/21 02:17:26][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0014,	1.1866 s / batch. (data: 3.47e-04). ETA=6 days, 8:04:17, max mem: 15.2 GB 
[06/21 02:19:25][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0020,	1.1865 s / batch. (data: 2.81e-04). ETA=6 days, 8:01:37, max mem: 15.2 GB 
[06/21 02:21:23][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0014,	1.1847 s / batch. (data: 2.80e-04). ETA=6 days, 7:45:43, max mem: 15.2 GB 
[06/21 02:23:21][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0012,	1.1792 s / batch. (data: 3.04e-04). ETA=6 days, 7:01:57, max mem: 15.2 GB 
[06/21 02:25:19][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0013,	1.1874 s / batch. (data: 2.99e-04). ETA=6 days, 8:02:39, max mem: 15.2 GB 
[06/21 02:27:18][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0016,	1.1854 s / batch. (data: 4.09e-04). ETA=6 days, 7:45:17, max mem: 15.2 GB 
[06/21 02:29:16][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0015,	1.1827 s / batch. (data: 4.01e-04). ETA=6 days, 7:22:53, max mem: 15.2 GB 
[06/21 02:31:15][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0013,	1.1880 s / batch. (data: 3.38e-04). ETA=6 days, 8:01:25, max mem: 15.2 GB 
[06/21 02:33:13][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0019,	1.1840 s / batch. (data: 5.79e-04). ETA=6 days, 7:28:22, max mem: 15.2 GB 
[06/21 02:35:11][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0014,	1.1843 s / batch. (data: 3.22e-04). ETA=6 days, 7:28:49, max mem: 15.2 GB 
[06/21 02:37:09][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0018,	1.1759 s / batch. (data: 1.22e-04). ETA=6 days, 6:22:40, max mem: 15.2 GB 
[06/21 02:37:15][INFO] visual_prompt:  319: Epoch 8 / 100: avg data time: 4.22e-03, avg batch time: 1.1915, average train loss: 0.0016
[06/21 02:46:09][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.001, 5.1705 s / batch. (data: 1.15e-04)max mem: 15.18835 GB 
[06/21 02:54:26][INFO] visual_prompt:  476: Inference (val):avg data time: 1.56e-04, avg batch time: 5.1641, average loss: 0.0009
[06/21 02:54:26][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_shallow_cls_reinit_10/val_imagenet_invariances.json
[06/21 02:54:26][INFO] visual_prompt:  257: Training 9 / 100 epoch, with learning rate 0.8
[06/21 02:57:10][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0014,	1.1868 s / batch. (data: 3.61e-04). ETA=6 days, 7:44:07, max mem: 15.2 GB 
[06/21 02:59:08][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0014,	1.1805 s / batch. (data: 3.60e-04). ETA=6 days, 6:54:04, max mem: 15.2 GB 
[06/21 03:01:07][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0014,	1.1855 s / batch. (data: 3.23e-04). ETA=6 days, 7:30:05, max mem: 15.2 GB 
[06/21 03:03:05][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0014,	1.1835 s / batch. (data: 3.54e-04). ETA=6 days, 7:13:12, max mem: 15.2 GB 
[06/21 03:05:03][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0015,	1.1799 s / batch. (data: 3.38e-04). ETA=6 days, 6:43:12, max mem: 15.2 GB 
[06/21 03:07:02][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0013,	1.1910 s / batch. (data: 2.84e-04). ETA=6 days, 8:06:14, max mem: 15.2 GB 
[06/21 03:09:00][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0020,	1.1854 s / batch. (data: 2.97e-04). ETA=6 days, 7:21:13, max mem: 15.2 GB 
[06/21 03:10:59][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0015,	1.1783 s / batch. (data: 3.24e-04). ETA=6 days, 6:25:15, max mem: 15.2 GB 
[06/21 03:12:57][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0016,	1.1815 s / batch. (data: 3.01e-04). ETA=6 days, 6:47:40, max mem: 15.2 GB 
[06/21 03:14:55][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0018,	1.1821 s / batch. (data: 3.55e-04). ETA=6 days, 6:50:10, max mem: 15.2 GB 
[06/21 03:16:53][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0013,	1.1838 s / batch. (data: 3.32e-04). ETA=6 days, 7:01:19, max mem: 15.2 GB 
[06/21 03:18:52][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0015,	1.1865 s / batch. (data: 3.15e-04). ETA=6 days, 7:20:17, max mem: 15.2 GB 
[06/21 03:20:50][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0013,	1.1844 s / batch. (data: 4.10e-04). ETA=6 days, 7:02:03, max mem: 15.2 GB 
[06/21 03:22:48][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0012,	1.1838 s / batch. (data: 2.86e-04). ETA=6 days, 6:55:25, max mem: 15.2 GB 
[06/21 03:24:47][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0014,	1.1858 s / batch. (data: 3.24e-04). ETA=6 days, 7:08:43, max mem: 15.2 GB 
[06/21 03:26:45][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0014,	1.1812 s / batch. (data: 3.45e-04). ETA=6 days, 6:31:50, max mem: 15.2 GB 
[06/21 03:28:43][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0020,	1.1796 s / batch. (data: 3.16e-04). ETA=6 days, 6:17:35, max mem: 15.2 GB 
[06/21 03:30:42][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0015,	1.1825 s / batch. (data: 3.62e-04). ETA=6 days, 6:37:51, max mem: 15.2 GB 
[06/21 03:32:40][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0018,	1.1827 s / batch. (data: 3.39e-04). ETA=6 days, 6:37:12, max mem: 15.2 GB 
[06/21 03:34:38][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0016,	1.1876 s / batch. (data: 3.78e-04). ETA=6 days, 7:12:33, max mem: 15.2 GB 
[06/21 03:36:36][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0015,	1.1793 s / batch. (data: 3.53e-04). ETA=6 days, 6:06:55, max mem: 15.2 GB 
[06/21 03:38:34][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0016,	1.1809 s / batch. (data: 3.54e-04). ETA=6 days, 6:17:40, max mem: 15.2 GB 
[06/21 03:40:33][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0019,	1.1900 s / batch. (data: 3.16e-04). ETA=6 days, 7:25:02, max mem: 15.2 GB 
[06/21 03:42:31][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0020,	1.1845 s / batch. (data: 3.50e-04). ETA=6 days, 6:41:08, max mem: 15.2 GB 
[06/21 03:44:29][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0013,	1.1778 s / batch. (data: 3.47e-04). ETA=6 days, 5:48:00, max mem: 15.2 GB 
[06/21 03:46:28][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0014,	1.1840 s / batch. (data: 3.44e-04). ETA=6 days, 6:33:32, max mem: 15.2 GB 
[06/21 03:48:26][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0013,	1.1833 s / batch. (data: 3.63e-04). ETA=6 days, 6:25:48, max mem: 15.2 GB 
[06/21 03:50:24][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0014,	1.1867 s / batch. (data: 3.42e-04). ETA=6 days, 6:50:12, max mem: 15.2 GB 
[06/21 03:52:23][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0022,	1.1813 s / batch. (data: 3.47e-04). ETA=6 days, 6:06:52, max mem: 15.2 GB 
[06/21 03:54:21][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0013,	1.1836 s / batch. (data: 3.05e-04). ETA=6 days, 6:22:07, max mem: 15.2 GB 
[06/21 03:56:19][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0012,	1.1814 s / batch. (data: 3.36e-04). ETA=6 days, 6:03:59, max mem: 15.2 GB 
[06/21 03:58:18][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0017,	1.1843 s / batch. (data: 3.28e-04). ETA=6 days, 6:23:53, max mem: 15.2 GB 
[06/21 04:00:16][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0014,	1.1750 s / batch. (data: 3.34e-04). ETA=6 days, 5:10:52, max mem: 15.2 GB 
[06/21 04:02:14][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0030,	1.1944 s / batch. (data: 3.66e-04). ETA=6 days, 7:36:36, max mem: 15.2 GB 
[06/21 04:04:12][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0014,	1.1805 s / batch. (data: 3.47e-04). ETA=6 days, 5:48:47, max mem: 15.2 GB 
[06/21 04:06:10][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0013,	1.1832 s / batch. (data: 2.99e-04). ETA=6 days, 6:07:19, max mem: 15.2 GB 
[06/21 04:08:09][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0014,	1.1875 s / batch. (data: 2.99e-04). ETA=6 days, 6:38:18, max mem: 15.2 GB 
[06/21 04:10:07][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0020,	1.1872 s / batch. (data: 2.95e-04). ETA=6 days, 6:34:01, max mem: 15.2 GB 
[06/21 04:12:05][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0017,	1.1806 s / batch. (data: 2.99e-04). ETA=6 days, 5:41:43, max mem: 15.2 GB 
[06/21 04:14:04][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0014,	1.1791 s / batch. (data: 3.32e-04). ETA=6 days, 5:28:09, max mem: 15.2 GB 
[06/21 04:16:02][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0014,	1.1792 s / batch. (data: 3.05e-04). ETA=6 days, 5:27:16, max mem: 15.2 GB 
[06/21 04:18:01][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0017,	1.1843 s / batch. (data: 3.45e-04). ETA=6 days, 6:03:51, max mem: 15.2 GB 
[06/21 04:19:59][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0017,	1.1823 s / batch. (data: 3.52e-04). ETA=6 days, 5:46:48, max mem: 15.2 GB 
[06/21 04:21:57][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0013,	1.1859 s / batch. (data: 3.83e-04). ETA=6 days, 6:12:23, max mem: 15.2 GB 
[06/21 04:23:56][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0020,	1.1908 s / batch. (data: 3.38e-04). ETA=6 days, 6:47:15, max mem: 15.2 GB 
[06/21 04:25:54][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0019,	1.1768 s / batch. (data: 3.55e-04). ETA=6 days, 4:59:04, max mem: 15.2 GB 
[06/21 04:27:52][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0019,	1.1885 s / batch. (data: 3.43e-04). ETA=6 days, 6:25:55, max mem: 15.2 GB 
[06/21 04:29:51][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0022,	1.1880 s / batch. (data: 3.17e-04). ETA=6 days, 6:20:31, max mem: 15.2 GB 
[06/21 04:31:50][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0015,	1.1831 s / batch. (data: 3.06e-04). ETA=6 days, 5:40:47, max mem: 15.2 GB 
[06/21 04:33:49][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0013,	1.1862 s / batch. (data: 1.20e-04). ETA=6 days, 6:02:28, max mem: 15.2 GB 
[06/21 04:33:55][INFO] visual_prompt:  319: Epoch 9 / 100: avg data time: 4.19e-03, avg batch time: 1.1926, average train loss: 0.0015
[06/21 04:42:49][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.001, 5.1830 s / batch. (data: 1.42e-04)max mem: 15.18835 GB 
[06/21 04:51:05][INFO] visual_prompt:  476: Inference (val):avg data time: 1.57e-04, avg batch time: 5.1630, average loss: 0.0008
[06/21 04:51:05][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_shallow_cls_reinit_10/val_imagenet_invariances.json
[06/21 04:51:05][INFO] visual_prompt:  257: Training 10 / 100 epoch, with learning rate 0.9
[06/21 04:53:47][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0015,	1.1871 s / batch. (data: 3.10e-04). ETA=6 days, 6:07:37, max mem: 15.2 GB 
[06/21 04:55:45][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0015,	1.1781 s / batch. (data: 2.88e-04). ETA=6 days, 4:56:49, max mem: 15.2 GB 
[06/21 04:57:44][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0013,	1.1880 s / batch. (data: 3.10e-04). ETA=6 days, 6:09:54, max mem: 15.2 GB 
[06/21 04:59:42][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0014,	1.1921 s / batch. (data: 2.63e-04). ETA=6 days, 6:39:17, max mem: 15.2 GB 
[06/21 05:01:41][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0014,	1.1765 s / batch. (data: 3.03e-04). ETA=6 days, 4:39:21, max mem: 15.2 GB 
[06/21 05:03:39][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0020,	1.1814 s / batch. (data: 3.01e-04). ETA=6 days, 5:14:27, max mem: 15.2 GB 
[06/21 05:05:38][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0017,	1.1760 s / batch. (data: 3.17e-04). ETA=6 days, 4:31:25, max mem: 15.2 GB 
[06/21 05:07:36][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0018,	1.1822 s / batch. (data: 3.55e-04). ETA=6 days, 5:16:22, max mem: 15.2 GB 
[06/21 05:09:34][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0013,	1.1822 s / batch. (data: 3.11e-04). ETA=6 days, 5:14:27, max mem: 15.2 GB 
[06/21 05:11:33][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0014,	1.1788 s / batch. (data: 3.36e-04). ETA=6 days, 4:46:24, max mem: 15.2 GB 
[06/21 05:13:31][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0020,	1.1790 s / batch. (data: 2.91e-04). ETA=6 days, 4:46:04, max mem: 15.2 GB 
[06/21 05:15:28][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0017,	1.1783 s / batch. (data: 3.23e-04). ETA=6 days, 4:39:13, max mem: 15.2 GB 
[06/21 05:17:27][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0015,	1.1840 s / batch. (data: 3.51e-04). ETA=6 days, 5:20:30, max mem: 15.2 GB 
[06/21 05:19:25][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0015,	1.1807 s / batch. (data: 3.14e-04). ETA=6 days, 4:52:54, max mem: 15.2 GB 
[06/21 05:21:23][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0019,	1.1832 s / batch. (data: 2.95e-04). ETA=6 days, 5:10:31, max mem: 15.2 GB 
[06/21 05:23:21][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0012,	1.1867 s / batch. (data: 3.11e-04). ETA=6 days, 5:34:21, max mem: 15.2 GB 
[06/21 05:25:20][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0018,	1.1877 s / batch. (data: 3.05e-04). ETA=6 days, 5:40:23, max mem: 15.2 GB 
[06/21 05:27:18][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0015,	1.1841 s / batch. (data: 3.05e-04). ETA=6 days, 5:11:16, max mem: 15.2 GB 
[06/21 05:29:17][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0016,	1.1795 s / batch. (data: 2.42e-04). ETA=6 days, 4:34:39, max mem: 15.2 GB 
[06/21 05:31:15][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0015,	1.1839 s / batch. (data: 2.98e-04). ETA=6 days, 5:05:25, max mem: 15.2 GB 
[06/21 05:33:14][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0023,	1.1905 s / batch. (data: 2.95e-04). ETA=6 days, 5:53:08, max mem: 15.2 GB 
[06/21 05:35:13][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0013,	1.1903 s / batch. (data: 4.13e-04). ETA=6 days, 5:50:08, max mem: 15.2 GB 
[06/21 05:37:11][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0014,	1.1822 s / batch. (data: 3.08e-04). ETA=6 days, 4:47:05, max mem: 15.2 GB 
[06/21 05:39:09][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0013,	1.1742 s / batch. (data: 3.00e-04). ETA=6 days, 3:44:29, max mem: 15.2 GB 
[06/21 05:41:07][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0022,	1.1854 s / batch. (data: 3.17e-04). ETA=6 days, 5:07:18, max mem: 15.2 GB 
[06/21 05:43:05][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0014,	1.1788 s / batch. (data: 3.03e-04). ETA=6 days, 4:15:33, max mem: 15.2 GB 
[06/21 05:45:04][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0020,	1.1827 s / batch. (data: 2.58e-04). ETA=6 days, 4:42:35, max mem: 15.2 GB 
[06/21 05:47:02][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0019,	1.1830 s / batch. (data: 3.45e-04). ETA=6 days, 4:43:23, max mem: 15.2 GB 
[06/21 05:49:00][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0013,	1.1905 s / batch. (data: 2.63e-04). ETA=6 days, 5:37:19, max mem: 15.2 GB 
[06/21 05:50:59][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0012,	1.1947 s / batch. (data: 3.22e-04). ETA=6 days, 6:07:26, max mem: 15.2 GB 
[06/21 05:52:57][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0011,	1.1906 s / batch. (data: 4.37e-04). ETA=6 days, 5:34:11, max mem: 15.2 GB 
[06/21 05:54:56][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0018,	1.1894 s / batch. (data: 3.11e-04). ETA=6 days, 5:23:06, max mem: 15.2 GB 
[06/21 05:56:54][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0014,	1.1775 s / batch. (data: 3.25e-04). ETA=6 days, 3:51:40, max mem: 15.2 GB 
[06/21 05:58:52][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0012,	1.1869 s / batch. (data: 3.13e-04). ETA=6 days, 5:00:24, max mem: 15.2 GB 
[06/21 06:00:51][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0014,	1.1772 s / batch. (data: 2.96e-04). ETA=6 days, 3:45:40, max mem: 15.2 GB 
[06/21 06:02:49][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0013,	1.1784 s / batch. (data: 3.79e-04). ETA=6 days, 3:52:23, max mem: 15.2 GB 
[06/21 06:04:47][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0017,	1.1798 s / batch. (data: 3.49e-04). ETA=6 days, 4:01:26, max mem: 15.2 GB 
[06/21 06:06:45][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0013,	1.1834 s / batch. (data: 3.42e-04). ETA=6 days, 4:26:03, max mem: 15.2 GB 
[06/21 06:08:43][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0017,	1.1772 s / batch. (data: 3.40e-04). ETA=6 days, 3:37:30, max mem: 15.2 GB 
[06/21 06:10:42][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0023,	1.1849 s / batch. (data: 3.93e-04). ETA=6 days, 4:33:25, max mem: 15.2 GB 
[06/21 06:12:40][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0013,	1.1838 s / batch. (data: 2.96e-04). ETA=6 days, 4:23:39, max mem: 15.2 GB 
[06/21 06:14:38][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0019,	1.1774 s / batch. (data: 3.15e-04). ETA=6 days, 3:33:03, max mem: 15.2 GB 
[06/21 06:16:37][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0016,	1.1847 s / batch. (data: 4.46e-04). ETA=6 days, 4:26:01, max mem: 15.2 GB 
[06/21 06:18:35][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0019,	1.1796 s / batch. (data: 3.37e-04). ETA=6 days, 3:45:41, max mem: 15.2 GB 
[06/21 06:20:33][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0016,	1.1778 s / batch. (data: 3.14e-04). ETA=6 days, 3:30:42, max mem: 15.2 GB 
[06/21 06:22:32][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0016,	1.1875 s / batch. (data: 5.15e-04). ETA=6 days, 4:41:22, max mem: 15.2 GB 
[06/21 06:24:30][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0016,	1.1819 s / batch. (data: 3.29e-04). ETA=6 days, 3:57:40, max mem: 15.2 GB 
[06/21 06:26:28][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0015,	1.1784 s / batch. (data: 3.17e-04). ETA=6 days, 3:29:07, max mem: 15.2 GB 
[06/21 06:28:26][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0013,	1.1766 s / batch. (data: 3.12e-04). ETA=6 days, 3:13:51, max mem: 15.2 GB 
[06/21 06:30:24][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0016,	1.1738 s / batch. (data: 1.45e-04). ETA=6 days, 2:50:52, max mem: 15.2 GB 
[06/21 06:30:30][INFO] visual_prompt:  319: Epoch 10 / 100: avg data time: 4.23e-03, avg batch time: 1.1918, average train loss: 0.0015
[06/21 06:39:25][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.001, 5.1914 s / batch. (data: 1.52e-04)max mem: 15.18835 GB 
[06/21 06:47:42][INFO] visual_prompt:  476: Inference (val):avg data time: 1.62e-04, avg batch time: 5.1698, average loss: 0.0009
[06/21 06:47:42][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_shallow_cls_reinit_10/val_imagenet_invariances.json
[06/21 06:47:42][INFO] visual_prompt:  257: Training 11 / 100 epoch, with learning rate 1.0
[06/21 06:50:23][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0015,	1.1879 s / batch. (data: 3.18e-04). ETA=6 days, 4:34:41, max mem: 15.2 GB 
[06/21 06:52:22][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0017,	1.1858 s / batch. (data: 2.99e-04). ETA=6 days, 4:16:50, max mem: 15.2 GB 
[06/21 06:54:20][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0017,	1.1914 s / batch. (data: 2.95e-04). ETA=6 days, 4:56:20, max mem: 15.2 GB 
[06/21 06:56:19][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0015,	1.1854 s / batch. (data: 3.32e-04). ETA=6 days, 4:09:38, max mem: 15.2 GB 
[06/21 06:58:17][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0014,	1.1825 s / batch. (data: 3.53e-04). ETA=6 days, 3:46:11, max mem: 15.2 GB 
[06/21 07:00:15][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0013,	1.1778 s / batch. (data: 3.06e-04). ETA=6 days, 3:08:52, max mem: 15.2 GB 
[06/21 07:02:14][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0015,	1.1774 s / batch. (data: 3.29e-04). ETA=6 days, 3:04:02, max mem: 15.2 GB 
[06/21 07:04:12][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0018,	1.1843 s / batch. (data: 3.77e-04). ETA=6 days, 3:53:44, max mem: 15.2 GB 
[06/21 07:06:10][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0014,	1.1779 s / batch. (data: 5.07e-04). ETA=6 days, 3:03:38, max mem: 15.2 GB 
[06/21 07:08:09][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0015,	1.1852 s / batch. (data: 3.54e-04). ETA=6 days, 3:56:00, max mem: 15.2 GB 
[06/21 07:10:07][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0041,	1.1887 s / batch. (data: 3.39e-04). ETA=6 days, 4:20:35, max mem: 15.2 GB 
[06/21 07:12:06][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0015,	1.1870 s / batch. (data: 3.35e-04). ETA=6 days, 4:05:40, max mem: 15.2 GB 
[06/21 07:14:04][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0020,	1.1818 s / batch. (data: 3.07e-04). ETA=6 days, 3:24:47, max mem: 15.2 GB 
[06/21 07:16:02][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0015,	1.1760 s / batch. (data: 3.10e-04). ETA=6 days, 2:39:51, max mem: 15.2 GB 
[06/21 07:18:00][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0014,	1.1852 s / batch. (data: 3.53e-04). ETA=6 days, 3:46:07, max mem: 15.2 GB 
[06/21 07:19:58][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0014,	1.1828 s / batch. (data: 4.25e-04). ETA=6 days, 3:26:20, max mem: 15.2 GB 
[06/21 07:21:57][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0013,	1.1868 s / batch. (data: 3.85e-04). ETA=6 days, 3:54:35, max mem: 15.2 GB 
[06/21 07:23:55][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0020,	1.1864 s / batch. (data: 3.02e-04). ETA=6 days, 3:49:43, max mem: 15.2 GB 
[06/21 07:25:53][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0023,	1.1828 s / batch. (data: 3.76e-04). ETA=6 days, 3:20:54, max mem: 15.2 GB 
[06/21 07:27:52][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0014,	1.1858 s / batch. (data: 3.39e-04). ETA=6 days, 3:41:05, max mem: 15.2 GB 
[06/21 07:29:50][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0015,	1.1893 s / batch. (data: 3.27e-04). ETA=6 days, 4:05:05, max mem: 15.2 GB 
[06/21 07:31:48][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0014,	1.1864 s / batch. (data: 3.45e-04). ETA=6 days, 3:41:18, max mem: 15.2 GB 
[06/21 07:33:47][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0013,	1.1872 s / batch. (data: 2.96e-04). ETA=6 days, 3:45:42, max mem: 15.2 GB 
[06/21 07:35:45][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0014,	1.1809 s / batch. (data: 3.14e-04). ETA=6 days, 2:56:56, max mem: 15.2 GB 
[06/21 07:37:44][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0016,	1.1816 s / batch. (data: 2.86e-04). ETA=6 days, 2:59:43, max mem: 15.2 GB 
[06/21 07:39:42][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0017,	1.1775 s / batch. (data: 3.72e-04). ETA=6 days, 2:27:14, max mem: 15.2 GB 
[06/21 07:41:41][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0013,	1.1891 s / batch. (data: 3.21e-04). ETA=6 days, 3:51:58, max mem: 15.2 GB 
[06/21 07:43:39][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0013,	1.1842 s / batch. (data: 3.41e-04). ETA=6 days, 3:13:20, max mem: 15.2 GB 
[06/21 07:45:38][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0017,	1.1902 s / batch. (data: 3.46e-04). ETA=6 days, 3:56:15, max mem: 15.2 GB 
[06/21 07:47:36][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0015,	1.1878 s / batch. (data: 3.71e-04). ETA=6 days, 3:36:14, max mem: 15.2 GB 
[06/21 07:49:35][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0025,	1.1853 s / batch. (data: 2.78e-04). ETA=6 days, 3:15:46, max mem: 15.2 GB 
[06/21 07:51:33][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0011,	1.1833 s / batch. (data: 2.97e-04). ETA=6 days, 2:58:52, max mem: 15.2 GB 
[06/21 07:53:31][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0012,	1.1788 s / batch. (data: 3.31e-04). ETA=6 days, 2:23:21, max mem: 15.2 GB 
[06/21 07:55:30][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0019,	1.1814 s / batch. (data: 2.73e-04). ETA=6 days, 2:40:38, max mem: 15.2 GB 
[06/21 07:57:28][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0015,	1.1859 s / batch. (data: 3.04e-04). ETA=6 days, 3:12:30, max mem: 15.2 GB 
[06/21 07:59:26][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0014,	1.1853 s / batch. (data: 3.60e-04). ETA=6 days, 3:05:32, max mem: 15.2 GB 
[06/21 08:01:25][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0013,	1.1841 s / batch. (data: 3.00e-04). ETA=6 days, 2:54:46, max mem: 15.2 GB 
[06/21 08:03:23][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0013,	1.1859 s / batch. (data: 3.37e-04). ETA=6 days, 3:06:34, max mem: 15.2 GB 
[06/21 08:05:21][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0013,	1.1772 s / batch. (data: 3.21e-04). ETA=6 days, 1:59:18, max mem: 15.2 GB 
[06/21 08:07:19][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0022,	1.1902 s / batch. (data: 3.83e-04). ETA=6 days, 3:33:59, max mem: 15.2 GB 
[06/21 08:09:17][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0013,	1.1819 s / batch. (data: 3.24e-04). ETA=6 days, 2:30:53, max mem: 15.2 GB 
[06/21 08:11:16][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0017,	1.1836 s / batch. (data: 3.30e-04). ETA=6 days, 2:41:00, max mem: 15.2 GB 
[06/21 08:13:14][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0020,	1.1843 s / batch. (data: 4.99e-04). ETA=6 days, 2:44:07, max mem: 15.2 GB 
[06/21 08:15:12][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0011,	1.1841 s / batch. (data: 3.15e-04). ETA=6 days, 2:41:13, max mem: 15.2 GB 
[06/21 08:17:11][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0015,	1.1791 s / batch. (data: 3.11e-04). ETA=6 days, 2:02:10, max mem: 15.2 GB 
[06/21 08:19:09][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0018,	1.1849 s / batch. (data: 3.70e-04). ETA=6 days, 2:42:52, max mem: 15.2 GB 
[06/21 08:21:07][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0014,	1.1804 s / batch. (data: 3.29e-04). ETA=6 days, 2:07:45, max mem: 15.2 GB 
[06/21 08:23:05][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0012,	1.1801 s / batch. (data: 3.30e-04). ETA=6 days, 2:03:37, max mem: 15.2 GB 
[06/21 08:25:04][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0017,	1.1805 s / batch. (data: 3.15e-04). ETA=6 days, 2:04:33, max mem: 15.2 GB 
[06/21 08:27:02][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0014,	1.1823 s / batch. (data: 1.08e-04). ETA=6 days, 2:15:37, max mem: 15.2 GB 
[06/21 08:27:08][INFO] visual_prompt:  319: Epoch 11 / 100: avg data time: 4.24e-03, avg batch time: 1.1920, average train loss: 0.0016
[06/21 08:36:02][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.001, 5.1895 s / batch. (data: 1.63e-04)max mem: 15.18835 GB 
[06/21 08:44:19][INFO] visual_prompt:  476: Inference (val):avg data time: 1.48e-04, avg batch time: 5.1635, average loss: 0.0009
[06/21 08:44:19][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_shallow_cls_reinit_10/val_imagenet_invariances.json
[06/21 08:44:19][INFO] visual_prompt:  257: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[06/21 08:47:02][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0021,	1.1968 s / batch. (data: 3.41e-04). ETA=6 days, 4:01:34, max mem: 15.2 GB 
[06/21 08:49:02][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0017,	1.1930 s / batch. (data: 3.28e-04). ETA=6 days, 3:31:14, max mem: 15.2 GB 
[06/21 08:51:01][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0019,	1.1882 s / batch. (data: 3.35e-04). ETA=6 days, 2:53:17, max mem: 15.2 GB 
[06/21 08:52:59][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0013,	1.1857 s / batch. (data: 3.22e-04). ETA=6 days, 2:33:12, max mem: 15.2 GB 
[06/21 08:54:58][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0014,	1.1829 s / batch. (data: 4.39e-04). ETA=6 days, 2:10:06, max mem: 15.2 GB 
[06/21 08:56:56][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0011,	1.1879 s / batch. (data: 3.46e-04). ETA=6 days, 2:45:25, max mem: 15.2 GB 
[06/21 08:58:55][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0018,	1.1849 s / batch. (data: 3.40e-04). ETA=6 days, 2:21:20, max mem: 15.2 GB 
[06/21 09:00:53][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0020,	1.1867 s / batch. (data: 3.14e-04). ETA=6 days, 2:32:39, max mem: 15.2 GB 
[06/21 09:02:51][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0015,	1.1824 s / batch. (data: 2.84e-04). ETA=6 days, 1:58:29, max mem: 15.2 GB 
[06/21 09:04:50][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0013,	1.1881 s / batch. (data: 2.99e-04). ETA=6 days, 2:38:53, max mem: 15.2 GB 
[06/21 09:06:48][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0013,	1.1838 s / batch. (data: 3.42e-04). ETA=6 days, 2:05:01, max mem: 15.2 GB 
[06/21 09:08:46][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0014,	1.1813 s / batch. (data: 2.83e-04). ETA=6 days, 1:44:34, max mem: 15.2 GB 
[06/21 09:10:45][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0013,	1.1878 s / batch. (data: 3.46e-04). ETA=6 days, 2:30:31, max mem: 15.2 GB 
[06/21 09:12:43][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0018,	1.1822 s / batch. (data: 3.31e-04). ETA=6 days, 1:47:27, max mem: 15.2 GB 
[06/21 09:14:41][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0014,	1.1883 s / batch. (data: 3.58e-04). ETA=6 days, 2:30:52, max mem: 15.2 GB 
[06/21 09:16:40][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0014,	1.1864 s / batch. (data: 3.31e-04). ETA=6 days, 2:14:36, max mem: 15.2 GB 
[06/21 09:18:38][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0015,	1.1797 s / batch. (data: 3.36e-04). ETA=6 days, 1:22:49, max mem: 15.2 GB 
[06/21 09:20:37][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0013,	1.1811 s / batch. (data: 3.30e-04). ETA=6 days, 1:31:16, max mem: 15.2 GB 
[06/21 09:22:35][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0014,	1.1858 s / batch. (data: 3.35e-04). ETA=6 days, 2:04:12, max mem: 15.2 GB 
[06/21 09:24:34][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0014,	1.1857 s / batch. (data: 3.89e-04). ETA=6 days, 2:01:06, max mem: 15.2 GB 
[06/21 09:26:32][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0011,	1.1852 s / batch. (data: 3.86e-04). ETA=6 days, 1:55:50, max mem: 15.2 GB 
[06/21 09:28:31][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0014,	1.1841 s / batch. (data: 3.27e-04). ETA=6 days, 1:45:42, max mem: 15.2 GB 
[06/21 09:30:29][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0014,	1.1791 s / batch. (data: 3.10e-04). ETA=6 days, 1:06:44, max mem: 15.2 GB 
[06/21 09:32:27][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0020,	1.1789 s / batch. (data: 3.05e-04). ETA=6 days, 1:03:09, max mem: 15.2 GB 
[06/21 09:34:25][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0014,	1.1860 s / batch. (data: 2.89e-04). ETA=6 days, 1:54:05, max mem: 15.2 GB 
[06/21 09:36:23][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0017,	1.1777 s / batch. (data: 3.28e-04). ETA=6 days, 0:50:17, max mem: 15.2 GB 
[06/21 09:38:21][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0014,	1.1867 s / batch. (data: 4.71e-04). ETA=6 days, 1:54:47, max mem: 15.2 GB 
[06/21 09:40:19][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0015,	1.1821 s / batch. (data: 4.00e-04). ETA=6 days, 1:18:50, max mem: 15.2 GB 
[06/21 09:42:17][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0015,	1.1789 s / batch. (data: 3.12e-04). ETA=6 days, 0:53:50, max mem: 15.2 GB 
[06/21 09:44:15][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0028,	1.1803 s / batch. (data: 3.07e-04). ETA=6 days, 1:02:04, max mem: 15.2 GB 
[06/21 09:46:13][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0021,	1.1840 s / batch. (data: 3.04e-04). ETA=6 days, 1:27:04, max mem: 15.2 GB 
[06/21 09:48:11][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0015,	1.1784 s / batch. (data: 2.94e-04). ETA=6 days, 0:43:38, max mem: 15.2 GB 
[06/21 09:50:09][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0022,	1.1781 s / batch. (data: 3.32e-04). ETA=6 days, 0:39:27, max mem: 15.2 GB 
[06/21 09:52:07][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0016,	1.1796 s / batch. (data: 3.07e-04). ETA=6 days, 0:48:57, max mem: 15.2 GB 
[06/21 09:54:05][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0019,	1.1868 s / batch. (data: 3.09e-04). ETA=6 days, 1:39:39, max mem: 15.2 GB 
[06/21 09:56:03][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0014,	1.1746 s / batch. (data: 3.10e-04). ETA=6 days, 0:08:14, max mem: 15.2 GB 
[06/21 09:58:01][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0019,	1.1812 s / batch. (data: 3.10e-04). ETA=6 days, 0:54:51, max mem: 15.2 GB 
[06/21 09:59:59][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0013,	1.1788 s / batch. (data: 2.97e-04). ETA=6 days, 0:35:00, max mem: 15.2 GB 
[06/21 10:01:57][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0013,	1.1782 s / batch. (data: 2.99e-04). ETA=6 days, 0:28:47, max mem: 15.2 GB 
[06/21 10:03:54][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0012,	1.1781 s / batch. (data: 3.11e-04). ETA=6 days, 0:25:41, max mem: 15.2 GB 
[06/21 10:05:52][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0017,	1.1774 s / batch. (data: 3.00e-04). ETA=6 days, 0:19:02, max mem: 15.2 GB 
[06/21 10:07:50][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0012,	1.1760 s / batch. (data: 3.20e-04). ETA=6 days, 0:06:52, max mem: 15.2 GB 
[06/21 10:09:48][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0017,	1.1792 s / batch. (data: 3.25e-04). ETA=6 days, 0:28:20, max mem: 15.2 GB 
[06/21 10:11:46][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0027,	1.1771 s / batch. (data: 2.87e-04). ETA=6 days, 0:10:31, max mem: 15.2 GB 
[06/21 10:13:44][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0014,	1.1752 s / batch. (data: 3.00e-04). ETA=5 days, 23:55:06, max mem: 15.2 GB 
[06/21 10:15:42][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0015,	1.1751 s / batch. (data: 3.23e-04). ETA=5 days, 23:52:30, max mem: 15.2 GB 
[06/21 10:17:39][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0018,	1.1777 s / batch. (data: 3.12e-04). ETA=6 days, 0:09:03, max mem: 15.2 GB 
[06/21 10:19:37][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0012,	1.1792 s / batch. (data: 3.13e-04). ETA=6 days, 0:18:18, max mem: 15.2 GB 
[06/21 10:21:35][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0016,	1.1782 s / batch. (data: 3.31e-04). ETA=6 days, 0:09:18, max mem: 15.2 GB 
[06/21 10:23:33][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0017,	1.1767 s / batch. (data: 1.08e-04). ETA=5 days, 23:56:25, max mem: 15.2 GB 
[06/21 10:23:39][INFO] visual_prompt:  319: Epoch 12 / 100: avg data time: 4.22e-03, avg batch time: 1.1908, average train loss: 0.0015
[06/21 10:32:32][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.001, 5.1775 s / batch. (data: 1.61e-04)max mem: 15.18835 GB 
[06/21 10:40:48][INFO] visual_prompt:  476: Inference (val):avg data time: 1.56e-04, avg batch time: 5.1618, average loss: 0.0009
[06/21 10:40:48][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_shallow_cls_reinit_10/val_imagenet_invariances.json
[06/21 10:40:48][INFO] visual_prompt:  257: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[06/21 10:43:30][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0018,	1.1791 s / batch. (data: 3.59e-04). ETA=6 days, 0:11:48, max mem: 15.2 GB 
[06/21 10:45:28][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0019,	1.1815 s / batch. (data: 2.98e-04). ETA=6 days, 0:27:28, max mem: 15.2 GB 
[06/21 10:47:26][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0012,	1.1763 s / batch. (data: 4.01e-04). ETA=5 days, 23:46:53, max mem: 15.2 GB 
[06/21 10:49:24][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0020,	1.1801 s / batch. (data: 3.12e-04). ETA=6 days, 0:12:58, max mem: 15.2 GB 
[06/21 10:51:22][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0017,	1.1841 s / batch. (data: 3.55e-04). ETA=6 days, 0:40:29, max mem: 15.2 GB 
[06/21 10:53:20][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0012,	1.1827 s / batch. (data: 3.75e-04). ETA=6 days, 0:28:05, max mem: 15.2 GB 
[06/21 10:55:18][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0016,	1.1759 s / batch. (data: 3.09e-04). ETA=5 days, 23:36:35, max mem: 15.2 GB 
[06/21 10:57:16][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0011,	1.1840 s / batch. (data: 2.95e-04). ETA=6 days, 0:33:42, max mem: 15.2 GB 
[06/21 10:59:14][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0014,	1.1839 s / batch. (data: 3.45e-04). ETA=6 days, 0:30:58, max mem: 15.2 GB 
[06/21 11:01:12][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0012,	1.1771 s / batch. (data: 4.96e-04). ETA=5 days, 23:39:10, max mem: 15.2 GB 
[06/21 11:03:10][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0026,	1.1780 s / batch. (data: 2.96e-04). ETA=5 days, 23:43:37, max mem: 15.2 GB 
[06/21 11:05:08][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0012,	1.1852 s / batch. (data: 3.25e-04). ETA=6 days, 0:34:52, max mem: 15.2 GB 
[06/21 11:07:05][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0011,	1.1794 s / batch. (data: 2.93e-04). ETA=5 days, 23:50:18, max mem: 15.2 GB 
[06/21 11:09:03][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0018,	1.1809 s / batch. (data: 3.34e-04). ETA=5 days, 23:59:14, max mem: 15.2 GB 
[06/21 11:11:01][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0015,	1.1785 s / batch. (data: 3.07e-04). ETA=5 days, 23:39:29, max mem: 15.2 GB 
[06/21 11:12:59][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0015,	1.1786 s / batch. (data: 3.84e-04). ETA=5 days, 23:38:36, max mem: 15.2 GB 
[06/21 11:14:57][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0013,	1.1797 s / batch. (data: 3.24e-04). ETA=5 days, 23:44:56, max mem: 15.2 GB 
[06/21 11:16:55][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0014,	1.1833 s / batch. (data: 2.94e-04). ETA=6 days, 0:09:13, max mem: 15.2 GB 
[06/21 11:18:53][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0013,	1.1785 s / batch. (data: 2.81e-04). ETA=5 days, 23:31:57, max mem: 15.2 GB 
[06/21 11:20:51][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0016,	1.1800 s / batch. (data: 2.77e-04). ETA=5 days, 23:40:56, max mem: 15.2 GB 
[06/21 11:22:49][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0013,	1.1800 s / batch. (data: 3.54e-04). ETA=5 days, 23:38:57, max mem: 15.2 GB 
[06/21 11:24:47][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0017,	1.1782 s / batch. (data: 3.64e-04). ETA=5 days, 23:24:05, max mem: 15.2 GB 
[06/21 11:26:45][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0014,	1.1763 s / batch. (data: 3.18e-04). ETA=5 days, 23:08:21, max mem: 15.2 GB 
[06/21 11:28:43][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0016,	1.1794 s / batch. (data: 3.25e-04). ETA=5 days, 23:28:29, max mem: 15.2 GB 
[06/21 11:30:42][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0013,	1.1797 s / batch. (data: 3.87e-04). ETA=5 days, 23:29:15, max mem: 15.2 GB 
[06/21 11:32:40][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0019,	1.1844 s / batch. (data: 3.88e-04). ETA=6 days, 0:01:16, max mem: 15.2 GB 
[06/21 11:34:38][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0014,	1.1760 s / batch. (data: 3.19e-04). ETA=5 days, 22:58:07, max mem: 15.2 GB 
[06/21 11:36:36][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0014,	1.1806 s / batch. (data: 2.78e-04). ETA=5 days, 23:29:25, max mem: 15.2 GB 
[06/21 11:38:33][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0021,	1.1841 s / batch. (data: 2.76e-04). ETA=5 days, 23:53:18, max mem: 15.2 GB 
[06/21 11:40:31][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0016,	1.1842 s / batch. (data: 3.91e-04). ETA=5 days, 23:52:12, max mem: 15.2 GB 
[06/21 11:42:30][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0013,	1.1819 s / batch. (data: 4.40e-04). ETA=5 days, 23:33:13, max mem: 15.2 GB 
