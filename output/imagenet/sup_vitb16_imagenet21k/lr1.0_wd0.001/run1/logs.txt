[06/10 23:44:46][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/10 23:44:46][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,5,6,7
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/10 23:44:46][INFO] visual_prompt:   99: Command line arguments: None
[06/10 23:44:46][INFO] visual_prompt:  108: Training with config:
[06/10 23:44:46][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': False,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 31,
                      'NUM_TOKENS': 1550,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 1.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'mse',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/10 23:44:50][INFO] visual_prompt:   56: Total Parameters: 87758056	 Gradient Parameters: 1959400
[06/10 23:44:50][INFO] visual_prompt:   58: tuned percent:2.233
[06/10 23:44:50][INFO] visual_prompt:   44: Device used for model: 0
[06/10 23:44:50][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/10 23:44:50][INFO] visual_prompt:   49: Constructing imagenet dataset train...
[06/10 23:44:53][INFO] visual_prompt:  110: Number of images: 1281167
[06/10 23:44:53][INFO] visual_prompt:  111: Number of classes: 1000
[06/10 23:44:53][INFO] visual_prompt:   78: Loading validation data...
[06/10 23:44:53][INFO] visual_prompt:   49: Constructing imagenet dataset val...
[06/10 23:44:53][INFO] visual_prompt:  110: Number of images: 50000
[06/10 23:44:53][INFO] visual_prompt:  111: Number of classes: 1000
[06/10 23:44:53][INFO] visual_prompt:   81: Loading test data...
[06/10 23:44:53][INFO] visual_prompt:   83: ...no test data is constructed
[06/10 23:44:53][INFO] visual_prompt:  111: Constructing models...
[06/10 23:44:53][INFO] visual_prompt:  114: Setting up Evalutator...
[06/10 23:44:53][INFO] visual_prompt:  116: Setting up Trainer...
[06/10 23:44:53][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/10 23:44:53][INFO] visual_prompt:   59: module.head.last_layer.weight: True
[06/10 23:44:53][INFO] visual_prompt:   59: module.head.last_layer.bias: True
[06/10 23:44:53][INFO] visual_prompt:  200: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/10 23:44:53][INFO] visual_prompt:  220: Training 1 / 100 epoch, with learning rate 0.0
[06/10 23:47:25][INFO] visual_prompt:  264: 	Training 100/5004. train loss: 0.1266,	1.2952 s / batch. (data: 2.66e-04). ETA=7 days, 12:00:13, max mem: 14.9 GB 
[06/10 23:49:34][INFO] visual_prompt:  264: 	Training 200/5004. train loss: 0.1183,	1.2885 s / batch. (data: 3.97e-04). ETA=7 days, 11:01:50, max mem: 14.9 GB 
[06/10 23:51:43][INFO] visual_prompt:  264: 	Training 300/5004. train loss: 0.1167,	1.2924 s / batch. (data: 3.10e-04). ETA=7 days, 11:32:06, max mem: 14.9 GB 
[06/10 23:53:52][INFO] visual_prompt:  264: 	Training 400/5004. train loss: 0.1169,	1.2897 s / batch. (data: 5.00e-04). ETA=7 days, 11:07:24, max mem: 14.9 GB 
[06/10 23:56:02][INFO] visual_prompt:  264: 	Training 500/5004. train loss: 0.1175,	1.2921 s / batch. (data: 4.69e-04). ETA=7 days, 11:25:28, max mem: 14.9 GB 
[06/10 23:58:11][INFO] visual_prompt:  264: 	Training 600/5004. train loss: 0.1096,	1.2921 s / batch. (data: 3.60e-04). ETA=7 days, 11:22:49, max mem: 14.9 GB 
[06/11 00:00:20][INFO] visual_prompt:  264: 	Training 700/5004. train loss: 0.1064,	1.2880 s / batch. (data: 3.42e-04). ETA=7 days, 10:47:01, max mem: 14.9 GB 
[06/11 00:02:29][INFO] visual_prompt:  264: 	Training 800/5004. train loss: 0.1072,	1.2879 s / batch. (data: 3.50e-04). ETA=7 days, 10:44:01, max mem: 14.9 GB 
[06/11 00:04:38][INFO] visual_prompt:  264: 	Training 900/5004. train loss: 0.1096,	1.2903 s / batch. (data: 3.12e-04). ETA=7 days, 11:01:57, max mem: 14.9 GB 
[06/11 00:06:47][INFO] visual_prompt:  264: 	Training 1000/5004. train loss: 0.1091,	1.2930 s / batch. (data: 3.33e-04). ETA=7 days, 11:22:00, max mem: 14.9 GB 
[06/11 00:08:56][INFO] visual_prompt:  264: 	Training 1100/5004. train loss: 0.1156,	1.2881 s / batch. (data: 3.27e-04). ETA=7 days, 10:39:19, max mem: 14.9 GB 
[06/11 00:11:05][INFO] visual_prompt:  264: 	Training 1200/5004. train loss: 0.1117,	1.2898 s / batch. (data: 3.39e-04). ETA=7 days, 10:51:04, max mem: 14.9 GB 
[06/11 00:13:14][INFO] visual_prompt:  264: 	Training 1300/5004. train loss: 0.1114,	1.2885 s / batch. (data: 3.24e-04). ETA=7 days, 10:38:28, max mem: 14.9 GB 
[06/11 00:15:24][INFO] visual_prompt:  264: 	Training 1400/5004. train loss: 0.1095,	1.2947 s / batch. (data: 5.76e-04). ETA=7 days, 11:27:45, max mem: 14.9 GB 
[06/11 00:17:33][INFO] visual_prompt:  264: 	Training 1500/5004. train loss: 0.1138,	1.2901 s / batch. (data: 3.17e-04). ETA=7 days, 10:47:07, max mem: 14.9 GB 
[06/11 00:19:42][INFO] visual_prompt:  264: 	Training 1600/5004. train loss: 0.1191,	1.2900 s / batch. (data: 4.03e-04). ETA=7 days, 10:43:55, max mem: 14.9 GB 
[06/11 00:21:51][INFO] visual_prompt:  264: 	Training 1700/5004. train loss: 0.1223,	1.2972 s / batch. (data: 3.80e-04). ETA=7 days, 11:41:37, max mem: 14.9 GB 
[06/11 00:24:00][INFO] visual_prompt:  264: 	Training 1800/5004. train loss: 0.1143,	1.2952 s / batch. (data: 4.96e-04). ETA=7 days, 11:22:44, max mem: 14.9 GB 
[06/11 00:26:10][INFO] visual_prompt:  264: 	Training 1900/5004. train loss: 0.1135,	1.2931 s / batch. (data: 4.21e-04). ETA=7 days, 11:03:10, max mem: 14.9 GB 
[06/11 00:28:19][INFO] visual_prompt:  264: 	Training 2000/5004. train loss: 0.1045,	1.2923 s / batch. (data: 3.45e-04). ETA=7 days, 10:54:41, max mem: 14.9 GB 
[06/11 00:30:29][INFO] visual_prompt:  264: 	Training 2100/5004. train loss: 0.1130,	1.2946 s / batch. (data: 3.33e-04). ETA=7 days, 11:11:53, max mem: 14.9 GB 
[06/11 00:32:38][INFO] visual_prompt:  264: 	Training 2200/5004. train loss: 0.1129,	1.2966 s / batch. (data: 4.34e-04). ETA=7 days, 11:26:00, max mem: 14.9 GB 
[06/11 00:34:47][INFO] visual_prompt:  264: 	Training 2300/5004. train loss: 0.1102,	1.2922 s / batch. (data: 3.30e-04). ETA=7 days, 10:47:36, max mem: 14.9 GB 
[06/11 00:36:56][INFO] visual_prompt:  264: 	Training 2400/5004. train loss: 0.1144,	1.2902 s / batch. (data: 3.50e-04). ETA=7 days, 10:28:50, max mem: 14.9 GB 
[06/11 00:39:05][INFO] visual_prompt:  264: 	Training 2500/5004. train loss: 0.1185,	1.2926 s / batch. (data: 3.61e-04). ETA=7 days, 10:46:44, max mem: 14.9 GB 
[06/11 00:41:14][INFO] visual_prompt:  264: 	Training 2600/5004. train loss: 0.1108,	1.2909 s / batch. (data: 3.09e-04). ETA=7 days, 10:30:23, max mem: 14.9 GB 
[06/11 00:43:23][INFO] visual_prompt:  264: 	Training 2700/5004. train loss: 0.1140,	1.2940 s / batch. (data: 2.80e-04). ETA=7 days, 10:53:57, max mem: 14.9 GB 
[06/11 00:45:32][INFO] visual_prompt:  264: 	Training 2800/5004. train loss: 0.1133,	1.2909 s / batch. (data: 3.24e-04). ETA=7 days, 10:25:36, max mem: 14.9 GB 
[06/11 00:47:41][INFO] visual_prompt:  264: 	Training 2900/5004. train loss: 0.1170,	1.2879 s / batch. (data: 3.45e-04). ETA=7 days, 9:59:11, max mem: 14.9 GB 
[06/11 00:49:50][INFO] visual_prompt:  264: 	Training 3000/5004. train loss: 0.1077,	1.2887 s / batch. (data: 3.26e-04). ETA=7 days, 10:03:41, max mem: 14.9 GB 
[06/11 00:52:00][INFO] visual_prompt:  264: 	Training 3100/5004. train loss: 0.1044,	1.2967 s / batch. (data: 3.48e-04). ETA=7 days, 11:07:42, max mem: 14.9 GB 
[06/11 00:54:09][INFO] visual_prompt:  264: 	Training 3200/5004. train loss: 0.1141,	1.2956 s / batch. (data: 4.98e-04). ETA=7 days, 10:55:51, max mem: 14.9 GB 
[06/11 00:56:18][INFO] visual_prompt:  264: 	Training 3300/5004. train loss: 0.1103,	1.2909 s / batch. (data: 3.11e-04). ETA=7 days, 10:14:49, max mem: 14.9 GB 
[06/11 00:58:27][INFO] visual_prompt:  264: 	Training 3400/5004. train loss: 0.1096,	1.2929 s / batch. (data: 3.55e-04). ETA=7 days, 10:29:52, max mem: 14.9 GB 
[06/11 01:00:37][INFO] visual_prompt:  264: 	Training 3500/5004. train loss: 0.1110,	1.2967 s / batch. (data: 2.88e-04). ETA=7 days, 10:58:54, max mem: 14.9 GB 
[06/11 01:02:46][INFO] visual_prompt:  264: 	Training 3600/5004. train loss: 0.1153,	1.2898 s / batch. (data: 4.11e-04). ETA=7 days, 9:59:37, max mem: 14.9 GB 
[06/11 01:04:55][INFO] visual_prompt:  264: 	Training 3700/5004. train loss: 0.1065,	1.2935 s / batch. (data: 3.49e-04). ETA=7 days, 10:27:37, max mem: 14.9 GB 
[06/11 01:07:04][INFO] visual_prompt:  264: 	Training 3800/5004. train loss: 0.1175,	1.2926 s / batch. (data: 3.18e-04). ETA=7 days, 10:18:36, max mem: 14.9 GB 
[06/11 01:09:14][INFO] visual_prompt:  264: 	Training 3900/5004. train loss: 0.1176,	1.2925 s / batch. (data: 3.86e-04). ETA=7 days, 10:15:42, max mem: 14.9 GB 
[06/11 01:11:23][INFO] visual_prompt:  264: 	Training 4000/5004. train loss: 0.1110,	1.2891 s / batch. (data: 3.55e-04). ETA=7 days, 9:45:17, max mem: 14.9 GB 
[06/11 01:13:32][INFO] visual_prompt:  264: 	Training 4100/5004. train loss: 0.1112,	1.2868 s / batch. (data: 3.32e-04). ETA=7 days, 9:24:04, max mem: 14.9 GB 
[06/11 01:15:41][INFO] visual_prompt:  264: 	Training 4200/5004. train loss: 0.1181,	1.2906 s / batch. (data: 3.50e-04). ETA=7 days, 9:52:53, max mem: 14.9 GB 
[06/11 01:17:50][INFO] visual_prompt:  264: 	Training 4300/5004. train loss: 0.1074,	1.2879 s / batch. (data: 3.73e-04). ETA=7 days, 9:28:52, max mem: 14.9 GB 
[06/11 01:19:59][INFO] visual_prompt:  264: 	Training 4400/5004. train loss: 0.1138,	1.2933 s / batch. (data: 3.59e-04). ETA=7 days, 10:10:57, max mem: 14.9 GB 
[06/11 01:22:08][INFO] visual_prompt:  264: 	Training 4500/5004. train loss: 0.1025,	1.2912 s / batch. (data: 3.85e-04). ETA=7 days, 9:51:53, max mem: 14.9 GB 
[06/11 01:24:17][INFO] visual_prompt:  264: 	Training 4600/5004. train loss: 0.1140,	1.2883 s / batch. (data: 3.44e-04). ETA=7 days, 9:25:32, max mem: 14.9 GB 
[06/11 01:26:26][INFO] visual_prompt:  264: 	Training 4700/5004. train loss: 0.1148,	1.2874 s / batch. (data: 2.64e-04). ETA=7 days, 9:15:40, max mem: 14.9 GB 
[06/11 01:28:35][INFO] visual_prompt:  264: 	Training 4800/5004. train loss: 0.1168,	1.2916 s / batch. (data: 3.35e-04). ETA=7 days, 9:48:33, max mem: 14.9 GB 
[06/11 01:30:44][INFO] visual_prompt:  264: 	Training 4900/5004. train loss: 0.1060,	1.2923 s / batch. (data: 2.91e-04). ETA=7 days, 9:52:23, max mem: 14.9 GB 
[06/11 01:32:54][INFO] visual_prompt:  264: 	Training 5000/5004. train loss: 0.1216,	1.2900 s / batch. (data: 1.19e-04). ETA=7 days, 9:30:49, max mem: 14.9 GB 
[06/11 01:33:00][INFO] visual_prompt:  277: Epoch 1 / 100: avg data time: 4.32e-03, avg batch time: 1.2961, average train loss: 0.1135
[06/11 01:59:57][INFO] visual_prompt:  397: 	Test 100/196. loss: 0.064, 15.6917 s / batch. (data: 1.13e-04)max mem: 14.90891 GB 
[06/11 02:25:01][INFO] visual_prompt:  434: Inference (val):avg data time: 1.90e-04, avg batch time: 15.7096, average loss: 0.0669
[06/11 02:25:01][INFO] visual_prompt:  451: Saved invariances for val_imagenet at output/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/11 02:25:01][INFO] visual_prompt:  220: Training 2 / 100 epoch, with learning rate 0.1
[06/11 02:28:02][INFO] visual_prompt:  264: 	Training 100/5004. train loss: 0.1239,	1.2915 s / batch. (data: 3.12e-04). ETA=7 days, 9:41:15, max mem: 14.9 GB 
[06/11 02:30:11][INFO] visual_prompt:  264: 	Training 200/5004. train loss: 0.1167,	1.2909 s / batch. (data: 4.84e-04). ETA=7 days, 9:34:05, max mem: 14.9 GB 
[06/11 02:32:20][INFO] visual_prompt:  264: 	Training 300/5004. train loss: 0.1027,	1.2889 s / batch. (data: 3.10e-04). ETA=7 days, 9:15:21, max mem: 14.9 GB 
[06/11 02:34:29][INFO] visual_prompt:  264: 	Training 400/5004. train loss: 0.1056,	1.2902 s / batch. (data: 3.26e-04). ETA=7 days, 9:23:48, max mem: 14.9 GB 
[06/11 02:36:38][INFO] visual_prompt:  264: 	Training 500/5004. train loss: 0.1112,	1.2908 s / batch. (data: 3.67e-04). ETA=7 days, 9:26:52, max mem: 14.9 GB 
[06/11 02:38:47][INFO] visual_prompt:  264: 	Training 600/5004. train loss: 0.1116,	1.2870 s / batch. (data: 4.03e-04). ETA=7 days, 8:53:38, max mem: 14.9 GB 
[06/11 02:40:56][INFO] visual_prompt:  264: 	Training 700/5004. train loss: 0.1093,	1.2929 s / batch. (data: 3.44e-04). ETA=7 days, 9:39:35, max mem: 14.9 GB 
[06/11 02:43:06][INFO] visual_prompt:  264: 	Training 800/5004. train loss: 0.1109,	1.2945 s / batch. (data: 3.22e-04). ETA=7 days, 9:51:01, max mem: 14.9 GB 
[06/11 02:45:15][INFO] visual_prompt:  264: 	Training 900/5004. train loss: 0.1134,	1.2920 s / batch. (data: 4.52e-04). ETA=7 days, 9:27:55, max mem: 14.9 GB 
[06/11 02:47:24][INFO] visual_prompt:  264: 	Training 1000/5004. train loss: 0.1050,	1.2937 s / batch. (data: 3.89e-04). ETA=7 days, 9:39:47, max mem: 14.9 GB 
[06/11 02:49:33][INFO] visual_prompt:  264: 	Training 1100/5004. train loss: 0.1073,	1.2926 s / batch. (data: 3.62e-04). ETA=7 days, 9:28:34, max mem: 14.9 GB 
[06/11 02:51:43][INFO] visual_prompt:  264: 	Training 1200/5004. train loss: 0.1019,	1.2953 s / batch. (data: 3.69e-04). ETA=7 days, 9:48:32, max mem: 14.9 GB 
[06/11 02:53:52][INFO] visual_prompt:  264: 	Training 1300/5004. train loss: 0.1087,	1.2929 s / batch. (data: 3.32e-04). ETA=7 days, 9:26:38, max mem: 14.9 GB 
[06/11 02:56:02][INFO] visual_prompt:  264: 	Training 1400/5004. train loss: 0.0974,	1.2944 s / batch. (data: 3.57e-04). ETA=7 days, 9:36:53, max mem: 14.9 GB 
[06/11 02:58:11][INFO] visual_prompt:  264: 	Training 1500/5004. train loss: 0.0914,	1.2934 s / batch. (data: 3.96e-04). ETA=7 days, 9:26:51, max mem: 14.9 GB 
[06/11 03:00:20][INFO] visual_prompt:  264: 	Training 1600/5004. train loss: 0.0914,	1.2935 s / batch. (data: 3.70e-04). ETA=7 days, 9:25:22, max mem: 14.9 GB 
[06/11 03:02:29][INFO] visual_prompt:  264: 	Training 1700/5004. train loss: 0.0991,	1.2918 s / batch. (data: 3.40e-04). ETA=7 days, 9:09:06, max mem: 14.9 GB 
[06/11 03:04:39][INFO] visual_prompt:  264: 	Training 1800/5004. train loss: 0.0840,	1.2929 s / batch. (data: 2.69e-04). ETA=7 days, 9:16:01, max mem: 14.9 GB 
[06/11 03:06:48][INFO] visual_prompt:  264: 	Training 1900/5004. train loss: 0.0819,	1.2938 s / batch. (data: 4.50e-04). ETA=7 days, 9:21:07, max mem: 14.9 GB 
[06/11 03:08:57][INFO] visual_prompt:  264: 	Training 2000/5004. train loss: 0.0761,	1.2940 s / batch. (data: 3.15e-04). ETA=7 days, 9:20:48, max mem: 14.9 GB 
[06/11 03:11:07][INFO] visual_prompt:  264: 	Training 2100/5004. train loss: 0.0756,	1.2946 s / batch. (data: 3.27e-04). ETA=7 days, 9:23:44, max mem: 14.9 GB 
[06/11 03:13:16][INFO] visual_prompt:  264: 	Training 2200/5004. train loss: 0.0694,	1.2888 s / batch. (data: 3.13e-04). ETA=7 days, 8:33:37, max mem: 14.9 GB 
[06/11 03:15:25][INFO] visual_prompt:  264: 	Training 2300/5004. train loss: 0.0700,	1.2924 s / batch. (data: 3.60e-04). ETA=7 days, 9:01:29, max mem: 14.9 GB 
[06/11 03:17:34][INFO] visual_prompt:  264: 	Training 2400/5004. train loss: 0.0676,	1.2957 s / batch. (data: 3.67e-04). ETA=7 days, 9:26:32, max mem: 14.9 GB 
[06/11 03:19:43][INFO] visual_prompt:  264: 	Training 2500/5004. train loss: 0.0658,	1.2906 s / batch. (data: 3.08e-04). ETA=7 days, 8:42:03, max mem: 14.9 GB 
[06/11 03:21:52][INFO] visual_prompt:  264: 	Training 2600/5004. train loss: 0.0612,	1.2909 s / batch. (data: 2.97e-04). ETA=7 days, 8:42:13, max mem: 14.9 GB 
[06/11 03:24:02][INFO] visual_prompt:  264: 	Training 2700/5004. train loss: 0.0616,	1.2914 s / batch. (data: 3.40e-04). ETA=7 days, 8:44:04, max mem: 14.9 GB 
[06/11 03:26:11][INFO] visual_prompt:  264: 	Training 2800/5004. train loss: 0.0564,	1.2966 s / batch. (data: 3.21e-04). ETA=7 days, 9:24:48, max mem: 14.9 GB 
[06/11 03:28:20][INFO] visual_prompt:  264: 	Training 2900/5004. train loss: 0.0559,	1.2932 s / batch. (data: 3.71e-04). ETA=7 days, 8:54:36, max mem: 14.9 GB 
[06/11 03:30:30][INFO] visual_prompt:  264: 	Training 3000/5004. train loss: 0.0544,	1.2953 s / batch. (data: 3.08e-04). ETA=7 days, 9:10:24, max mem: 14.9 GB 
[06/11 03:32:39][INFO] visual_prompt:  264: 	Training 3100/5004. train loss: 0.0465,	1.2927 s / batch. (data: 3.35e-04). ETA=7 days, 8:46:21, max mem: 14.9 GB 
[06/11 03:34:48][INFO] visual_prompt:  264: 	Training 3200/5004. train loss: 0.0468,	1.2922 s / batch. (data: 3.46e-04). ETA=7 days, 8:39:53, max mem: 14.9 GB 
[06/11 03:36:58][INFO] visual_prompt:  264: 	Training 3300/5004. train loss: 0.0471,	1.2931 s / batch. (data: 3.57e-04). ETA=7 days, 8:45:33, max mem: 14.9 GB 
[06/11 03:39:07][INFO] visual_prompt:  264: 	Training 3400/5004. train loss: 0.0430,	1.2960 s / batch. (data: 3.58e-04). ETA=7 days, 9:07:00, max mem: 14.9 GB 
[06/11 03:41:16][INFO] visual_prompt:  264: 	Training 3500/5004. train loss: 0.0399,	1.2922 s / batch. (data: 3.86e-04). ETA=7 days, 8:33:54, max mem: 14.9 GB 
[06/11 03:43:25][INFO] visual_prompt:  264: 	Training 3600/5004. train loss: 0.0347,	1.2862 s / batch. (data: 3.14e-04). ETA=7 days, 7:42:50, max mem: 14.9 GB 
[06/11 03:45:34][INFO] visual_prompt:  264: 	Training 3700/5004. train loss: 0.0333,	1.2989 s / batch. (data: 3.32e-04). ETA=7 days, 9:24:21, max mem: 14.9 GB 
[06/11 03:47:43][INFO] visual_prompt:  264: 	Training 3800/5004. train loss: 0.0384,	1.2871 s / batch. (data: 2.72e-04). ETA=7 days, 7:45:34, max mem: 14.9 GB 
[06/11 03:49:52][INFO] visual_prompt:  264: 	Training 3900/5004. train loss: 0.0342,	1.2911 s / batch. (data: 2.71e-04). ETA=7 days, 8:16:18, max mem: 14.9 GB 
[06/11 03:52:01][INFO] visual_prompt:  264: 	Training 4000/5004. train loss: 0.0367,	1.2917 s / batch. (data: 3.85e-04). ETA=7 days, 8:18:44, max mem: 14.9 GB 
[06/11 03:54:10][INFO] visual_prompt:  264: 	Training 4100/5004. train loss: 0.0300,	1.2889 s / batch. (data: 3.56e-04). ETA=7 days, 7:53:39, max mem: 14.9 GB 
[06/11 03:56:20][INFO] visual_prompt:  264: 	Training 4200/5004. train loss: 0.0324,	1.2951 s / batch. (data: 3.09e-04). ETA=7 days, 8:42:30, max mem: 14.9 GB 
[06/11 03:58:29][INFO] visual_prompt:  264: 	Training 4300/5004. train loss: 0.0309,	1.2938 s / batch. (data: 3.69e-04). ETA=7 days, 8:29:30, max mem: 14.9 GB 
[06/11 04:00:39][INFO] visual_prompt:  264: 	Training 4400/5004. train loss: 0.0276,	1.2923 s / batch. (data: 4.05e-04). ETA=7 days, 8:15:34, max mem: 14.9 GB 
[06/11 04:02:48][INFO] visual_prompt:  264: 	Training 4500/5004. train loss: 0.0247,	1.2942 s / batch. (data: 3.20e-04). ETA=7 days, 8:28:42, max mem: 14.9 GB 
[06/11 04:04:57][INFO] visual_prompt:  264: 	Training 4600/5004. train loss: 0.0202,	1.2962 s / batch. (data: 3.28e-04). ETA=7 days, 8:42:48, max mem: 14.9 GB 
[06/11 04:07:06][INFO] visual_prompt:  264: 	Training 4700/5004. train loss: 0.0284,	1.2950 s / batch. (data: 3.50e-04). ETA=7 days, 8:30:35, max mem: 14.9 GB 
[06/11 04:09:16][INFO] visual_prompt:  264: 	Training 4800/5004. train loss: 0.0261,	1.2928 s / batch. (data: 4.53e-04). ETA=7 days, 8:11:01, max mem: 14.9 GB 
[06/11 04:11:25][INFO] visual_prompt:  264: 	Training 4900/5004. train loss: 0.0221,	1.2908 s / batch. (data: 2.68e-04). ETA=7 days, 7:52:17, max mem: 14.9 GB 
[06/11 04:13:34][INFO] visual_prompt:  264: 	Training 5000/5004. train loss: 0.0260,	1.2865 s / batch. (data: 1.14e-04). ETA=7 days, 7:14:50, max mem: 14.9 GB 
[06/11 04:13:41][INFO] visual_prompt:  277: Epoch 2 / 100: avg data time: 4.18e-03, avg batch time: 1.3026, average train loss: 0.0677
[06/11 04:40:26][INFO] visual_prompt:  397: 	Test 100/196. loss: 0.013, 15.6812 s / batch. (data: 2.99e-04)max mem: 14.90891 GB 
[06/11 05:05:27][INFO] visual_prompt:  434: Inference (val):avg data time: 1.81e-04, avg batch time: 15.6485, average loss: 0.0142
[06/11 05:05:27][INFO] visual_prompt:  451: Saved invariances for val_imagenet at output/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/11 05:05:28][INFO] visual_prompt:  220: Training 3 / 100 epoch, with learning rate 0.2
[06/11 05:08:24][INFO] visual_prompt:  264: 	Training 100/5004. train loss: 0.0303,	1.2890 s / batch. (data: 3.03e-04). ETA=7 days, 7:32:45, max mem: 14.9 GB 
[06/11 05:10:33][INFO] visual_prompt:  264: 	Training 200/5004. train loss: 0.0229,	1.2889 s / batch. (data: 3.27e-04). ETA=7 days, 7:29:59, max mem: 14.9 GB 
[06/11 05:12:42][INFO] visual_prompt:  264: 	Training 300/5004. train loss: 0.0243,	1.2913 s / batch. (data: 3.14e-04). ETA=7 days, 7:47:37, max mem: 14.9 GB 
[06/11 05:14:51][INFO] visual_prompt:  264: 	Training 400/5004. train loss: 0.0245,	1.2918 s / batch. (data: 3.58e-04). ETA=7 days, 7:49:12, max mem: 14.9 GB 
[06/11 05:17:01][INFO] visual_prompt:  264: 	Training 500/5004. train loss: 0.0252,	1.2889 s / batch. (data: 3.35e-04). ETA=7 days, 7:23:42, max mem: 14.9 GB 
[06/11 05:19:10][INFO] visual_prompt:  264: 	Training 600/5004. train loss: 0.0183,	1.2893 s / batch. (data: 5.82e-04). ETA=7 days, 7:24:38, max mem: 14.9 GB 
[06/11 05:21:19][INFO] visual_prompt:  264: 	Training 700/5004. train loss: 0.0196,	1.2925 s / batch. (data: 3.51e-04). ETA=7 days, 7:49:09, max mem: 14.9 GB 
[06/11 05:23:28][INFO] visual_prompt:  264: 	Training 800/5004. train loss: 0.0219,	1.2935 s / batch. (data: 2.82e-04). ETA=7 days, 7:54:27, max mem: 14.9 GB 
[06/11 05:25:38][INFO] visual_prompt:  264: 	Training 900/5004. train loss: 0.0244,	1.2959 s / batch. (data: 3.21e-04). ETA=7 days, 8:12:08, max mem: 14.9 GB 
[06/11 05:27:47][INFO] visual_prompt:  264: 	Training 1000/5004. train loss: 0.0289,	1.2944 s / batch. (data: 3.61e-04). ETA=7 days, 7:57:56, max mem: 14.9 GB 
[06/11 05:29:56][INFO] visual_prompt:  264: 	Training 1100/5004. train loss: 0.0187,	1.2904 s / batch. (data: 3.29e-04). ETA=7 days, 7:23:08, max mem: 14.9 GB 
[06/11 05:32:05][INFO] visual_prompt:  264: 	Training 1200/5004. train loss: 0.0189,	1.2886 s / batch. (data: 3.03e-04). ETA=7 days, 7:06:00, max mem: 14.9 GB 
[06/11 05:34:14][INFO] visual_prompt:  264: 	Training 1300/5004. train loss: 0.0190,	1.2925 s / batch. (data: 3.34e-04). ETA=7 days, 7:35:50, max mem: 14.9 GB 
[06/11 05:36:23][INFO] visual_prompt:  264: 	Training 1400/5004. train loss: 0.0217,	1.2903 s / batch. (data: 2.95e-04). ETA=7 days, 7:16:10, max mem: 14.9 GB 
[06/11 05:38:32][INFO] visual_prompt:  264: 	Training 1500/5004. train loss: 0.0192,	1.2877 s / batch. (data: 3.23e-04). ETA=7 days, 6:52:32, max mem: 14.9 GB 
[06/11 05:40:41][INFO] visual_prompt:  264: 	Training 1600/5004. train loss: 0.0199,	1.2881 s / batch. (data: 3.06e-04). ETA=7 days, 6:53:47, max mem: 14.9 GB 
[06/11 05:42:50][INFO] visual_prompt:  264: 	Training 1700/5004. train loss: 0.0247,	1.2872 s / batch. (data: 2.84e-04). ETA=7 days, 6:43:53, max mem: 14.9 GB 
[06/11 05:44:59][INFO] visual_prompt:  264: 	Training 1800/5004. train loss: 0.0240,	1.2892 s / batch. (data: 3.54e-04). ETA=7 days, 6:57:55, max mem: 14.9 GB 
[06/11 05:47:08][INFO] visual_prompt:  264: 	Training 1900/5004. train loss: 0.0210,	1.2902 s / batch. (data: 2.94e-04). ETA=7 days, 7:04:22, max mem: 14.9 GB 
[06/11 05:49:17][INFO] visual_prompt:  264: 	Training 2000/5004. train loss: 0.0200,	1.2952 s / batch. (data: 3.31e-04). ETA=7 days, 7:43:02, max mem: 14.9 GB 
[06/11 05:51:26][INFO] visual_prompt:  264: 	Training 2100/5004. train loss: 0.0171,	1.2917 s / batch. (data: 2.91e-04). ETA=7 days, 7:12:13, max mem: 14.9 GB 
[06/11 05:53:35][INFO] visual_prompt:  264: 	Training 2200/5004. train loss: 0.0167,	1.2922 s / batch. (data: 3.71e-04). ETA=7 days, 7:14:14, max mem: 14.9 GB 
[06/11 05:55:44][INFO] visual_prompt:  264: 	Training 2300/5004. train loss: 0.0160,	1.2889 s / batch. (data: 3.35e-04). ETA=7 days, 6:44:44, max mem: 14.9 GB 
[06/11 05:57:54][INFO] visual_prompt:  264: 	Training 2400/5004. train loss: 0.0153,	1.2922 s / batch. (data: 3.24e-04). ETA=7 days, 7:09:36, max mem: 14.9 GB 
[06/11 06:00:03][INFO] visual_prompt:  264: 	Training 2500/5004. train loss: 0.0161,	1.2946 s / batch. (data: 5.86e-04). ETA=7 days, 7:27:16, max mem: 14.9 GB 
[06/11 06:02:12][INFO] visual_prompt:  264: 	Training 2600/5004. train loss: 0.0170,	1.3002 s / batch. (data: 3.20e-04). ETA=7 days, 8:10:48, max mem: 14.9 GB 
[06/11 06:04:21][INFO] visual_prompt:  264: 	Training 2700/5004. train loss: 0.0197,	1.2984 s / batch. (data: 4.26e-04). ETA=7 days, 7:53:48, max mem: 14.9 GB 
[06/11 06:06:30][INFO] visual_prompt:  264: 	Training 2800/5004. train loss: 0.0187,	1.2880 s / batch. (data: 3.33e-04). ETA=7 days, 6:26:50, max mem: 14.9 GB 
[06/11 06:08:39][INFO] visual_prompt:  264: 	Training 2900/5004. train loss: 0.0243,	1.2879 s / batch. (data: 2.91e-04). ETA=7 days, 6:23:47, max mem: 14.9 GB 
[06/11 06:10:48][INFO] visual_prompt:  264: 	Training 3000/5004. train loss: 0.0272,	1.2885 s / batch. (data: 3.31e-04). ETA=7 days, 6:26:29, max mem: 14.9 GB 
[06/11 06:12:57][INFO] visual_prompt:  264: 	Training 3100/5004. train loss: 0.0212,	1.2874 s / batch. (data: 3.23e-04). ETA=7 days, 6:15:45, max mem: 14.9 GB 
[06/11 06:15:06][INFO] visual_prompt:  264: 	Training 3200/5004. train loss: 0.0172,	1.2920 s / batch. (data: 3.95e-04). ETA=7 days, 6:50:57, max mem: 14.9 GB 
[06/11 06:17:15][INFO] visual_prompt:  264: 	Training 3300/5004. train loss: 0.0138,	1.2934 s / batch. (data: 3.35e-04). ETA=7 days, 6:59:49, max mem: 14.9 GB 
[06/11 06:19:24][INFO] visual_prompt:  264: 	Training 3400/5004. train loss: 0.0184,	1.2864 s / batch. (data: 3.38e-04). ETA=7 days, 6:00:46, max mem: 14.9 GB 
[06/11 06:21:33][INFO] visual_prompt:  264: 	Training 3500/5004. train loss: 0.0162,	1.2890 s / batch. (data: 3.48e-04). ETA=7 days, 6:20:16, max mem: 14.9 GB 
[06/11 06:23:42][INFO] visual_prompt:  264: 	Training 3600/5004. train loss: 0.0215,	1.2897 s / batch. (data: 3.53e-04). ETA=7 days, 6:23:34, max mem: 14.9 GB 
[06/11 06:25:51][INFO] visual_prompt:  264: 	Training 3700/5004. train loss: 0.0182,	1.2890 s / batch. (data: 4.09e-04). ETA=7 days, 6:16:08, max mem: 14.9 GB 
[06/11 06:28:00][INFO] visual_prompt:  264: 	Training 3800/5004. train loss: 0.0166,	1.2887 s / batch. (data: 3.40e-04). ETA=7 days, 6:11:13, max mem: 14.9 GB 
[06/11 06:30:09][INFO] visual_prompt:  264: 	Training 3900/5004. train loss: 0.0227,	1.2894 s / batch. (data: 2.92e-04). ETA=7 days, 6:14:34, max mem: 14.9 GB 
[06/11 06:32:19][INFO] visual_prompt:  264: 	Training 4000/5004. train loss: 0.0193,	1.2900 s / batch. (data: 2.90e-04). ETA=7 days, 6:17:07, max mem: 14.9 GB 
[06/11 06:34:28][INFO] visual_prompt:  264: 	Training 4100/5004. train loss: 0.0174,	1.2963 s / batch. (data: 3.12e-04). ETA=7 days, 7:06:36, max mem: 14.9 GB 
[06/11 06:36:37][INFO] visual_prompt:  264: 	Training 4200/5004. train loss: 0.0182,	1.2949 s / batch. (data: 3.60e-04). ETA=7 days, 6:52:33, max mem: 14.9 GB 
[06/11 06:38:46][INFO] visual_prompt:  264: 	Training 4300/5004. train loss: 0.0228,	1.2977 s / batch. (data: 3.96e-04). ETA=7 days, 7:13:41, max mem: 14.9 GB 
[06/11 06:40:56][INFO] visual_prompt:  264: 	Training 4400/5004. train loss: 0.0270,	1.2921 s / batch. (data: 3.21e-04). ETA=7 days, 6:26:12, max mem: 14.9 GB 
[06/11 06:43:05][INFO] visual_prompt:  264: 	Training 4500/5004. train loss: 0.0164,	1.2938 s / batch. (data: 3.96e-04). ETA=7 days, 6:37:20, max mem: 14.9 GB 
[06/11 06:45:14][INFO] visual_prompt:  264: 	Training 4600/5004. train loss: 0.0204,	1.2891 s / batch. (data: 1.91e-04). ETA=7 days, 5:57:29, max mem: 14.9 GB 
[06/11 06:47:23][INFO] visual_prompt:  264: 	Training 4700/5004. train loss: 0.0211,	1.2866 s / batch. (data: 3.28e-04). ETA=7 days, 5:34:46, max mem: 14.9 GB 
[06/11 06:49:32][INFO] visual_prompt:  264: 	Training 4800/5004. train loss: 0.0169,	1.2864 s / batch. (data: 3.65e-04). ETA=7 days, 5:30:52, max mem: 14.9 GB 
[06/11 06:51:41][INFO] visual_prompt:  264: 	Training 4900/5004. train loss: 0.0200,	1.3003 s / batch. (data: 3.25e-04). ETA=7 days, 7:21:10, max mem: 14.9 GB 
[06/11 06:53:50][INFO] visual_prompt:  264: 	Training 5000/5004. train loss: 0.0207,	1.2901 s / batch. (data: 1.29e-04). ETA=7 days, 5:56:38, max mem: 14.9 GB 
[06/11 06:53:56][INFO] visual_prompt:  277: Epoch 3 / 100: avg data time: 4.53e-03, avg batch time: 1.3004, average train loss: 0.0208
[06/11 07:20:43][INFO] visual_prompt:  397: 	Test 100/196. loss: 0.010, 15.6831 s / batch. (data: 1.73e-04)max mem: 14.90891 GB 
[06/11 07:45:45][INFO] visual_prompt:  434: Inference (val):avg data time: 1.93e-04, avg batch time: 15.6578, average loss: 0.0105
[06/11 07:45:45][INFO] visual_prompt:  451: Saved invariances for val_imagenet at output/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/11 07:45:46][INFO] visual_prompt:  220: Training 4 / 100 epoch, with learning rate 0.3
[06/11 07:48:40][INFO] visual_prompt:  264: 	Training 100/5004. train loss: 0.0228,	1.2938 s / batch. (data: 3.14e-04). ETA=7 days, 6:24:03, max mem: 14.9 GB 
[06/11 07:50:49][INFO] visual_prompt:  264: 	Training 200/5004. train loss: 0.0324,	1.2930 s / batch. (data: 3.45e-04). ETA=7 days, 6:15:53, max mem: 14.9 GB 
[06/11 07:52:58][INFO] visual_prompt:  264: 	Training 300/5004. train loss: 0.0361,	1.2901 s / batch. (data: 3.98e-04). ETA=7 days, 5:49:51, max mem: 14.9 GB 
[06/11 07:55:07][INFO] visual_prompt:  264: 	Training 400/5004. train loss: 0.0304,	1.2904 s / batch. (data: 3.39e-04). ETA=7 days, 5:50:17, max mem: 14.9 GB 
[06/11 07:57:16][INFO] visual_prompt:  264: 	Training 500/5004. train loss: 0.0285,	1.2890 s / batch. (data: 3.28e-04). ETA=7 days, 5:36:54, max mem: 14.9 GB 
[06/11 07:59:25][INFO] visual_prompt:  264: 	Training 600/5004. train loss: 0.0282,	1.2904 s / batch. (data: 2.66e-04). ETA=7 days, 5:45:53, max mem: 14.9 GB 
[06/11 08:01:34][INFO] visual_prompt:  264: 	Training 700/5004. train loss: 0.0336,	1.2895 s / batch. (data: 3.28e-04). ETA=7 days, 5:36:46, max mem: 14.9 GB 
[06/11 08:03:43][INFO] visual_prompt:  264: 	Training 800/5004. train loss: 0.0226,	1.2947 s / batch. (data: 2.93e-04). ETA=7 days, 6:16:47, max mem: 14.9 GB 
[06/11 08:05:52][INFO] visual_prompt:  264: 	Training 900/5004. train loss: 0.0238,	1.2906 s / batch. (data: 3.28e-04). ETA=7 days, 5:41:06, max mem: 14.9 GB 
[06/11 08:08:01][INFO] visual_prompt:  264: 	Training 1000/5004. train loss: 0.0243,	1.2890 s / batch. (data: 3.27e-04). ETA=7 days, 5:26:11, max mem: 14.9 GB 
[06/11 08:10:10][INFO] visual_prompt:  264: 	Training 1100/5004. train loss: 0.0185,	1.2884 s / batch. (data: 3.10e-04). ETA=7 days, 5:19:06, max mem: 14.9 GB 
[06/11 08:12:19][INFO] visual_prompt:  264: 	Training 1200/5004. train loss: 0.0221,	1.2893 s / batch. (data: 4.46e-04). ETA=7 days, 5:24:02, max mem: 14.9 GB 
[06/11 08:14:28][INFO] visual_prompt:  264: 	Training 1300/5004. train loss: 0.0177,	1.2913 s / batch. (data: 3.48e-04). ETA=7 days, 5:38:44, max mem: 14.9 GB 
[06/11 08:16:37][INFO] visual_prompt:  264: 	Training 1400/5004. train loss: 0.0169,	1.2899 s / batch. (data: 3.57e-04). ETA=7 days, 5:24:57, max mem: 14.9 GB 
[06/11 08:18:47][INFO] visual_prompt:  264: 	Training 1500/5004. train loss: 0.0203,	1.2967 s / batch. (data: 2.98e-04). ETA=7 days, 6:17:34, max mem: 14.9 GB 
[06/11 08:20:56][INFO] visual_prompt:  264: 	Training 1600/5004. train loss: 0.0160,	1.2932 s / batch. (data: 3.71e-04). ETA=7 days, 5:47:35, max mem: 14.9 GB 
[06/11 08:23:05][INFO] visual_prompt:  264: 	Training 1700/5004. train loss: 0.0147,	1.2928 s / batch. (data: 3.32e-04). ETA=7 days, 5:42:01, max mem: 14.9 GB 
[06/11 08:25:15][INFO] visual_prompt:  264: 	Training 1800/5004. train loss: 0.0163,	1.2933 s / batch. (data: 3.39e-04). ETA=7 days, 5:43:43, max mem: 14.9 GB 
[06/11 08:27:24][INFO] visual_prompt:  264: 	Training 1900/5004. train loss: 0.0180,	1.2929 s / batch. (data: 3.21e-04). ETA=7 days, 5:38:24, max mem: 14.9 GB 
[06/11 08:29:33][INFO] visual_prompt:  264: 	Training 2000/5004. train loss: 0.0225,	1.2874 s / batch. (data: 3.86e-04). ETA=7 days, 4:52:11, max mem: 14.9 GB 
[06/11 08:31:43][INFO] visual_prompt:  264: 	Training 2100/5004. train loss: 0.0176,	1.2892 s / batch. (data: 3.25e-04). ETA=7 days, 5:04:23, max mem: 14.9 GB 
[06/11 08:33:52][INFO] visual_prompt:  264: 	Training 2200/5004. train loss: 0.0223,	1.2942 s / batch. (data: 3.42e-04). ETA=7 days, 5:42:29, max mem: 14.9 GB 
[06/11 08:36:01][INFO] visual_prompt:  264: 	Training 2300/5004. train loss: 0.0219,	1.2901 s / batch. (data: 3.25e-04). ETA=7 days, 5:07:05, max mem: 14.9 GB 
[06/11 08:38:10][INFO] visual_prompt:  264: 	Training 2400/5004. train loss: 0.0224,	1.2881 s / batch. (data: 3.03e-04). ETA=7 days, 4:48:36, max mem: 14.9 GB 
[06/11 08:40:19][INFO] visual_prompt:  264: 	Training 2500/5004. train loss: 0.0228,	1.2927 s / batch. (data: 3.13e-04). ETA=7 days, 5:23:49, max mem: 14.9 GB 
[06/11 08:42:27][INFO] visual_prompt:  264: 	Training 2600/5004. train loss: 0.0229,	1.2872 s / batch. (data: 3.27e-04). ETA=7 days, 4:37:27, max mem: 14.9 GB 
[06/11 08:44:36][INFO] visual_prompt:  264: 	Training 2700/5004. train loss: 0.0232,	1.2903 s / batch. (data: 2.98e-04). ETA=7 days, 5:00:33, max mem: 14.9 GB 
[06/11 08:46:45][INFO] visual_prompt:  264: 	Training 2800/5004. train loss: 0.0168,	1.2893 s / batch. (data: 3.31e-04). ETA=7 days, 4:49:40, max mem: 14.9 GB 
[06/11 08:48:54][INFO] visual_prompt:  264: 	Training 2900/5004. train loss: 0.0215,	1.2891 s / batch. (data: 3.83e-04). ETA=7 days, 4:46:16, max mem: 14.9 GB 
[06/11 08:51:03][INFO] visual_prompt:  264: 	Training 3000/5004. train loss: 0.0292,	1.2893 s / batch. (data: 3.60e-04). ETA=7 days, 4:46:03, max mem: 14.9 GB 
[06/11 08:53:12][INFO] visual_prompt:  264: 	Training 3100/5004. train loss: 0.0155,	1.2926 s / batch. (data: 4.43e-04). ETA=7 days, 5:09:44, max mem: 14.9 GB 
[06/11 08:55:22][INFO] visual_prompt:  264: 	Training 3200/5004. train loss: 0.0220,	1.2901 s / batch. (data: 2.94e-04). ETA=7 days, 4:47:53, max mem: 14.9 GB 
[06/11 08:57:31][INFO] visual_prompt:  264: 	Training 3300/5004. train loss: 0.0196,	1.2933 s / batch. (data: 3.40e-04). ETA=7 days, 5:11:17, max mem: 14.9 GB 
[06/11 08:59:40][INFO] visual_prompt:  264: 	Training 3400/5004. train loss: 0.0224,	1.2907 s / batch. (data: 3.31e-04). ETA=7 days, 4:48:21, max mem: 14.9 GB 
[06/11 09:01:49][INFO] visual_prompt:  264: 	Training 3500/5004. train loss: 0.0287,	1.2942 s / batch. (data: 3.40e-04). ETA=7 days, 5:14:09, max mem: 14.9 GB 
[06/11 09:03:59][INFO] visual_prompt:  264: 	Training 3600/5004. train loss: 0.0174,	1.2987 s / batch. (data: 4.34e-04). ETA=7 days, 5:48:25, max mem: 14.9 GB 
[06/11 09:06:08][INFO] visual_prompt:  264: 	Training 3700/5004. train loss: 0.0242,	1.2912 s / batch. (data: 3.61e-04). ETA=7 days, 4:46:18, max mem: 14.9 GB 
[06/11 09:08:17][INFO] visual_prompt:  264: 	Training 3800/5004. train loss: 0.0261,	1.2888 s / batch. (data: 3.30e-04). ETA=7 days, 4:24:18, max mem: 14.9 GB 
[06/11 09:10:26][INFO] visual_prompt:  264: 	Training 3900/5004. train loss: 0.0217,	1.2894 s / batch. (data: 3.69e-04). ETA=7 days, 4:26:56, max mem: 14.9 GB 
[06/11 09:12:35][INFO] visual_prompt:  264: 	Training 4000/5004. train loss: 0.0217,	1.2904 s / batch. (data: 2.97e-04). ETA=7 days, 4:33:13, max mem: 14.9 GB 
[06/11 09:14:44][INFO] visual_prompt:  264: 	Training 4100/5004. train loss: 0.0220,	1.2855 s / batch. (data: 3.18e-04). ETA=7 days, 3:51:41, max mem: 14.9 GB 
[06/11 09:16:53][INFO] visual_prompt:  264: 	Training 4200/5004. train loss: 0.0223,	1.2885 s / batch. (data: 4.37e-04). ETA=7 days, 4:13:31, max mem: 14.9 GB 
[06/11 09:19:01][INFO] visual_prompt:  264: 	Training 4300/5004. train loss: 0.0237,	1.2867 s / batch. (data: 3.01e-04). ETA=7 days, 3:57:10, max mem: 14.9 GB 
[06/11 09:21:10][INFO] visual_prompt:  264: 	Training 4400/5004. train loss: 0.0184,	1.2903 s / batch. (data: 4.54e-04). ETA=7 days, 4:23:21, max mem: 14.9 GB 
[06/11 09:23:19][INFO] visual_prompt:  264: 	Training 4500/5004. train loss: 0.0228,	1.2909 s / batch. (data: 3.19e-04). ETA=7 days, 4:25:57, max mem: 14.9 GB 
[06/11 09:25:28][INFO] visual_prompt:  264: 	Training 4600/5004. train loss: 0.0171,	1.2886 s / batch. (data: 3.12e-04). ETA=7 days, 4:05:25, max mem: 14.9 GB 
[06/11 09:27:37][INFO] visual_prompt:  264: 	Training 4700/5004. train loss: 0.0188,	1.2896 s / batch. (data: 3.81e-04). ETA=7 days, 4:11:35, max mem: 14.9 GB 
[06/11 09:29:46][INFO] visual_prompt:  264: 	Training 4800/5004. train loss: 0.0272,	1.2907 s / batch. (data: 4.80e-04). ETA=7 days, 4:17:52, max mem: 14.9 GB 
[06/11 09:31:55][INFO] visual_prompt:  264: 	Training 4900/5004. train loss: 0.0240,	1.2902 s / batch. (data: 3.29e-04). ETA=7 days, 4:12:04, max mem: 14.9 GB 
[06/11 09:34:04][INFO] visual_prompt:  264: 	Training 5000/5004. train loss: 0.0220,	1.2878 s / batch. (data: 1.38e-04). ETA=7 days, 3:51:05, max mem: 14.9 GB 
[06/11 09:34:11][INFO] visual_prompt:  277: Epoch 4 / 100: avg data time: 4.56e-03, avg batch time: 1.2997, average train loss: 0.0224
[06/11 10:00:55][INFO] visual_prompt:  397: 	Test 100/196. loss: 0.014, 15.6910 s / batch. (data: 2.14e-04)max mem: 14.90891 GB 
[06/11 10:25:58][INFO] visual_prompt:  434: Inference (val):avg data time: 1.98e-04, avg batch time: 15.6578, average loss: 0.0152
[06/11 10:25:58][INFO] visual_prompt:  451: Saved invariances for val_imagenet at output/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/11 10:25:58][INFO] visual_prompt:  220: Training 5 / 100 epoch, with learning rate 0.4
[06/11 10:28:53][INFO] visual_prompt:  264: 	Training 100/5004. train loss: 0.0302,	1.2893 s / batch. (data: 3.24e-04). ETA=7 days, 4:00:53, max mem: 14.9 GB 
[06/11 10:31:02][INFO] visual_prompt:  264: 	Training 200/5004. train loss: 0.0269,	1.2882 s / batch. (data: 3.23e-04). ETA=7 days, 3:49:31, max mem: 14.9 GB 
[06/11 10:33:11][INFO] visual_prompt:  264: 	Training 300/5004. train loss: 0.0243,	1.2877 s / batch. (data: 3.12e-04). ETA=7 days, 3:43:00, max mem: 14.9 GB 
[06/11 10:35:20][INFO] visual_prompt:  264: 	Training 400/5004. train loss: 0.0249,	1.2901 s / batch. (data: 3.23e-04). ETA=7 days, 4:00:36, max mem: 14.9 GB 
[06/11 10:37:29][INFO] visual_prompt:  264: 	Training 500/5004. train loss: 0.0235,	1.2888 s / batch. (data: 2.77e-04). ETA=7 days, 3:47:32, max mem: 14.9 GB 
[06/11 10:39:38][INFO] visual_prompt:  264: 	Training 600/5004. train loss: 0.0202,	1.2928 s / batch. (data: 3.15e-04). ETA=7 days, 4:17:21, max mem: 14.9 GB 
[06/11 10:41:47][INFO] visual_prompt:  264: 	Training 700/5004. train loss: 0.0238,	1.2888 s / batch. (data: 4.69e-04). ETA=7 days, 3:43:14, max mem: 14.9 GB 
[06/11 10:43:56][INFO] visual_prompt:  264: 	Training 800/5004. train loss: 0.0218,	1.2924 s / batch. (data: 3.60e-04). ETA=7 days, 4:10:21, max mem: 14.9 GB 
[06/11 10:46:05][INFO] visual_prompt:  264: 	Training 900/5004. train loss: 0.0298,	1.2918 s / batch. (data: 3.13e-04). ETA=7 days, 4:02:58, max mem: 14.9 GB 
[06/11 10:48:15][INFO] visual_prompt:  264: 	Training 1000/5004. train loss: 0.0169,	1.2967 s / batch. (data: 3.03e-04). ETA=7 days, 4:40:16, max mem: 14.9 GB 
[06/11 10:50:24][INFO] visual_prompt:  264: 	Training 1100/5004. train loss: 0.0220,	1.2936 s / batch. (data: 3.77e-04). ETA=7 days, 4:13:07, max mem: 14.9 GB 
[06/11 10:52:34][INFO] visual_prompt:  264: 	Training 1200/5004. train loss: 0.0267,	1.2958 s / batch. (data: 3.40e-04). ETA=7 days, 4:28:55, max mem: 14.9 GB 
[06/11 10:54:43][INFO] visual_prompt:  264: 	Training 1300/5004. train loss: 0.0319,	1.2917 s / batch. (data: 3.31e-04). ETA=7 days, 3:54:11, max mem: 14.9 GB 
[06/11 10:56:53][INFO] visual_prompt:  264: 	Training 1400/5004. train loss: 0.0289,	1.2923 s / batch. (data: 3.42e-04). ETA=7 days, 3:56:50, max mem: 14.9 GB 
[06/11 10:59:02][INFO] visual_prompt:  264: 	Training 1500/5004. train loss: 0.0243,	1.2902 s / batch. (data: 3.07e-04). ETA=7 days, 3:37:18, max mem: 14.9 GB 
[06/11 11:01:11][INFO] visual_prompt:  264: 	Training 1600/5004. train loss: 0.0270,	1.2892 s / batch. (data: 2.08e-04). ETA=7 days, 3:27:44, max mem: 14.9 GB 
[06/11 11:03:20][INFO] visual_prompt:  264: 	Training 1700/5004. train loss: 0.0225,	1.2857 s / batch. (data: 4.56e-04). ETA=7 days, 2:57:08, max mem: 14.9 GB 
[06/11 11:05:29][INFO] visual_prompt:  264: 	Training 1800/5004. train loss: 0.0261,	1.2888 s / batch. (data: 3.76e-04). ETA=7 days, 3:20:06, max mem: 14.9 GB 
[06/11 11:07:38][INFO] visual_prompt:  264: 	Training 1900/5004. train loss: 0.0261,	1.2883 s / batch. (data: 3.86e-04). ETA=7 days, 3:13:43, max mem: 14.9 GB 
[06/11 11:09:47][INFO] visual_prompt:  264: 	Training 2000/5004. train loss: 0.0344,	1.2909 s / batch. (data: 3.71e-04). ETA=7 days, 3:32:24, max mem: 14.9 GB 
[06/11 11:11:56][INFO] visual_prompt:  264: 	Training 2100/5004. train loss: 0.0243,	1.2900 s / batch. (data: 3.20e-04). ETA=7 days, 3:23:15, max mem: 14.9 GB 
[06/11 11:14:05][INFO] visual_prompt:  264: 	Training 2200/5004. train loss: 0.0186,	1.2887 s / batch. (data: 3.60e-04). ETA=7 days, 3:10:12, max mem: 14.9 GB 
[06/11 11:16:14][INFO] visual_prompt:  264: 	Training 2300/5004. train loss: 0.0263,	1.2870 s / batch. (data: 2.76e-04). ETA=7 days, 2:54:47, max mem: 14.9 GB 
[06/11 11:18:23][INFO] visual_prompt:  264: 	Training 2400/5004. train loss: 0.0195,	1.2947 s / batch. (data: 3.36e-04). ETA=7 days, 3:54:19, max mem: 14.9 GB 
[06/11 11:20:32][INFO] visual_prompt:  264: 	Training 2500/5004. train loss: 0.0263,	1.2945 s / batch. (data: 4.14e-04). ETA=7 days, 3:50:33, max mem: 14.9 GB 
[06/11 11:22:42][INFO] visual_prompt:  264: 	Training 2600/5004. train loss: 0.0220,	1.2923 s / batch. (data: 3.56e-04). ETA=7 days, 3:30:48, max mem: 14.9 GB 
[06/11 11:24:51][INFO] visual_prompt:  264: 	Training 2700/5004. train loss: 0.0224,	1.2919 s / batch. (data: 3.77e-04). ETA=7 days, 3:25:02, max mem: 14.9 GB 
[06/11 11:27:00][INFO] visual_prompt:  264: 	Training 2800/5004. train loss: 0.0262,	1.2926 s / batch. (data: 3.37e-04). ETA=7 days, 3:29:04, max mem: 14.9 GB 
[06/11 11:29:09][INFO] visual_prompt:  264: 	Training 2900/5004. train loss: 0.0248,	1.2925 s / batch. (data: 3.53e-04). ETA=7 days, 3:25:44, max mem: 14.9 GB 
[06/11 11:31:18][INFO] visual_prompt:  264: 	Training 3000/5004. train loss: 0.0253,	1.2912 s / batch. (data: 3.25e-04). ETA=7 days, 3:13:09, max mem: 14.9 GB 
[06/11 11:33:28][INFO] visual_prompt:  264: 	Training 3100/5004. train loss: 0.0238,	1.2918 s / batch. (data: 3.18e-04). ETA=7 days, 3:15:32, max mem: 14.9 GB 
[06/11 11:35:37][INFO] visual_prompt:  264: 	Training 3200/5004. train loss: 0.0235,	1.2913 s / batch. (data: 3.96e-04). ETA=7 days, 3:09:26, max mem: 14.9 GB 
[06/11 11:37:46][INFO] visual_prompt:  264: 	Training 3300/5004. train loss: 0.0293,	1.2905 s / batch. (data: 3.33e-04). ETA=7 days, 3:00:58, max mem: 14.9 GB 
[06/11 11:39:55][INFO] visual_prompt:  264: 	Training 3400/5004. train loss: 0.0294,	1.2902 s / batch. (data: 3.19e-04). ETA=7 days, 2:57:01, max mem: 14.9 GB 
[06/11 11:42:04][INFO] visual_prompt:  264: 	Training 3500/5004. train loss: 0.0307,	1.2880 s / batch. (data: 2.87e-04). ETA=7 days, 2:36:52, max mem: 14.9 GB 
[06/11 11:44:13][INFO] visual_prompt:  264: 	Training 3600/5004. train loss: 0.0233,	1.2890 s / batch. (data: 3.33e-04). ETA=7 days, 2:42:43, max mem: 14.9 GB 
[06/11 11:46:22][INFO] visual_prompt:  264: 	Training 3700/5004. train loss: 0.0178,	1.2884 s / batch. (data: 3.32e-04). ETA=7 days, 2:36:14, max mem: 14.9 GB 
[06/11 11:48:30][INFO] visual_prompt:  264: 	Training 3800/5004. train loss: 0.0194,	1.2867 s / batch. (data: 2.83e-04). ETA=7 days, 2:20:29, max mem: 14.9 GB 
[06/11 11:50:39][INFO] visual_prompt:  264: 	Training 3900/5004. train loss: 0.0201,	1.2870 s / batch. (data: 3.36e-04). ETA=7 days, 2:20:57, max mem: 14.9 GB 
[06/11 11:52:48][INFO] visual_prompt:  264: 	Training 4000/5004. train loss: 0.0194,	1.2906 s / batch. (data: 3.01e-04). ETA=7 days, 2:46:51, max mem: 14.9 GB 
[06/11 11:54:57][INFO] visual_prompt:  264: 	Training 4100/5004. train loss: 0.0168,	1.2952 s / batch. (data: 4.60e-04). ETA=7 days, 3:21:29, max mem: 14.9 GB 
[06/11 11:57:07][INFO] visual_prompt:  264: 	Training 4200/5004. train loss: 0.0206,	1.2941 s / batch. (data: 4.11e-04). ETA=7 days, 3:10:36, max mem: 14.9 GB 
[06/11 11:59:16][INFO] visual_prompt:  264: 	Training 4300/5004. train loss: 0.0213,	1.2916 s / batch. (data: 3.79e-04). ETA=7 days, 2:48:30, max mem: 14.9 GB 
[06/11 12:01:25][INFO] visual_prompt:  264: 	Training 4400/5004. train loss: 0.0220,	1.2900 s / batch. (data: 3.50e-04). ETA=7 days, 2:33:56, max mem: 14.9 GB 
[06/11 12:03:34][INFO] visual_prompt:  264: 	Training 4500/5004. train loss: 0.0187,	1.2896 s / batch. (data: 3.26e-04). ETA=7 days, 2:28:15, max mem: 14.9 GB 
[06/11 12:05:43][INFO] visual_prompt:  264: 	Training 4600/5004. train loss: 0.0209,	1.2932 s / batch. (data: 3.48e-04). ETA=7 days, 2:55:00, max mem: 14.9 GB 
[06/11 12:07:53][INFO] visual_prompt:  264: 	Training 4700/5004. train loss: 0.0207,	1.2920 s / batch. (data: 3.34e-04). ETA=7 days, 2:43:16, max mem: 14.9 GB 
[06/11 12:10:02][INFO] visual_prompt:  264: 	Training 4800/5004. train loss: 0.0218,	1.2933 s / batch. (data: 3.96e-04). ETA=7 days, 2:51:35, max mem: 14.9 GB 
[06/11 12:12:11][INFO] visual_prompt:  264: 	Training 4900/5004. train loss: 0.0272,	1.2927 s / batch. (data: 3.32e-04). ETA=7 days, 2:44:21, max mem: 14.9 GB 
[06/11 12:14:20][INFO] visual_prompt:  264: 	Training 5000/5004. train loss: 0.0176,	1.2859 s / batch. (data: 8.11e-05). ETA=7 days, 1:48:26, max mem: 14.9 GB 
[06/11 12:14:27][INFO] visual_prompt:  277: Epoch 5 / 100: avg data time: 4.54e-03, avg batch time: 1.3005, average train loss: 0.0242
[06/11 12:41:14][INFO] visual_prompt:  397: 	Test 100/196. loss: 0.012, 15.7072 s / batch. (data: 3.00e-04)max mem: 14.90891 GB 
[06/11 13:06:19][INFO] visual_prompt:  434: Inference (val):avg data time: 2.09e-04, avg batch time: 15.6785, average loss: 0.0129
[06/11 13:06:19][INFO] visual_prompt:  451: Saved invariances for val_imagenet at output/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/11 13:06:19][INFO] visual_prompt:  220: Training 6 / 100 epoch, with learning rate 0.5
[06/11 13:09:13][INFO] visual_prompt:  264: 	Training 100/5004. train loss: 0.0269,	1.2926 s / batch. (data: 3.46e-04). ETA=7 days, 2:39:17, max mem: 14.9 GB 
