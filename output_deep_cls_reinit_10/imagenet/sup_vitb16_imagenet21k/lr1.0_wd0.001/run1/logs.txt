[06/16 23:38:25][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/16 23:38:25][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,4,5,6
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/16 23:38:25][INFO] visual_prompt:   99: Command line arguments: None
[06/16 23:38:25][INFO] visual_prompt:  108: Training with config:
[06/16 23:38:25][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': False,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls-reinit+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 1.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'mse',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'USE_CLS_TOKEN': True,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/16 23:38:28][INFO] visual_prompt:   56: Total Parameters: 98103568	 Gradient Parameters: 12305680
[06/16 23:38:28][INFO] visual_prompt:   58: tuned percent:12.544
[06/16 23:38:29][INFO] visual_prompt:   44: Device used for model: 0
[06/16 23:38:29][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/16 23:38:29][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/16 23:38:32][INFO] visual_prompt:  158: Number of images: 1281167
[06/16 23:38:32][INFO] visual_prompt:  159: Number of classes: 1000
[06/16 23:38:32][INFO] visual_prompt:   78: Loading validation data...
[06/16 23:38:32][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/16 23:38:32][INFO] visual_prompt:  158: Number of images: 50000
[06/16 23:38:32][INFO] visual_prompt:  159: Number of classes: 1000
[06/16 23:38:32][INFO] visual_prompt:   81: Loading test data...
[06/16 23:38:32][INFO] visual_prompt:   83: ...no test data is constructed
[06/16 23:38:32][INFO] visual_prompt:  111: Constructing models...
[06/16 23:38:32][INFO] visual_prompt:  114: Setting up Evalutator...
[06/16 23:38:32][INFO] visual_prompt:  116: Setting up Trainer...
[06/16 23:38:32][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.deep_prompt_embeddings: True
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/16 23:38:32][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/16 23:38:32][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/16 23:38:32][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/16 23:38:32][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/16 23:38:32][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/16 23:38:32][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/16 23:38:32][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/16 23:38:32][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/16 23:38:32][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/16 23:38:32][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/16 23:38:32][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/16 23:38:32][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/16 23:38:32][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/16 23:38:32][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/16 23:38:32][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/16 23:38:32][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/16 23:38:32][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/16 23:38:32][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/16 23:38:32][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/16 23:38:32][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/16 23:38:32][INFO] visual_prompt:  238: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/16 23:38:32][INFO] visual_prompt:  257: Training 1 / 100 epoch, with learning rate 0.0
[06/16 23:40:52][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0268,	1.1747 s / batch. (data: 3.08e-04). ETA=6 days, 19:14:59, max mem: 15.1 GB 
[06/16 23:42:49][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0199,	1.1731 s / batch. (data: 3.71e-04). ETA=6 days, 18:59:50, max mem: 15.1 GB 
[06/16 23:44:47][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0241,	1.1696 s / batch. (data: 5.20e-04). ETA=6 days, 18:28:48, max mem: 15.1 GB 
[06/16 23:46:44][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0224,	1.1735 s / batch. (data: 3.12e-04). ETA=6 days, 18:58:57, max mem: 15.1 GB 
[06/16 23:48:42][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0245,	1.1715 s / batch. (data: 3.02e-04). ETA=6 days, 18:40:20, max mem: 15.1 GB 
[06/16 23:50:39][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0238,	1.1710 s / batch. (data: 3.59e-04). ETA=6 days, 18:34:47, max mem: 15.1 GB 
[06/16 23:52:36][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0249,	1.1794 s / batch. (data: 3.33e-04). ETA=6 days, 19:42:25, max mem: 15.1 GB 
[06/16 23:54:34][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0247,	1.1728 s / batch. (data: 3.60e-04). ETA=6 days, 18:45:37, max mem: 15.1 GB 
[06/16 23:56:31][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0204,	1.1725 s / batch. (data: 3.00e-04). ETA=6 days, 18:41:11, max mem: 15.1 GB 
[06/16 23:58:29][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0245,	1.1764 s / batch. (data: 3.42e-04). ETA=6 days, 19:11:39, max mem: 15.1 GB 
[06/17 00:00:26][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0236,	1.1700 s / batch. (data: 3.67e-04). ETA=6 days, 18:16:30, max mem: 15.1 GB 
[06/17 00:02:23][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0236,	1.1707 s / batch. (data: 3.54e-04). ETA=6 days, 18:20:37, max mem: 15.1 GB 
[06/17 00:04:21][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0239,	1.1715 s / batch. (data: 3.40e-04). ETA=6 days, 18:24:36, max mem: 15.1 GB 
[06/17 00:06:18][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0262,	1.1751 s / batch. (data: 4.31e-04). ETA=6 days, 18:53:09, max mem: 15.1 GB 
[06/17 00:08:15][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0235,	1.1697 s / batch. (data: 3.45e-04). ETA=6 days, 18:06:04, max mem: 15.1 GB 
[06/17 00:10:12][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0273,	1.1746 s / batch. (data: 3.52e-04). ETA=6 days, 18:45:11, max mem: 15.1 GB 
[06/17 00:12:10][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0237,	1.1744 s / batch. (data: 3.26e-04). ETA=6 days, 18:40:49, max mem: 15.1 GB 
[06/17 00:14:07][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0221,	1.1709 s / batch. (data: 3.28e-04). ETA=6 days, 18:10:27, max mem: 15.1 GB 
[06/17 00:16:05][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0239,	1.1789 s / batch. (data: 3.09e-04). ETA=6 days, 19:14:51, max mem: 15.1 GB 
[06/17 00:18:02][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0202,	1.1760 s / batch. (data: 2.59e-04). ETA=6 days, 18:48:32, max mem: 15.1 GB 
[06/17 00:19:59][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0237,	1.1764 s / batch. (data: 3.61e-04). ETA=6 days, 18:50:05, max mem: 15.1 GB 
[06/17 00:21:57][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0227,	1.1727 s / batch. (data: 3.16e-04). ETA=6 days, 18:17:15, max mem: 15.1 GB 
[06/17 00:23:54][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0208,	1.1778 s / batch. (data: 3.56e-04). ETA=6 days, 18:58:06, max mem: 15.1 GB 
[06/17 00:25:52][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0199,	1.1741 s / batch. (data: 3.03e-04). ETA=6 days, 18:24:57, max mem: 15.1 GB 
[06/17 00:27:49][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0233,	1.1723 s / batch. (data: 3.16e-04). ETA=6 days, 18:08:32, max mem: 15.1 GB 
[06/17 00:29:46][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0219,	1.1712 s / batch. (data: 3.08e-04). ETA=6 days, 17:56:47, max mem: 15.1 GB 
[06/17 00:31:44][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0237,	1.1784 s / batch. (data: 3.31e-04). ETA=6 days, 18:54:42, max mem: 15.1 GB 
[06/17 00:33:42][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0233,	1.1767 s / batch. (data: 3.12e-04). ETA=6 days, 18:38:38, max mem: 15.1 GB 
[06/17 00:35:39][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0250,	1.1776 s / batch. (data: 4.73e-04). ETA=6 days, 18:44:21, max mem: 15.1 GB 
[06/17 00:37:36][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0230,	1.1705 s / batch. (data: 4.69e-04). ETA=6 days, 17:43:32, max mem: 15.1 GB 
[06/17 00:39:34][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0222,	1.1760 s / batch. (data: 3.16e-04). ETA=6 days, 18:26:46, max mem: 15.1 GB 
[06/17 00:41:31][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0242,	1.1747 s / batch. (data: 3.66e-04). ETA=6 days, 18:14:18, max mem: 15.1 GB 
[06/17 00:43:29][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0254,	1.1757 s / batch. (data: 2.71e-04). ETA=6 days, 18:20:32, max mem: 15.1 GB 
[06/17 00:45:26][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0270,	1.1706 s / batch. (data: 2.76e-04). ETA=6 days, 17:36:35, max mem: 15.1 GB 
[06/17 00:47:23][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0247,	1.1714 s / batch. (data: 3.26e-04). ETA=6 days, 17:40:44, max mem: 15.1 GB 
[06/17 00:49:21][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0274,	1.1764 s / batch. (data: 4.78e-04). ETA=6 days, 18:20:37, max mem: 15.1 GB 
[06/17 00:51:18][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0224,	1.1711 s / batch. (data: 4.08e-04). ETA=6 days, 17:34:44, max mem: 15.1 GB 
[06/17 00:53:15][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0248,	1.1721 s / batch. (data: 3.11e-04). ETA=6 days, 17:40:51, max mem: 15.1 GB 
[06/17 00:55:13][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0257,	1.1731 s / batch. (data: 3.56e-04). ETA=6 days, 17:47:35, max mem: 15.1 GB 
[06/17 00:57:10][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0232,	1.1711 s / batch. (data: 3.20e-04). ETA=6 days, 17:28:46, max mem: 15.1 GB 
[06/17 00:59:08][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0266,	1.1772 s / batch. (data: 3.07e-04). ETA=6 days, 18:17:29, max mem: 15.1 GB 
[06/17 01:01:05][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0262,	1.1722 s / batch. (data: 2.91e-04). ETA=6 days, 17:34:04, max mem: 15.1 GB 
[06/17 01:03:03][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0258,	1.1711 s / batch. (data: 3.32e-04). ETA=6 days, 17:22:51, max mem: 15.1 GB 
[06/17 01:05:00][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0245,	1.1726 s / batch. (data: 3.26e-04). ETA=6 days, 17:33:17, max mem: 15.1 GB 
[06/17 01:06:57][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0267,	1.1719 s / batch. (data: 3.94e-04). ETA=6 days, 17:25:51, max mem: 15.1 GB 
[06/17 01:08:55][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0231,	1.1705 s / batch. (data: 3.36e-04). ETA=6 days, 17:12:29, max mem: 15.1 GB 
[06/17 01:10:52][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0232,	1.1681 s / batch. (data: 2.87e-04). ETA=6 days, 16:50:34, max mem: 15.1 GB 
[06/17 01:12:49][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0272,	1.1723 s / batch. (data: 3.68e-04). ETA=6 days, 17:23:04, max mem: 15.1 GB 
[06/17 01:14:47][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0251,	1.1796 s / batch. (data: 3.20e-04). ETA=6 days, 18:21:07, max mem: 15.1 GB 
[06/17 01:16:44][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0280,	1.1767 s / batch. (data: 1.07e-04). ETA=6 days, 17:55:22, max mem: 15.1 GB 
[06/17 01:16:50][INFO] visual_prompt:  319: Epoch 1 / 100: avg data time: 4.32e-03, avg batch time: 1.1785, average train loss: 0.0242
[06/17 01:25:47][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.012, 5.1816 s / batch. (data: 1.18e-04)max mem: 15.06516 GB 
[06/17 01:34:05][INFO] visual_prompt:  476: Inference (val):avg data time: 1.46e-04, avg batch time: 5.1806, average loss: 0.0119
[06/17 01:34:05][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/17 01:34:05][INFO] visual_prompt:  257: Training 2 / 100 epoch, with learning rate 0.1
[06/17 01:36:49][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0152,	1.1835 s / batch. (data: 4.22e-04). ETA=6 days, 18:49:42, max mem: 15.1 GB 
[06/17 01:38:46][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0133,	1.1744 s / batch. (data: 3.10e-04). ETA=6 days, 17:32:26, max mem: 15.1 GB 
[06/17 01:40:44][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0114,	1.1763 s / batch. (data: 3.27e-04). ETA=6 days, 17:46:29, max mem: 15.1 GB 
[06/17 01:42:41][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0084,	1.1751 s / batch. (data: 3.14e-04). ETA=6 days, 17:34:17, max mem: 15.1 GB 
[06/17 01:44:39][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0074,	1.1769 s / batch. (data: 3.51e-04). ETA=6 days, 17:47:40, max mem: 15.1 GB 
[06/17 01:46:36][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0076,	1.1726 s / batch. (data: 2.87e-04). ETA=6 days, 17:09:50, max mem: 15.1 GB 
[06/17 01:48:34][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0061,	1.1746 s / batch. (data: 3.90e-04). ETA=6 days, 17:24:31, max mem: 15.1 GB 
[06/17 01:50:31][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0069,	1.1770 s / batch. (data: 4.22e-04). ETA=6 days, 17:42:02, max mem: 15.1 GB 
[06/17 01:52:28][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0060,	1.1733 s / batch. (data: 2.85e-04). ETA=6 days, 17:09:38, max mem: 15.1 GB 
[06/17 01:54:26][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0055,	1.1702 s / batch. (data: 3.03e-04). ETA=6 days, 16:42:01, max mem: 15.1 GB 
[06/17 01:56:23][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0050,	1.1738 s / batch. (data: 2.95e-04). ETA=6 days, 17:10:15, max mem: 15.1 GB 
[06/17 01:58:20][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0050,	1.1724 s / batch. (data: 2.64e-04). ETA=6 days, 16:56:56, max mem: 15.1 GB 
[06/17 02:00:18][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0047,	1.1764 s / batch. (data: 3.16e-04). ETA=6 days, 17:27:18, max mem: 15.1 GB 
[06/17 02:02:15][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0050,	1.1782 s / batch. (data: 3.25e-04). ETA=6 days, 17:40:04, max mem: 15.1 GB 
[06/17 02:04:12][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0052,	1.1665 s / batch. (data: 2.93e-04). ETA=6 days, 16:02:05, max mem: 15.1 GB 
[06/17 02:06:10][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0040,	1.1726 s / batch. (data: 2.92e-04). ETA=6 days, 16:50:07, max mem: 15.1 GB 
[06/17 02:08:07][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0039,	1.1820 s / batch. (data: 3.72e-04). ETA=6 days, 18:05:24, max mem: 15.1 GB 
[06/17 02:10:05][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0045,	1.1725 s / batch. (data: 2.93e-04). ETA=6 days, 16:45:44, max mem: 15.1 GB 
[06/17 02:12:02][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0039,	1.1739 s / batch. (data: 2.86e-04). ETA=6 days, 16:54:59, max mem: 15.1 GB 
[06/17 02:14:00][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0035,	1.1698 s / batch. (data: 2.78e-04). ETA=6 days, 16:19:43, max mem: 15.1 GB 
[06/17 02:15:57][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0043,	1.1772 s / batch. (data: 3.29e-04). ETA=6 days, 17:18:14, max mem: 15.1 GB 
[06/17 02:17:55][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0030,	1.1721 s / batch. (data: 2.73e-04). ETA=6 days, 16:34:54, max mem: 15.1 GB 
[06/17 02:19:53][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0035,	1.1750 s / batch. (data: 2.66e-04). ETA=6 days, 16:56:37, max mem: 15.1 GB 
[06/17 02:21:50][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0033,	1.1753 s / batch. (data: 3.10e-04). ETA=6 days, 16:57:07, max mem: 15.1 GB 
[06/17 02:23:47][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0034,	1.1799 s / batch. (data: 2.96e-04). ETA=6 days, 17:32:45, max mem: 15.1 GB 
[06/17 02:25:45][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0030,	1.1782 s / batch. (data: 2.95e-04). ETA=6 days, 17:16:31, max mem: 15.1 GB 
[06/17 02:27:42][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0029,	1.1757 s / batch. (data: 3.46e-04). ETA=6 days, 16:54:46, max mem: 15.1 GB 
[06/17 02:29:40][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0033,	1.1697 s / batch. (data: 3.19e-04). ETA=6 days, 16:02:59, max mem: 15.1 GB 
[06/17 02:31:37][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0026,	1.1698 s / batch. (data: 3.16e-04). ETA=6 days, 16:02:20, max mem: 15.1 GB 
[06/17 02:33:35][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0024,	1.1715 s / batch. (data: 2.97e-04). ETA=6 days, 16:14:20, max mem: 15.1 GB 
[06/17 02:35:32][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0023,	1.1751 s / batch. (data: 3.13e-04). ETA=6 days, 16:41:30, max mem: 15.1 GB 
[06/17 02:37:30][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0020,	1.1748 s / batch. (data: 3.59e-04). ETA=6 days, 16:37:24, max mem: 15.1 GB 
[06/17 02:39:27][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0019,	1.1769 s / batch. (data: 3.64e-04). ETA=6 days, 16:52:24, max mem: 15.1 GB 
[06/17 02:41:24][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0015,	1.1746 s / batch. (data: 3.63e-04). ETA=6 days, 16:31:39, max mem: 15.1 GB 
[06/17 02:43:22][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0012,	1.1744 s / batch. (data: 3.45e-04). ETA=6 days, 16:28:21, max mem: 15.1 GB 
[06/17 02:45:19][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0013,	1.1790 s / batch. (data: 3.54e-04). ETA=6 days, 17:04:09, max mem: 15.1 GB 
[06/17 02:47:17][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0009,	1.1769 s / batch. (data: 2.59e-04). ETA=6 days, 16:44:57, max mem: 15.1 GB 
[06/17 02:49:14][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0011,	1.1745 s / batch. (data: 2.71e-04). ETA=6 days, 16:22:42, max mem: 15.1 GB 
[06/17 02:51:12][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0010,	1.1782 s / batch. (data: 2.74e-04). ETA=6 days, 16:51:43, max mem: 15.1 GB 
[06/17 02:53:09][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0009,	1.1758 s / batch. (data: 3.25e-04). ETA=6 days, 16:29:34, max mem: 15.1 GB 
[06/17 02:55:07][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0008,	1.1758 s / batch. (data: 3.29e-04). ETA=6 days, 16:27:28, max mem: 15.1 GB 
[06/17 02:57:05][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0007,	1.1742 s / batch. (data: 3.09e-04). ETA=6 days, 16:12:24, max mem: 15.1 GB 
[06/17 02:59:02][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0007,	1.1747 s / batch. (data: 4.21e-04). ETA=6 days, 16:15:03, max mem: 15.1 GB 
[06/17 03:01:00][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0009,	1.1727 s / batch. (data: 3.31e-04). ETA=6 days, 15:56:15, max mem: 15.1 GB 
[06/17 03:02:57][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0008,	1.1763 s / batch. (data: 3.45e-04). ETA=6 days, 16:24:01, max mem: 15.1 GB 
[06/17 03:04:55][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0008,	1.1770 s / batch. (data: 4.25e-04). ETA=6 days, 16:28:02, max mem: 15.1 GB 
[06/17 03:06:52][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0006,	1.1748 s / batch. (data: 4.10e-04). ETA=6 days, 16:07:43, max mem: 15.1 GB 
[06/17 03:08:49][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0008,	1.1769 s / batch. (data: 3.52e-04). ETA=6 days, 16:23:11, max mem: 15.1 GB 
[06/17 03:10:47][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0009,	1.1751 s / batch. (data: 2.96e-04). ETA=6 days, 16:06:07, max mem: 15.1 GB 
[06/17 03:12:44][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0006,	1.1702 s / batch. (data: 1.14e-04). ETA=6 days, 15:24:16, max mem: 15.1 GB 
[06/17 03:12:50][INFO] visual_prompt:  319: Epoch 2 / 100: avg data time: 4.36e-03, avg batch time: 1.1839, average train loss: 0.0039
[06/17 03:21:46][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.2105 s / batch. (data: 1.10e-04)max mem: 15.06516 GB 
[06/17 03:30:04][INFO] visual_prompt:  476: Inference (val):avg data time: 1.59e-04, avg batch time: 5.1746, average loss: 0.0003
[06/17 03:30:04][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/17 03:30:04][INFO] visual_prompt:  257: Training 3 / 100 epoch, with learning rate 0.2
[06/17 03:32:42][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0023,	1.1769 s / batch. (data: 3.15e-04). ETA=6 days, 16:16:44, max mem: 15.1 GB 
[06/17 03:34:40][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0050,	1.1708 s / batch. (data: 3.27e-04). ETA=6 days, 15:24:56, max mem: 15.1 GB 
[06/17 03:36:37][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0006,	1.1751 s / batch. (data: 3.61e-04). ETA=6 days, 15:58:32, max mem: 15.1 GB 
[06/17 03:38:34][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0006,	1.1705 s / batch. (data: 2.99e-04). ETA=6 days, 15:18:55, max mem: 15.1 GB 
[06/17 03:40:32][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0006,	1.1729 s / batch. (data: 2.91e-04). ETA=6 days, 15:36:35, max mem: 15.1 GB 
[06/17 03:42:29][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0023,	1.1687 s / batch. (data: 3.35e-04). ETA=6 days, 15:00:24, max mem: 15.1 GB 
[06/17 03:44:27][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0006,	1.1743 s / batch. (data: 3.42e-04). ETA=6 days, 15:43:55, max mem: 15.1 GB 
[06/17 03:46:24][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0005,	1.1740 s / batch. (data: 3.15e-04). ETA=6 days, 15:39:22, max mem: 15.1 GB 
[06/17 03:48:21][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0005,	1.1731 s / batch. (data: 2.97e-04). ETA=6 days, 15:30:37, max mem: 15.1 GB 
[06/17 03:50:19][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0005,	1.1763 s / batch. (data: 3.30e-04). ETA=6 days, 15:54:19, max mem: 15.1 GB 
[06/17 03:52:16][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0016,	1.1735 s / batch. (data: 3.84e-04). ETA=6 days, 15:29:23, max mem: 15.1 GB 
[06/17 03:54:13][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0006,	1.1739 s / batch. (data: 2.81e-04). ETA=6 days, 15:30:39, max mem: 15.1 GB 
[06/17 03:56:11][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0005,	1.1726 s / batch. (data: 2.69e-04). ETA=6 days, 15:18:37, max mem: 15.1 GB 
[06/17 03:58:08][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0005,	1.1737 s / batch. (data: 2.79e-04). ETA=6 days, 15:25:27, max mem: 15.1 GB 
[06/17 04:00:06][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0005,	1.1778 s / batch. (data: 2.47e-04). ETA=6 days, 15:56:38, max mem: 15.1 GB 
[06/17 04:02:03][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0006,	1.1766 s / batch. (data: 3.29e-04). ETA=6 days, 15:45:23, max mem: 15.1 GB 
[06/17 04:04:01][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0016,	1.1747 s / batch. (data: 3.23e-04). ETA=6 days, 15:27:43, max mem: 15.1 GB 
[06/17 04:05:58][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0016,	1.1727 s / batch. (data: 2.73e-04). ETA=6 days, 15:09:47, max mem: 15.1 GB 
[06/17 04:07:56][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0006,	1.1769 s / batch. (data: 3.12e-04). ETA=6 days, 15:41:37, max mem: 15.1 GB 
[06/17 04:09:53][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0005,	1.1719 s / batch. (data: 3.31e-04). ETA=6 days, 14:59:02, max mem: 15.1 GB 
[06/17 04:11:50][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0014,	1.1739 s / batch. (data: 3.36e-04). ETA=6 days, 15:13:17, max mem: 15.1 GB 
[06/17 04:13:48][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0005,	1.1696 s / batch. (data: 3.29e-04). ETA=6 days, 14:36:46, max mem: 15.1 GB 
[06/17 04:15:45][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0010,	1.1768 s / batch. (data: 3.31e-04). ETA=6 days, 15:33:02, max mem: 15.1 GB 
[06/17 04:17:43][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0007,	1.1781 s / batch. (data: 3.99e-04). ETA=6 days, 15:41:19, max mem: 15.1 GB 
[06/17 04:19:40][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0012,	1.1754 s / batch. (data: 3.41e-04). ETA=6 days, 15:17:50, max mem: 15.1 GB 
[06/17 04:21:38][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0005,	1.1716 s / batch. (data: 3.50e-04). ETA=6 days, 14:44:37, max mem: 15.1 GB 
[06/17 04:23:35][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0015,	1.1748 s / batch. (data: 3.35e-04). ETA=6 days, 15:09:14, max mem: 15.1 GB 
[06/17 04:25:32][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0011,	1.1734 s / batch. (data: 3.70e-04). ETA=6 days, 14:55:55, max mem: 15.1 GB 
[06/17 04:27:30][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0005,	1.1757 s / batch. (data: 4.33e-04). ETA=6 days, 15:12:04, max mem: 15.1 GB 
[06/17 04:29:27][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0017,	1.1682 s / batch. (data: 3.17e-04). ETA=6 days, 14:09:43, max mem: 15.1 GB 
[06/17 04:31:25][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0032,	1.1809 s / batch. (data: 3.13e-04). ETA=6 days, 15:50:42, max mem: 15.1 GB 
[06/17 04:33:22][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0005,	1.1784 s / batch. (data: 3.61e-04). ETA=6 days, 15:28:03, max mem: 15.1 GB 
[06/17 04:35:20][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0009,	1.1678 s / batch. (data: 3.60e-04). ETA=6 days, 14:00:13, max mem: 15.1 GB 
[06/17 04:37:17][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0007,	1.1739 s / batch. (data: 3.11e-04). ETA=6 days, 14:48:03, max mem: 15.1 GB 
[06/17 04:39:14][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0004,	1.1771 s / batch. (data: 3.73e-04). ETA=6 days, 15:12:05, max mem: 15.1 GB 
[06/17 04:41:12][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0016,	1.1728 s / batch. (data: 2.73e-04). ETA=6 days, 14:35:28, max mem: 15.1 GB 
[06/17 04:43:09][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0026,	1.1772 s / batch. (data: 3.57e-04). ETA=6 days, 15:08:41, max mem: 15.1 GB 
[06/17 04:45:07][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0021,	1.1739 s / batch. (data: 3.41e-04). ETA=6 days, 14:39:53, max mem: 15.1 GB 
[06/17 04:47:04][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0017,	1.1752 s / batch. (data: 3.45e-04). ETA=6 days, 14:48:27, max mem: 15.1 GB 
[06/17 04:49:02][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0005,	1.1755 s / batch. (data: 3.13e-04). ETA=6 days, 14:49:33, max mem: 15.1 GB 
[06/17 04:50:59][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0015,	1.1752 s / batch. (data: 3.33e-04). ETA=6 days, 14:45:09, max mem: 15.1 GB 
[06/17 04:52:57][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0015,	1.1737 s / batch. (data: 3.30e-04). ETA=6 days, 14:30:44, max mem: 15.1 GB 
[06/17 04:54:54][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0023,	1.1673 s / batch. (data: 2.65e-04). ETA=6 days, 13:37:04, max mem: 15.1 GB 
[06/17 04:56:51][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0021,	1.1737 s / batch. (data: 3.56e-04). ETA=6 days, 14:27:05, max mem: 15.1 GB 
[06/17 04:58:49][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0087,	1.1734 s / batch. (data: 3.66e-04). ETA=6 days, 14:22:17, max mem: 15.1 GB 
[06/17 05:00:46][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0006,	1.1706 s / batch. (data: 4.91e-04). ETA=6 days, 13:57:31, max mem: 15.1 GB 
[06/17 05:02:44][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0020,	1.1812 s / batch. (data: 3.60e-04). ETA=6 days, 15:21:29, max mem: 15.1 GB 
[06/17 05:04:41][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0005,	1.1707 s / batch. (data: 2.97e-04). ETA=6 days, 13:54:20, max mem: 15.1 GB 
[06/17 05:06:39][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0005,	1.1794 s / batch. (data: 3.29e-04). ETA=6 days, 15:03:16, max mem: 15.1 GB 
[06/17 05:08:36][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0021,	1.1734 s / batch. (data: 1.28e-04). ETA=6 days, 14:13:02, max mem: 15.1 GB 
[06/17 05:08:42][INFO] visual_prompt:  319: Epoch 3 / 100: avg data time: 4.36e-03, avg batch time: 1.1824, average train loss: 0.0012
[06/17 05:17:36][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1833 s / batch. (data: 1.97e-04)max mem: 15.06516 GB 
[06/17 05:25:52][INFO] visual_prompt:  476: Inference (val):avg data time: 1.47e-04, avg batch time: 5.1648, average loss: 0.0002
[06/17 05:25:52][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/17 05:25:52][INFO] visual_prompt:  257: Training 4 / 100 epoch, with learning rate 0.3
[06/17 05:28:33][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0016,	1.1749 s / batch. (data: 3.18e-04). ETA=6 days, 14:22:39, max mem: 15.1 GB 
[06/17 05:30:30][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0006,	1.1737 s / batch. (data: 2.90e-04). ETA=6 days, 14:10:59, max mem: 15.1 GB 
[06/17 05:32:28][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0007,	1.1736 s / batch. (data: 3.93e-04). ETA=6 days, 14:08:21, max mem: 15.1 GB 
[06/17 05:34:25][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0005,	1.1689 s / batch. (data: 3.57e-04). ETA=6 days, 13:28:03, max mem: 15.1 GB 
[06/17 05:36:22][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0012,	1.1706 s / batch. (data: 3.09e-04). ETA=6 days, 13:40:20, max mem: 15.1 GB 
[06/17 05:38:20][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0005,	1.1776 s / batch. (data: 2.52e-04). ETA=6 days, 14:35:02, max mem: 15.1 GB 
[06/17 05:40:17][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0006,	1.1733 s / batch. (data: 3.17e-04). ETA=6 days, 13:58:27, max mem: 15.1 GB 
[06/17 05:42:15][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0020,	1.1711 s / batch. (data: 3.20e-04). ETA=6 days, 13:38:29, max mem: 15.1 GB 
[06/17 05:44:12][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0011,	1.1776 s / batch. (data: 3.57e-04). ETA=6 days, 14:28:30, max mem: 15.1 GB 
[06/17 05:46:10][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0013,	1.1730 s / batch. (data: 3.35e-04). ETA=6 days, 13:49:28, max mem: 15.1 GB 
[06/17 05:48:08][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0015,	1.1733 s / batch. (data: 3.40e-04). ETA=6 days, 13:50:04, max mem: 15.1 GB 
[06/17 05:50:05][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0007,	1.1719 s / batch. (data: 2.58e-04). ETA=6 days, 13:36:46, max mem: 15.1 GB 
[06/17 05:52:03][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0025,	1.1732 s / batch. (data: 2.92e-04). ETA=6 days, 13:45:40, max mem: 15.1 GB 
[06/17 05:54:00][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0013,	1.1756 s / batch. (data: 3.58e-04). ETA=6 days, 14:02:44, max mem: 15.1 GB 
[06/17 05:55:58][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0033,	1.1735 s / batch. (data: 2.95e-04). ETA=6 days, 13:43:43, max mem: 15.1 GB 
[06/17 05:57:55][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0009,	1.1766 s / batch. (data: 3.37e-04). ETA=6 days, 14:06:58, max mem: 15.1 GB 
[06/17 05:59:52][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0008,	1.1746 s / batch. (data: 2.58e-04). ETA=6 days, 13:49:14, max mem: 15.1 GB 
[06/17 06:01:50][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0013,	1.1775 s / batch. (data: 2.99e-04). ETA=6 days, 14:10:24, max mem: 15.1 GB 
[06/17 06:03:47][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0015,	1.1728 s / batch. (data: 2.99e-04). ETA=6 days, 13:30:33, max mem: 15.1 GB 
[06/17 06:05:45][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0009,	1.1667 s / batch. (data: 2.91e-04). ETA=6 days, 12:39:08, max mem: 15.1 GB 
[06/17 06:07:42][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0021,	1.1776 s / batch. (data: 3.08e-04). ETA=6 days, 14:05:34, max mem: 15.1 GB 
[06/17 06:09:40][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0010,	1.1742 s / batch. (data: 2.88e-04). ETA=6 days, 13:36:12, max mem: 15.1 GB 
[06/17 06:11:37][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0009,	1.1747 s / batch. (data: 3.08e-04). ETA=6 days, 13:38:17, max mem: 15.1 GB 
[06/17 06:13:35][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0011,	1.1738 s / batch. (data: 3.24e-04). ETA=6 days, 13:29:14, max mem: 15.1 GB 
[06/17 06:15:32][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0007,	1.1753 s / batch. (data: 3.35e-04). ETA=6 days, 13:39:18, max mem: 15.1 GB 
[06/17 06:17:30][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0008,	1.1721 s / batch. (data: 4.03e-04). ETA=6 days, 13:11:22, max mem: 15.1 GB 
[06/17 06:19:27][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0013,	1.1785 s / batch. (data: 4.47e-04). ETA=6 days, 14:00:38, max mem: 15.1 GB 
[06/17 06:21:25][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0017,	1.1698 s / batch. (data: 4.65e-04). ETA=6 days, 12:49:02, max mem: 15.1 GB 
[06/17 06:23:22][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0010,	1.1733 s / batch. (data: 3.60e-04). ETA=6 days, 13:14:45, max mem: 15.1 GB 
[06/17 06:25:20][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0006,	1.1714 s / batch. (data: 2.70e-04). ETA=6 days, 12:57:51, max mem: 15.1 GB 
[06/17 06:27:17][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0006,	1.1741 s / batch. (data: 4.30e-04). ETA=6 days, 13:17:42, max mem: 15.1 GB 
[06/17 06:29:14][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0007,	1.1760 s / batch. (data: 3.81e-04). ETA=6 days, 13:30:40, max mem: 15.1 GB 
[06/17 06:31:12][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0010,	1.1738 s / batch. (data: 3.10e-04). ETA=6 days, 13:10:55, max mem: 15.1 GB 
[06/17 06:33:09][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0006,	1.1740 s / batch. (data: 2.61e-04). ETA=6 days, 13:11:08, max mem: 15.1 GB 
[06/17 06:35:07][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0007,	1.1778 s / batch. (data: 3.46e-04). ETA=6 days, 13:39:16, max mem: 15.1 GB 
[06/17 06:37:04][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0008,	1.1769 s / batch. (data: 2.80e-04). ETA=6 days, 13:30:35, max mem: 15.1 GB 
[06/17 06:39:02][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0008,	1.1766 s / batch. (data: 2.74e-04). ETA=6 days, 13:25:52, max mem: 15.1 GB 
[06/17 06:40:59][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0011,	1.1776 s / batch. (data: 2.85e-04). ETA=6 days, 13:32:12, max mem: 15.1 GB 
[06/17 06:42:57][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0018,	1.1722 s / batch. (data: 3.10e-04). ETA=6 days, 12:46:48, max mem: 15.1 GB 
[06/17 06:44:54][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0007,	1.1728 s / batch. (data: 3.74e-04). ETA=6 days, 12:49:20, max mem: 15.1 GB 
[06/17 06:46:52][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0006,	1.1737 s / batch. (data: 3.21e-04). ETA=6 days, 12:54:38, max mem: 15.1 GB 
[06/17 06:48:49][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0009,	1.1747 s / batch. (data: 3.84e-04). ETA=6 days, 13:00:42, max mem: 15.1 GB 
[06/17 06:50:46][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0017,	1.1771 s / batch. (data: 3.36e-04). ETA=6 days, 13:18:22, max mem: 15.1 GB 
[06/17 06:52:44][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0012,	1.1739 s / batch. (data: 3.47e-04). ETA=6 days, 12:50:50, max mem: 15.1 GB 
[06/17 06:54:41][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0010,	1.1759 s / batch. (data: 2.91e-04). ETA=6 days, 13:04:52, max mem: 15.1 GB 
[06/17 06:56:39][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0010,	1.1710 s / batch. (data: 3.25e-04). ETA=6 days, 12:23:23, max mem: 15.1 GB 
[06/17 06:58:36][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0009,	1.1837 s / batch. (data: 3.11e-04). ETA=6 days, 14:03:17, max mem: 15.1 GB 
[06/17 07:00:34][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0007,	1.1702 s / batch. (data: 3.78e-04). ETA=6 days, 12:12:54, max mem: 15.1 GB 
[06/17 07:02:31][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0007,	1.1732 s / batch. (data: 3.14e-04). ETA=6 days, 12:35:27, max mem: 15.1 GB 
[06/17 07:04:29][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0012,	1.1704 s / batch. (data: 1.34e-04). ETA=6 days, 12:10:41, max mem: 15.1 GB 
[06/17 07:04:35][INFO] visual_prompt:  319: Epoch 4 / 100: avg data time: 4.35e-03, avg batch time: 1.1833, average train loss: 0.0013
[06/17 07:13:30][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1906 s / batch. (data: 1.95e-04)max mem: 15.06516 GB 
[06/17 07:21:47][INFO] visual_prompt:  476: Inference (val):avg data time: 1.49e-04, avg batch time: 5.1686, average loss: 0.0002
[06/17 07:21:47][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/17 07:21:47][INFO] visual_prompt:  257: Training 5 / 100 epoch, with learning rate 0.4
[06/17 07:24:26][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0009,	1.1755 s / batch. (data: 3.35e-04). ETA=6 days, 12:49:47, max mem: 15.1 GB 
[06/17 07:26:23][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0008,	1.1742 s / batch. (data: 3.96e-04). ETA=6 days, 12:36:59, max mem: 15.1 GB 
[06/17 07:28:20][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0012,	1.1743 s / batch. (data: 3.28e-04). ETA=6 days, 12:35:46, max mem: 15.1 GB 
[06/17 07:30:18][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0008,	1.1771 s / batch. (data: 2.71e-04). ETA=6 days, 12:56:09, max mem: 15.1 GB 
[06/17 07:32:15][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0008,	1.1728 s / batch. (data: 2.81e-04). ETA=6 days, 12:20:30, max mem: 15.1 GB 
[06/17 07:34:13][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0007,	1.1728 s / batch. (data: 3.21e-04). ETA=6 days, 12:18:26, max mem: 15.1 GB 
[06/17 07:36:10][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0008,	1.1752 s / batch. (data: 2.75e-04). ETA=6 days, 12:35:15, max mem: 15.1 GB 
[06/17 07:38:08][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0019,	1.1703 s / batch. (data: 3.77e-04). ETA=6 days, 11:54:08, max mem: 15.1 GB 
[06/17 07:40:05][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0008,	1.1832 s / batch. (data: 3.97e-04). ETA=6 days, 13:35:47, max mem: 15.1 GB 
[06/17 07:42:03][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0007,	1.1747 s / batch. (data: 2.90e-04). ETA=6 days, 12:25:11, max mem: 15.1 GB 
[06/17 07:44:00][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0007,	1.1735 s / batch. (data: 2.79e-04). ETA=6 days, 12:14:15, max mem: 15.1 GB 
[06/17 07:45:57][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0018,	1.1738 s / batch. (data: 3.09e-04). ETA=6 days, 12:14:22, max mem: 15.1 GB 
[06/17 07:47:55][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0007,	1.1759 s / batch. (data: 3.56e-04). ETA=6 days, 12:29:21, max mem: 15.1 GB 
[06/17 07:49:52][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0007,	1.1774 s / batch. (data: 2.57e-04). ETA=6 days, 12:38:59, max mem: 15.1 GB 
[06/17 07:51:50][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0008,	1.1758 s / batch. (data: 3.13e-04). ETA=6 days, 12:24:08, max mem: 15.1 GB 
[06/17 07:53:47][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0008,	1.1748 s / batch. (data: 3.45e-04). ETA=6 days, 12:14:20, max mem: 15.1 GB 
[06/17 07:55:45][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0007,	1.1723 s / batch. (data: 3.80e-04). ETA=6 days, 11:52:36, max mem: 15.1 GB 
[06/17 07:57:42][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0008,	1.1719 s / batch. (data: 3.10e-04). ETA=6 days, 11:47:22, max mem: 15.1 GB 
[06/17 07:59:40][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0009,	1.1763 s / batch. (data: 2.63e-04). ETA=6 days, 12:20:58, max mem: 15.1 GB 
[06/17 08:01:37][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0011,	1.1771 s / batch. (data: 3.15e-04). ETA=6 days, 12:24:58, max mem: 15.1 GB 
[06/17 08:03:35][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0008,	1.1789 s / batch. (data: 3.19e-04). ETA=6 days, 12:37:06, max mem: 15.1 GB 
[06/17 08:05:33][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0027,	1.1772 s / batch. (data: 3.50e-04). ETA=6 days, 12:22:08, max mem: 15.1 GB 
[06/17 08:07:31][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0004,	1.1752 s / batch. (data: 2.98e-04). ETA=6 days, 12:04:02, max mem: 15.1 GB 
[06/17 08:09:28][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0008,	1.1741 s / batch. (data: 3.32e-04). ETA=6 days, 11:53:15, max mem: 15.1 GB 
[06/17 08:11:25][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0024,	1.1750 s / batch. (data: 2.71e-04). ETA=6 days, 11:58:21, max mem: 15.1 GB 
[06/17 08:13:23][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0007,	1.1714 s / batch. (data: 3.06e-04). ETA=6 days, 11:28:08, max mem: 15.1 GB 
[06/17 08:15:21][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0008,	1.1810 s / batch. (data: 3.74e-04). ETA=6 days, 12:42:08, max mem: 15.1 GB 
[06/17 08:17:18][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0015,	1.1784 s / batch. (data: 2.64e-04). ETA=6 days, 12:19:49, max mem: 15.1 GB 
[06/17 08:19:16][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0007,	1.1809 s / batch. (data: 2.72e-04). ETA=6 days, 12:37:36, max mem: 15.1 GB 
[06/17 08:21:14][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0013,	1.1773 s / batch. (data: 3.20e-04). ETA=6 days, 12:07:01, max mem: 15.1 GB 
[06/17 08:23:11][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0006,	1.1759 s / batch. (data: 3.55e-04). ETA=6 days, 11:53:54, max mem: 15.1 GB 
[06/17 08:25:08][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0006,	1.1728 s / batch. (data: 3.09e-04). ETA=6 days, 11:27:05, max mem: 15.1 GB 
[06/17 08:27:06][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0007,	1.1725 s / batch. (data: 2.85e-04). ETA=6 days, 11:22:48, max mem: 15.1 GB 
[06/17 08:29:03][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0007,	1.1759 s / batch. (data: 2.61e-04). ETA=6 days, 11:48:27, max mem: 15.1 GB 
[06/17 08:31:00][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0007,	1.1731 s / batch. (data: 3.07e-04). ETA=6 days, 11:24:01, max mem: 15.1 GB 
[06/17 08:32:58][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0006,	1.1756 s / batch. (data: 3.62e-04). ETA=6 days, 11:42:00, max mem: 15.1 GB 
[06/17 08:34:55][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0039,	1.1758 s / batch. (data: 4.37e-04). ETA=6 days, 11:41:19, max mem: 15.1 GB 
[06/17 08:36:53][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0012,	1.1737 s / batch. (data: 2.78e-04). ETA=6 days, 11:22:41, max mem: 15.1 GB 
[06/17 08:38:50][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0020,	1.1748 s / batch. (data: 3.48e-04). ETA=6 days, 11:29:17, max mem: 15.1 GB 
[06/17 08:40:48][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0007,	1.1701 s / batch. (data: 2.76e-04). ETA=6 days, 10:50:04, max mem: 15.1 GB 
[06/17 08:42:45][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0008,	1.1723 s / batch. (data: 3.67e-04). ETA=6 days, 11:05:39, max mem: 15.1 GB 
[06/17 08:44:42][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0007,	1.1728 s / batch. (data: 3.36e-04). ETA=6 days, 11:08:05, max mem: 15.1 GB 
[06/17 08:46:40][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0001,	1.1755 s / batch. (data: 2.90e-04). ETA=6 days, 11:27:21, max mem: 15.1 GB 
[06/17 08:48:37][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0006,	1.1733 s / batch. (data: 3.43e-04). ETA=6 days, 11:07:41, max mem: 15.1 GB 
[06/17 08:50:34][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0007,	1.1729 s / batch. (data: 3.14e-04). ETA=6 days, 11:03:06, max mem: 15.1 GB 
[06/17 08:52:32][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0015,	1.1703 s / batch. (data: 2.83e-04). ETA=6 days, 10:39:50, max mem: 15.1 GB 
[06/17 08:54:29][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0012,	1.1729 s / batch. (data: 3.65e-04). ETA=6 days, 10:58:38, max mem: 15.1 GB 
[06/17 08:56:26][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0001,	1.1774 s / batch. (data: 3.43e-04). ETA=6 days, 11:32:45, max mem: 15.1 GB 
[06/17 08:58:24][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0003,	1.1703 s / batch. (data: 3.10e-04). ETA=6 days, 10:34:31, max mem: 15.1 GB 
[06/17 09:00:21][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0006,	1.1677 s / batch. (data: 1.44e-04). ETA=6 days, 10:11:27, max mem: 15.1 GB 
[06/17 09:00:27][INFO] visual_prompt:  319: Epoch 5 / 100: avg data time: 4.41e-03, avg batch time: 1.1829, average train loss: 0.0010
[06/17 09:09:21][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1880 s / batch. (data: 1.94e-04)max mem: 15.06516 GB 
[06/17 09:17:37][INFO] visual_prompt:  476: Inference (val):avg data time: 1.45e-04, avg batch time: 5.1616, average loss: 0.0002
[06/17 09:17:37][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/17 09:17:37][INFO] visual_prompt:  257: Training 6 / 100 epoch, with learning rate 0.5
[06/17 09:20:18][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0008,	1.1736 s / batch. (data: 3.76e-04). ETA=6 days, 10:56:44, max mem: 15.1 GB 
[06/17 09:22:16][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0008,	1.1688 s / batch. (data: 3.52e-04). ETA=6 days, 10:16:32, max mem: 15.1 GB 
[06/17 09:24:13][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0009,	1.1766 s / batch. (data: 2.93e-04). ETA=6 days, 11:16:20, max mem: 15.1 GB 
[06/17 09:26:11][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0021,	1.1667 s / batch. (data: 2.70e-04). ETA=6 days, 9:55:45, max mem: 15.1 GB 
[06/17 09:28:08][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0009,	1.1722 s / batch. (data: 3.73e-04). ETA=6 days, 10:37:49, max mem: 15.1 GB 
[06/17 09:30:05][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0007,	1.1729 s / batch. (data: 3.82e-04). ETA=6 days, 10:41:22, max mem: 15.1 GB 
[06/17 09:32:03][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0009,	1.1785 s / batch. (data: 3.76e-04). ETA=6 days, 11:23:25, max mem: 15.1 GB 
[06/17 09:34:00][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0008,	1.1741 s / batch. (data: 3.73e-04). ETA=6 days, 10:46:29, max mem: 15.1 GB 
[06/17 09:35:58][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0020,	1.1707 s / batch. (data: 3.44e-04). ETA=6 days, 10:18:04, max mem: 15.1 GB 
[06/17 09:37:55][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0021,	1.1732 s / batch. (data: 3.20e-04). ETA=6 days, 10:35:53, max mem: 15.1 GB 
[06/17 09:39:53][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0008,	1.1794 s / batch. (data: 3.63e-04). ETA=6 days, 11:22:45, max mem: 15.1 GB 
[06/17 09:41:50][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0001,	1.1723 s / batch. (data: 2.56e-04). ETA=6 days, 10:24:38, max mem: 15.1 GB 
[06/17 09:43:48][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0008,	1.1764 s / batch. (data: 3.35e-04). ETA=6 days, 10:55:04, max mem: 15.1 GB 
[06/17 09:45:45][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0001,	1.1735 s / batch. (data: 3.47e-04). ETA=6 days, 10:30:03, max mem: 15.1 GB 
[06/17 09:47:42][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0010,	1.1729 s / batch. (data: 2.82e-04). ETA=6 days, 10:23:48, max mem: 15.1 GB 
[06/17 09:49:40][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0008,	1.1724 s / batch. (data: 3.36e-04). ETA=6 days, 10:17:59, max mem: 15.1 GB 
[06/17 09:51:37][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0009,	1.1715 s / batch. (data: 3.23e-04). ETA=6 days, 10:08:30, max mem: 15.1 GB 
[06/17 09:53:35][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0016,	1.1759 s / batch. (data: 3.22e-04). ETA=6 days, 10:41:32, max mem: 15.1 GB 
[06/17 09:55:32][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0007,	1.1763 s / batch. (data: 3.36e-04). ETA=6 days, 10:42:29, max mem: 15.1 GB 
[06/17 09:57:29][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0008,	1.1749 s / batch. (data: 3.25e-04). ETA=6 days, 10:29:17, max mem: 15.1 GB 
[06/17 09:59:27][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0007,	1.1756 s / batch. (data: 3.12e-04). ETA=6 days, 10:33:22, max mem: 15.1 GB 
[06/17 10:01:24][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0007,	1.1753 s / batch. (data: 3.27e-04). ETA=6 days, 10:28:29, max mem: 15.1 GB 
[06/17 10:03:21][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0011,	1.1717 s / batch. (data: 3.29e-04). ETA=6 days, 9:58:49, max mem: 15.1 GB 
[06/17 10:05:19][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0009,	1.1693 s / batch. (data: 3.15e-04). ETA=6 days, 9:37:28, max mem: 15.1 GB 
[06/17 10:07:16][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0007,	1.1725 s / batch. (data: 3.38e-04). ETA=6 days, 10:00:54, max mem: 15.1 GB 
[06/17 10:09:13][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0009,	1.1728 s / batch. (data: 3.49e-04). ETA=6 days, 10:00:54, max mem: 15.1 GB 
[06/17 10:11:10][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0025,	1.1715 s / batch. (data: 2.63e-04). ETA=6 days, 9:49:16, max mem: 15.1 GB 
[06/17 10:13:08][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0001,	1.1679 s / batch. (data: 2.85e-04). ETA=6 days, 9:19:05, max mem: 15.1 GB 
[06/17 10:15:05][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0009,	1.1757 s / batch. (data: 3.24e-04). ETA=6 days, 10:18:34, max mem: 15.1 GB 
[06/17 10:17:02][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0001,	1.1809 s / batch. (data: 3.17e-04). ETA=6 days, 10:57:14, max mem: 15.1 GB 
[06/17 10:19:00][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0001,	1.1709 s / batch. (data: 3.07e-04). ETA=6 days, 9:36:36, max mem: 15.1 GB 
[06/17 10:20:57][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0011,	1.1715 s / batch. (data: 2.85e-04). ETA=6 days, 9:39:32, max mem: 15.1 GB 
[06/17 10:22:54][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0009,	1.1726 s / batch. (data: 3.45e-04). ETA=6 days, 9:46:23, max mem: 15.1 GB 
[06/17 10:24:52][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0002,	1.1762 s / batch. (data: 4.47e-04). ETA=6 days, 10:12:03, max mem: 15.1 GB 
[06/17 10:26:49][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0002,	1.1709 s / batch. (data: 3.52e-04). ETA=6 days, 9:28:48, max mem: 15.1 GB 
[06/17 10:28:46][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0007,	1.1716 s / batch. (data: 3.29e-04). ETA=6 days, 9:32:36, max mem: 15.1 GB 
[06/17 10:30:44][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0008,	1.1720 s / batch. (data: 3.71e-04). ETA=6 days, 9:33:49, max mem: 15.1 GB 
[06/17 10:32:41][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0011,	1.1722 s / batch. (data: 3.26e-04). ETA=6 days, 9:32:56, max mem: 15.1 GB 
[06/17 10:34:39][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0007,	1.1725 s / batch. (data: 4.70e-04). ETA=6 days, 9:33:53, max mem: 15.1 GB 
[06/17 10:36:36][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0007,	1.1703 s / batch. (data: 2.98e-04). ETA=6 days, 9:14:07, max mem: 15.1 GB 
[06/17 10:38:33][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0008,	1.1797 s / batch. (data: 3.01e-04). ETA=6 days, 10:26:15, max mem: 15.1 GB 
[06/17 10:40:30][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0007,	1.1751 s / batch. (data: 2.86e-04). ETA=6 days, 9:47:50, max mem: 15.1 GB 
[06/17 10:42:28][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0008,	1.1756 s / batch. (data: 2.55e-04). ETA=6 days, 9:49:49, max mem: 15.1 GB 
[06/17 10:44:25][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0008,	1.1655 s / batch. (data: 3.10e-04). ETA=6 days, 8:29:08, max mem: 15.1 GB 
[06/17 10:46:23][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0012,	1.1781 s / batch. (data: 3.32e-04). ETA=6 days, 10:05:50, max mem: 15.1 GB 
[06/17 10:48:20][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0008,	1.1765 s / batch. (data: 3.64e-04). ETA=6 days, 9:51:25, max mem: 15.1 GB 
[06/17 10:50:18][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0009,	1.1739 s / batch. (data: 4.22e-04). ETA=6 days, 9:28:53, max mem: 15.1 GB 
[06/17 10:52:15][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0002,	1.1818 s / batch. (data: 3.22e-04). ETA=6 days, 10:28:51, max mem: 15.1 GB 
[06/17 10:54:12][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0008,	1.1747 s / batch. (data: 3.77e-04). ETA=6 days, 9:31:32, max mem: 15.1 GB 
[06/17 10:56:09][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0002,	1.1725 s / batch. (data: 2.16e-04). ETA=6 days, 9:12:02, max mem: 15.1 GB 
[06/17 10:56:15][INFO] visual_prompt:  319: Epoch 6 / 100: avg data time: 4.29e-03, avg batch time: 1.1824, average train loss: 0.0009
[06/17 11:05:11][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1946 s / batch. (data: 1.40e-04)max mem: 15.06516 GB 
[06/17 11:13:26][INFO] visual_prompt:  476: Inference (val):avg data time: 1.37e-04, avg batch time: 5.1639, average loss: 0.0002
[06/17 11:13:26][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/17 11:13:26][INFO] visual_prompt:  257: Training 7 / 100 epoch, with learning rate 0.6
[06/17 11:16:05][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0007,	1.1712 s / batch. (data: 2.41e-04). ETA=6 days, 8:59:32, max mem: 15.1 GB 
[06/17 11:18:02][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0002,	1.1662 s / batch. (data: 3.00e-04). ETA=6 days, 8:18:36, max mem: 15.1 GB 
[06/17 11:19:59][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0008,	1.1755 s / batch. (data: 3.34e-04). ETA=6 days, 9:29:45, max mem: 15.1 GB 
[06/17 11:21:57][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0002,	1.1709 s / batch. (data: 2.92e-04). ETA=6 days, 8:51:15, max mem: 15.1 GB 
[06/17 11:23:54][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0013,	1.1723 s / batch. (data: 3.37e-04). ETA=6 days, 9:00:44, max mem: 15.1 GB 
[06/17 11:25:51][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0002,	1.1746 s / batch. (data: 3.45e-04). ETA=6 days, 9:16:38, max mem: 15.1 GB 
[06/17 11:27:49][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0036,	1.1766 s / batch. (data: 3.84e-04). ETA=6 days, 9:30:36, max mem: 15.1 GB 
[06/17 11:29:46][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0011,	1.1736 s / batch. (data: 3.18e-04). ETA=6 days, 9:04:37, max mem: 15.1 GB 
[06/17 11:31:43][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0005,	1.1723 s / batch. (data: 4.38e-04). ETA=6 days, 8:52:38, max mem: 15.1 GB 
[06/17 11:33:40][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0007,	1.1719 s / batch. (data: 3.11e-04). ETA=6 days, 8:47:47, max mem: 15.1 GB 
[06/17 11:35:38][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0017,	1.1762 s / batch. (data: 3.66e-04). ETA=6 days, 9:19:30, max mem: 15.1 GB 
[06/17 11:37:35][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0003,	1.1743 s / batch. (data: 3.44e-04). ETA=6 days, 9:02:15, max mem: 15.1 GB 
[06/17 11:39:32][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0009,	1.1659 s / batch. (data: 4.52e-04). ETA=6 days, 7:54:36, max mem: 15.1 GB 
[06/17 11:41:29][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0005,	1.1749 s / batch. (data: 3.12e-04). ETA=6 days, 9:03:35, max mem: 15.1 GB 
[06/17 11:43:26][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0004,	1.1685 s / batch. (data: 2.95e-04). ETA=6 days, 8:11:39, max mem: 15.1 GB 
[06/17 11:45:24][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0003,	1.1752 s / batch. (data: 2.73e-04). ETA=6 days, 9:02:07, max mem: 15.1 GB 
[06/17 11:47:21][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0007,	1.1737 s / batch. (data: 3.82e-04). ETA=6 days, 8:47:48, max mem: 15.1 GB 
[06/17 11:49:18][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0007,	1.1737 s / batch. (data: 3.29e-04). ETA=6 days, 8:46:30, max mem: 15.1 GB 
[06/17 11:51:15][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0011,	1.1687 s / batch. (data: 3.86e-04). ETA=6 days, 8:04:45, max mem: 15.1 GB 
[06/17 11:53:13][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0010,	1.1706 s / batch. (data: 3.16e-04). ETA=6 days, 8:18:03, max mem: 15.1 GB 
[06/17 11:55:10][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0008,	1.1754 s / batch. (data: 3.21e-04). ETA=6 days, 8:53:09, max mem: 15.1 GB 
[06/17 11:57:07][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0002,	1.1717 s / batch. (data: 3.75e-04). ETA=6 days, 8:22:32, max mem: 15.1 GB 
[06/17 11:59:05][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0001,	1.1661 s / batch. (data: 3.23e-04). ETA=6 days, 7:36:53, max mem: 15.1 GB 
[06/17 12:01:02][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0035,	1.1732 s / batch. (data: 3.24e-04). ETA=6 days, 8:30:33, max mem: 15.1 GB 
[06/17 12:02:59][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0008,	1.1724 s / batch. (data: 3.04e-04). ETA=6 days, 8:22:33, max mem: 15.1 GB 
[06/17 12:04:56][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0025,	1.1724 s / batch. (data: 3.40e-04). ETA=6 days, 8:20:16, max mem: 15.1 GB 
[06/17 12:06:54][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0001,	1.1709 s / batch. (data: 2.97e-04). ETA=6 days, 8:06:28, max mem: 15.1 GB 
[06/17 12:08:51][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0006,	1.1678 s / batch. (data: 3.38e-04). ETA=6 days, 7:40:38, max mem: 15.1 GB 
[06/17 12:10:48][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0008,	1.1720 s / batch. (data: 3.45e-04). ETA=6 days, 8:11:05, max mem: 15.1 GB 
[06/17 12:12:45][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0002,	1.1699 s / batch. (data: 3.11e-04). ETA=6 days, 7:52:53, max mem: 15.1 GB 
[06/17 12:14:43][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0002,	1.1757 s / batch. (data: 3.09e-04). ETA=6 days, 8:36:21, max mem: 15.1 GB 
[06/17 12:16:40][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0018,	1.1788 s / batch. (data: 3.84e-04). ETA=6 days, 8:58:14, max mem: 15.1 GB 
[06/17 12:18:37][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0008,	1.1752 s / batch. (data: 2.46e-04). ETA=6 days, 8:28:11, max mem: 15.1 GB 
[06/17 12:20:35][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0002,	1.1689 s / batch. (data: 3.03e-04). ETA=6 days, 7:37:42, max mem: 15.1 GB 
[06/17 12:22:32][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0002,	1.1671 s / batch. (data: 3.73e-04). ETA=6 days, 7:21:46, max mem: 15.1 GB 
[06/17 12:24:29][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0007,	1.1731 s / batch. (data: 3.00e-04). ETA=6 days, 8:06:30, max mem: 15.1 GB 
[06/17 12:26:27][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0009,	1.1758 s / batch. (data: 3.80e-04). ETA=6 days, 8:25:15, max mem: 15.1 GB 
[06/17 12:28:24][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0011,	1.1711 s / batch. (data: 3.07e-04). ETA=6 days, 7:46:33, max mem: 15.1 GB 
[06/17 12:30:21][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0020,	1.1706 s / batch. (data: 3.66e-04). ETA=6 days, 7:40:42, max mem: 15.1 GB 
[06/17 12:32:18][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0002,	1.1780 s / batch. (data: 2.90e-04). ETA=6 days, 8:36:19, max mem: 15.1 GB 
[06/17 12:34:16][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0009,	1.1706 s / batch. (data: 2.72e-04). ETA=6 days, 7:37:12, max mem: 15.1 GB 
[06/17 12:36:13][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0008,	1.1720 s / batch. (data: 3.10e-04). ETA=6 days, 7:45:40, max mem: 15.1 GB 
[06/17 12:38:10][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0008,	1.1702 s / batch. (data: 3.20e-04). ETA=6 days, 7:29:39, max mem: 15.1 GB 
[06/17 12:40:07][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0003,	1.1743 s / batch. (data: 3.34e-04). ETA=6 days, 7:59:55, max mem: 15.1 GB 
[06/17 12:42:05][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0002,	1.1663 s / batch. (data: 3.49e-04). ETA=6 days, 6:55:43, max mem: 15.1 GB 
[06/17 12:44:02][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0003,	1.1748 s / batch. (data: 3.45e-04). ETA=6 days, 8:00:00, max mem: 15.1 GB 
[06/17 12:45:59][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0002,	1.1695 s / batch. (data: 3.24e-04). ETA=6 days, 7:17:00, max mem: 15.1 GB 
[06/17 12:47:56][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0002,	1.1676 s / batch. (data: 3.03e-04). ETA=6 days, 6:59:59, max mem: 15.1 GB 
[06/17 12:49:53][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0007,	1.1729 s / batch. (data: 3.24e-04). ETA=6 days, 7:39:24, max mem: 15.1 GB 
[06/17 12:51:50][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0008,	1.1641 s / batch. (data: 1.59e-04). ETA=6 days, 6:28:51, max mem: 15.1 GB 
[06/17 12:51:56][INFO] visual_prompt:  319: Epoch 7 / 100: avg data time: 4.31e-03, avg batch time: 1.1807, average train loss: 0.0009
[06/17 13:00:52][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1966 s / batch. (data: 1.28e-04)max mem: 15.06516 GB 
[06/17 13:09:07][INFO] visual_prompt:  476: Inference (val):avg data time: 1.46e-04, avg batch time: 5.1736, average loss: 0.0002
[06/17 13:09:07][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/17 13:09:07][INFO] visual_prompt:  257: Training 8 / 100 epoch, with learning rate 0.7
[06/17 13:11:46][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0008,	1.1725 s / batch. (data: 4.25e-04). ETA=6 days, 7:32:12, max mem: 15.1 GB 
[06/17 13:13:43][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0003,	1.1696 s / batch. (data: 3.17e-04). ETA=6 days, 7:07:53, max mem: 15.1 GB 
[06/17 13:15:40][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0012,	1.1766 s / batch. (data: 3.46e-04). ETA=6 days, 7:59:56, max mem: 15.1 GB 
[06/17 13:17:38][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0010,	1.1753 s / batch. (data: 3.60e-04). ETA=6 days, 7:47:57, max mem: 15.1 GB 
[06/17 13:19:35][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0001,	1.1717 s / batch. (data: 3.30e-04). ETA=6 days, 7:18:09, max mem: 15.1 GB 
[06/17 13:21:33][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0002,	1.1746 s / batch. (data: 3.26e-04). ETA=6 days, 7:38:29, max mem: 15.1 GB 
[06/17 13:23:31][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0008,	1.1781 s / batch. (data: 4.69e-04). ETA=6 days, 8:03:45, max mem: 15.1 GB 
[06/17 13:25:28][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0016,	1.1727 s / batch. (data: 3.05e-04). ETA=6 days, 7:20:25, max mem: 15.1 GB 
[06/17 13:27:25][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0009,	1.1648 s / batch. (data: 2.95e-04). ETA=6 days, 6:16:39, max mem: 15.1 GB 
[06/17 13:29:22][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0008,	1.1715 s / batch. (data: 3.27e-04). ETA=6 days, 7:06:55, max mem: 15.1 GB 
[06/17 13:31:19][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0001,	1.1727 s / batch. (data: 3.32e-04). ETA=6 days, 7:14:30, max mem: 15.1 GB 
[06/17 13:33:17][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0009,	1.1709 s / batch. (data: 3.75e-04). ETA=6 days, 6:58:25, max mem: 15.1 GB 
[06/17 13:35:14][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0002,	1.1752 s / batch. (data: 2.96e-04). ETA=6 days, 7:29:39, max mem: 15.1 GB 
[06/17 13:37:11][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0009,	1.1700 s / batch. (data: 3.29e-04). ETA=6 days, 6:47:17, max mem: 15.1 GB 
[06/17 13:39:08][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0014,	1.1766 s / batch. (data: 2.72e-04). ETA=6 days, 7:36:36, max mem: 15.1 GB 
[06/17 13:41:05][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0011,	1.1720 s / batch. (data: 2.98e-04). ETA=6 days, 6:58:41, max mem: 15.1 GB 
[06/17 13:43:03][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0009,	1.1688 s / batch. (data: 2.52e-04). ETA=6 days, 6:32:22, max mem: 15.1 GB 
[06/17 13:45:00][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0008,	1.1711 s / batch. (data: 3.95e-04). ETA=6 days, 6:48:15, max mem: 15.1 GB 
[06/17 13:46:57][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0008,	1.1692 s / batch. (data: 3.18e-04). ETA=6 days, 6:31:29, max mem: 15.1 GB 
[06/17 13:48:54][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0012,	1.1725 s / batch. (data: 3.19e-04). ETA=6 days, 6:55:15, max mem: 15.1 GB 
[06/17 13:50:51][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0035,	1.1711 s / batch. (data: 3.10e-04). ETA=6 days, 6:42:27, max mem: 15.1 GB 
[06/17 13:52:48][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0008,	1.1688 s / batch. (data: 3.63e-04). ETA=6 days, 6:22:47, max mem: 15.1 GB 
[06/17 13:54:46][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0010,	1.1717 s / batch. (data: 2.68e-04). ETA=6 days, 6:43:03, max mem: 15.1 GB 
[06/17 13:56:43][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0012,	1.1733 s / batch. (data: 2.94e-04). ETA=6 days, 6:53:47, max mem: 15.1 GB 
[06/17 13:58:40][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0011,	1.1738 s / batch. (data: 3.47e-04). ETA=6 days, 6:55:01, max mem: 15.1 GB 
[06/17 14:00:37][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0004,	1.1725 s / batch. (data: 2.94e-04). ETA=6 days, 6:43:13, max mem: 15.1 GB 
[06/17 14:02:35][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0008,	1.1743 s / batch. (data: 2.69e-04). ETA=6 days, 6:55:07, max mem: 15.1 GB 
[06/17 14:04:32][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0008,	1.1727 s / batch. (data: 2.88e-04). ETA=6 days, 6:40:48, max mem: 15.1 GB 
[06/17 14:06:29][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0002,	1.1735 s / batch. (data: 3.38e-04). ETA=6 days, 6:45:29, max mem: 15.1 GB 
[06/17 14:08:26][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0007,	1.1704 s / batch. (data: 3.07e-04). ETA=6 days, 6:19:05, max mem: 15.1 GB 
[06/17 14:10:23][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0025,	1.1713 s / batch. (data: 3.40e-04). ETA=6 days, 6:24:03, max mem: 15.1 GB 
[06/17 14:12:21][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0009,	1.1690 s / batch. (data: 3.52e-04). ETA=6 days, 6:04:36, max mem: 15.1 GB 
[06/17 14:14:18][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0010,	1.1763 s / batch. (data: 3.09e-04). ETA=6 days, 6:58:52, max mem: 15.1 GB 
[06/17 14:16:15][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0001,	1.1728 s / batch. (data: 2.94e-04). ETA=6 days, 6:30:03, max mem: 15.1 GB 
[06/17 14:18:12][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0008,	1.1784 s / batch. (data: 3.09e-04). ETA=6 days, 7:11:01, max mem: 15.1 GB 
[06/17 14:20:09][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0008,	1.1739 s / batch. (data: 3.13e-04). ETA=6 days, 6:34:15, max mem: 15.1 GB 
[06/17 14:22:07][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0008,	1.1692 s / batch. (data: 3.31e-04). ETA=6 days, 5:56:25, max mem: 15.1 GB 
[06/17 14:24:04][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0010,	1.1736 s / batch. (data: 3.09e-04). ETA=6 days, 6:28:25, max mem: 15.1 GB 
[06/17 14:26:01][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0009,	1.1667 s / batch. (data: 2.74e-04). ETA=6 days, 5:33:19, max mem: 15.1 GB 
[06/17 14:27:58][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0011,	1.1651 s / batch. (data: 4.06e-04). ETA=6 days, 5:18:57, max mem: 15.1 GB 
[06/17 14:29:55][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0002,	1.1711 s / batch. (data: 4.42e-04). ETA=6 days, 6:03:12, max mem: 15.1 GB 
[06/17 14:31:52][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0008,	1.1730 s / batch. (data: 3.18e-04). ETA=6 days, 6:15:39, max mem: 15.1 GB 
[06/17 14:33:49][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0018,	1.1724 s / batch. (data: 2.90e-04). ETA=6 days, 6:09:39, max mem: 15.1 GB 
[06/17 14:35:46][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0008,	1.1690 s / batch. (data: 2.83e-04). ETA=6 days, 5:41:05, max mem: 15.1 GB 
[06/17 14:37:43][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0010,	1.1735 s / batch. (data: 2.83e-04). ETA=6 days, 6:13:54, max mem: 15.1 GB 
[06/17 14:39:40][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0009,	1.1709 s / batch. (data: 2.90e-04). ETA=6 days, 5:51:48, max mem: 15.1 GB 
[06/17 14:41:37][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0022,	1.1768 s / batch. (data: 3.09e-04). ETA=6 days, 6:35:36, max mem: 15.1 GB 
[06/17 14:43:34][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0009,	1.1675 s / batch. (data: 4.01e-04). ETA=6 days, 5:22:04, max mem: 15.1 GB 
[06/17 14:45:31][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0010,	1.1713 s / batch. (data: 3.05e-04). ETA=6 days, 5:49:33, max mem: 15.1 GB 
[06/17 14:47:28][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0009,	1.1596 s / batch. (data: 1.11e-04). ETA=6 days, 4:17:24, max mem: 15.1 GB 
[06/17 14:47:34][INFO] visual_prompt:  319: Epoch 8 / 100: avg data time: 4.15e-03, avg batch time: 1.1802, average train loss: 0.0009
[06/17 14:56:25][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1630 s / batch. (data: 1.96e-04)max mem: 15.06516 GB 
[06/17 15:04:39][INFO] visual_prompt:  476: Inference (val):avg data time: 1.37e-04, avg batch time: 5.1398, average loss: 0.0002
[06/17 15:04:39][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/17 15:04:40][INFO] visual_prompt:  257: Training 9 / 100 epoch, with learning rate 0.8
[06/17 15:07:22][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0009,	1.1687 s / batch. (data: 2.54e-04). ETA=6 days, 5:25:24, max mem: 15.1 GB 
[06/17 15:09:19][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0014,	1.1665 s / batch. (data: 3.20e-04). ETA=6 days, 5:06:18, max mem: 15.1 GB 
[06/17 15:11:16][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0008,	1.1659 s / batch. (data: 3.09e-04). ETA=6 days, 4:59:52, max mem: 15.1 GB 
[06/17 15:13:13][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0003,	1.1701 s / batch. (data: 2.89e-04). ETA=6 days, 5:29:52, max mem: 15.1 GB 
[06/17 15:15:10][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0001,	1.1671 s / batch. (data: 2.62e-04). ETA=6 days, 5:05:19, max mem: 15.1 GB 
[06/17 15:17:07][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0014,	1.1679 s / batch. (data: 3.22e-04). ETA=6 days, 5:09:20, max mem: 15.1 GB 
[06/17 15:19:04][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0009,	1.1680 s / batch. (data: 3.46e-04). ETA=6 days, 5:08:26, max mem: 15.1 GB 
[06/17 15:21:01][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0001,	1.1642 s / batch. (data: 3.59e-04). ETA=6 days, 4:37:01, max mem: 15.1 GB 
[06/17 15:22:58][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0003,	1.1685 s / batch. (data: 3.08e-04). ETA=6 days, 5:08:23, max mem: 15.1 GB 
[06/17 15:24:55][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0013,	1.1700 s / batch. (data: 2.95e-04). ETA=6 days, 5:17:30, max mem: 15.1 GB 
[06/17 15:26:52][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0012,	1.1688 s / batch. (data: 2.42e-04). ETA=6 days, 5:06:52, max mem: 15.1 GB 
[06/17 15:28:50][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0011,	1.1676 s / batch. (data: 2.52e-04). ETA=6 days, 4:55:23, max mem: 15.1 GB 
[06/17 15:30:47][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0007,	1.1860 s / batch. (data: 3.50e-04). ETA=6 days, 7:14:24, max mem: 15.1 GB 
[06/17 15:32:44][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0009,	1.1741 s / batch. (data: 3.29e-04). ETA=6 days, 5:40:51, max mem: 15.1 GB 
[06/17 15:34:42][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0008,	1.1743 s / batch. (data: 3.22e-04). ETA=6 days, 5:40:40, max mem: 15.1 GB 
[06/17 15:36:39][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0002,	1.1701 s / batch. (data: 3.98e-04). ETA=6 days, 5:07:00, max mem: 15.1 GB 
[06/17 15:38:36][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0001,	1.1668 s / batch. (data: 3.13e-04). ETA=6 days, 4:39:18, max mem: 15.1 GB 
[06/17 15:40:33][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0010,	1.1749 s / batch. (data: 3.19e-04). ETA=6 days, 5:39:53, max mem: 15.1 GB 
[06/17 15:42:30][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0009,	1.1754 s / batch. (data: 3.46e-04). ETA=6 days, 5:41:18, max mem: 15.1 GB 
[06/17 15:44:28][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0027,	1.1765 s / batch. (data: 3.04e-04). ETA=6 days, 5:48:07, max mem: 15.1 GB 
[06/17 15:46:25][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0032,	1.1728 s / batch. (data: 3.26e-04). ETA=6 days, 5:17:48, max mem: 15.1 GB 
[06/17 15:48:22][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0015,	1.1717 s / batch. (data: 3.35e-04). ETA=6 days, 5:07:37, max mem: 15.1 GB 
[06/17 15:50:19][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0043,	1.1725 s / batch. (data: 3.55e-04). ETA=6 days, 5:11:17, max mem: 15.1 GB 
[06/17 15:52:16][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0003,	1.1759 s / batch. (data: 3.28e-04). ETA=6 days, 5:35:40, max mem: 15.1 GB 
[06/17 15:54:13][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0009,	1.1695 s / batch. (data: 3.38e-04). ETA=6 days, 4:44:55, max mem: 15.1 GB 
[06/17 15:56:11][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0002,	1.1764 s / batch. (data: 3.74e-04). ETA=6 days, 5:35:39, max mem: 15.1 GB 
[06/17 15:58:08][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0010,	1.1754 s / batch. (data: 2.77e-04). ETA=6 days, 5:25:51, max mem: 15.1 GB 
[06/17 16:00:05][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0003,	1.1719 s / batch. (data: 3.30e-04). ETA=6 days, 4:57:00, max mem: 15.1 GB 
[06/17 16:02:03][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0003,	1.1693 s / batch. (data: 5.88e-04). ETA=6 days, 4:35:34, max mem: 15.1 GB 
[06/17 16:04:01][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0020,	1.1788 s / batch. (data: 3.49e-04). ETA=6 days, 5:45:49, max mem: 15.1 GB 
[06/17 16:05:58][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0010,	1.1738 s / batch. (data: 3.37e-04). ETA=6 days, 5:05:47, max mem: 15.1 GB 
[06/17 16:07:56][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0012,	1.1746 s / batch. (data: 3.38e-04). ETA=6 days, 5:09:56, max mem: 15.1 GB 
[06/17 16:09:53][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0009,	1.1738 s / batch. (data: 4.46e-04). ETA=6 days, 5:01:30, max mem: 15.1 GB 
[06/17 16:11:51][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0002,	1.1741 s / batch. (data: 3.30e-04). ETA=6 days, 5:02:19, max mem: 15.1 GB 
[06/17 16:13:48][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0002,	1.1708 s / batch. (data: 3.43e-04). ETA=6 days, 4:34:53, max mem: 15.1 GB 
[06/17 16:15:45][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0009,	1.1694 s / batch. (data: 2.98e-04). ETA=6 days, 4:22:03, max mem: 15.1 GB 
[06/17 16:17:43][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0008,	1.1705 s / batch. (data: 3.86e-04). ETA=6 days, 4:28:39, max mem: 15.1 GB 
[06/17 16:19:40][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0002,	1.1763 s / batch. (data: 2.89e-04). ETA=6 days, 5:10:47, max mem: 15.1 GB 
[06/17 16:21:38][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0009,	1.1727 s / batch. (data: 3.25e-04). ETA=6 days, 4:41:40, max mem: 15.1 GB 
[06/17 16:23:35][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0009,	1.1681 s / batch. (data: 3.78e-04). ETA=6 days, 4:04:24, max mem: 15.1 GB 
[06/17 16:25:32][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0002,	1.1739 s / batch. (data: 3.20e-04). ETA=6 days, 4:46:57, max mem: 15.1 GB 
[06/17 16:27:30][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0009,	1.1793 s / batch. (data: 3.04e-04). ETA=6 days, 5:25:41, max mem: 15.1 GB 
[06/17 16:29:27][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0010,	1.1756 s / batch. (data: 3.64e-04). ETA=6 days, 4:56:02, max mem: 15.1 GB 
[06/17 16:31:24][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0013,	1.1679 s / batch. (data: 3.35e-04). ETA=6 days, 3:55:29, max mem: 15.1 GB 
[06/17 16:33:22][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0001,	1.1742 s / batch. (data: 3.13e-04). ETA=6 days, 4:41:02, max mem: 15.1 GB 
[06/17 16:35:19][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0001,	1.1769 s / batch. (data: 4.07e-04). ETA=6 days, 4:59:53, max mem: 15.1 GB 
[06/17 16:37:17][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0008,	1.1750 s / batch. (data: 3.06e-04). ETA=6 days, 4:43:47, max mem: 15.1 GB 
[06/17 16:39:14][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0008,	1.1706 s / batch. (data: 3.36e-04). ETA=6 days, 4:08:15, max mem: 15.1 GB 
[06/17 16:41:11][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0001,	1.1726 s / batch. (data: 3.50e-04). ETA=6 days, 4:21:15, max mem: 15.1 GB 
[06/17 16:43:09][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0008,	1.1668 s / batch. (data: 1.29e-04). ETA=6 days, 3:35:45, max mem: 15.1 GB 
[06/17 16:43:14][INFO] visual_prompt:  319: Epoch 9 / 100: avg data time: 4.20e-03, avg batch time: 1.1818, average train loss: 0.0009
[06/17 16:52:08][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1942 s / batch. (data: 1.48e-04)max mem: 15.06516 GB 
[06/17 17:00:25][INFO] visual_prompt:  476: Inference (val):avg data time: 1.67e-04, avg batch time: 5.1625, average loss: 0.0002
[06/17 17:00:25][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/17 17:00:25][INFO] visual_prompt:  257: Training 10 / 100 epoch, with learning rate 0.9
[06/17 17:03:05][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0012,	1.1734 s / batch. (data: 4.52e-04). ETA=6 days, 4:23:38, max mem: 15.1 GB 
[06/17 17:05:02][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0007,	1.1767 s / batch. (data: 3.22e-04). ETA=6 days, 4:46:51, max mem: 15.1 GB 
[06/17 17:06:59][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0014,	1.1724 s / batch. (data: 3.48e-04). ETA=6 days, 4:11:35, max mem: 15.1 GB 
[06/17 17:08:57][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0018,	1.1761 s / batch. (data: 4.92e-04). ETA=6 days, 4:38:15, max mem: 15.1 GB 
[06/17 17:10:54][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0010,	1.1706 s / batch. (data: 3.51e-04). ETA=6 days, 3:54:32, max mem: 15.1 GB 
[06/17 17:12:51][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0012,	1.1692 s / batch. (data: 3.19e-04). ETA=6 days, 3:41:38, max mem: 15.1 GB 
[06/17 17:14:49][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0001,	1.1748 s / batch. (data: 3.60e-04). ETA=6 days, 4:22:35, max mem: 15.1 GB 
[06/17 17:16:46][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0001,	1.1727 s / batch. (data: 3.49e-04). ETA=6 days, 4:04:27, max mem: 15.1 GB 
[06/17 17:18:43][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0001,	1.1744 s / batch. (data: 4.99e-04). ETA=6 days, 4:15:30, max mem: 15.1 GB 
[06/17 17:20:41][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0015,	1.1747 s / batch. (data: 4.02e-04). ETA=6 days, 4:16:02, max mem: 15.1 GB 
[06/17 17:22:38][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0001,	1.1735 s / batch. (data: 3.48e-04). ETA=6 days, 4:04:16, max mem: 15.1 GB 
[06/17 17:24:35][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0001,	1.1760 s / batch. (data: 4.11e-04). ETA=6 days, 4:21:30, max mem: 15.1 GB 
[06/17 17:26:33][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0003,	1.1732 s / batch. (data: 3.04e-04). ETA=6 days, 3:58:16, max mem: 15.1 GB 
[06/17 17:28:30][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0010,	1.1685 s / batch. (data: 3.40e-04). ETA=6 days, 3:20:48, max mem: 15.1 GB 
[06/17 17:30:27][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0008,	1.1741 s / batch. (data: 3.20e-04). ETA=6 days, 4:01:37, max mem: 15.1 GB 
[06/17 17:32:25][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0028,	1.1776 s / batch. (data: 3.35e-04). ETA=6 days, 4:25:38, max mem: 15.1 GB 
[06/17 17:34:22][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0001,	1.1727 s / batch. (data: 3.03e-04). ETA=6 days, 3:47:07, max mem: 15.1 GB 
[06/17 17:36:20][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0014,	1.1745 s / batch. (data: 3.20e-04). ETA=6 days, 3:58:09, max mem: 15.1 GB 
[06/17 17:38:17][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0010,	1.1673 s / batch. (data: 2.98e-04). ETA=6 days, 3:01:54, max mem: 15.1 GB 
[06/17 17:40:14][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0011,	1.1790 s / batch. (data: 4.46e-04). ETA=6 days, 4:28:33, max mem: 15.1 GB 
[06/17 17:42:12][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0008,	1.1723 s / batch. (data: 2.59e-04). ETA=6 days, 3:36:23, max mem: 15.1 GB 
[06/17 17:44:09][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0008,	1.1750 s / batch. (data: 4.16e-04). ETA=6 days, 3:54:23, max mem: 15.1 GB 
[06/17 17:46:06][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0041,	1.1728 s / batch. (data: 3.68e-04). ETA=6 days, 3:35:50, max mem: 15.1 GB 
[06/17 17:48:04][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0015,	1.1787 s / batch. (data: 3.33e-04). ETA=6 days, 4:18:45, max mem: 15.1 GB 
[06/17 17:50:01][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0002,	1.1724 s / batch. (data: 3.48e-04). ETA=6 days, 3:28:40, max mem: 15.1 GB 
[06/17 17:51:58][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0011,	1.1792 s / batch. (data: 3.76e-04). ETA=6 days, 4:18:30, max mem: 15.1 GB 
[06/17 17:53:56][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0001,	1.1729 s / batch. (data: 3.01e-04). ETA=6 days, 3:29:03, max mem: 15.1 GB 
[06/17 17:55:53][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0001,	1.1737 s / batch. (data: 3.11e-04). ETA=6 days, 3:33:15, max mem: 15.1 GB 
[06/17 17:57:51][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0012,	1.1757 s / batch. (data: 3.42e-04). ETA=6 days, 3:45:41, max mem: 15.1 GB 
[06/17 17:59:48][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0012,	1.1805 s / batch. (data: 4.55e-04). ETA=6 days, 4:20:36, max mem: 15.1 GB 
[06/17 18:01:45][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0002,	1.1700 s / batch. (data: 3.32e-04). ETA=6 days, 2:59:16, max mem: 15.1 GB 
[06/17 18:03:42][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0013,	1.1711 s / batch. (data: 3.31e-04). ETA=6 days, 3:05:43, max mem: 15.1 GB 
[06/17 18:05:40][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0008,	1.1691 s / batch. (data: 3.41e-04). ETA=6 days, 2:48:45, max mem: 15.1 GB 
[06/17 18:07:37][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0009,	1.1688 s / batch. (data: 3.47e-04). ETA=6 days, 2:43:59, max mem: 15.1 GB 
[06/17 18:09:34][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0011,	1.1727 s / batch. (data: 3.47e-04). ETA=6 days, 3:11:26, max mem: 15.1 GB 
[06/17 18:11:32][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0018,	1.1769 s / batch. (data: 3.52e-04). ETA=6 days, 3:41:13, max mem: 15.1 GB 
[06/17 18:13:29][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0021,	1.1749 s / batch. (data: 3.06e-04). ETA=6 days, 3:24:01, max mem: 15.1 GB 
[06/17 18:15:27][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0004,	1.1748 s / batch. (data: 3.57e-04). ETA=6 days, 3:21:22, max mem: 15.1 GB 
[06/17 18:17:24][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0009,	1.1731 s / batch. (data: 3.57e-04). ETA=6 days, 3:07:09, max mem: 15.1 GB 
[06/17 18:19:21][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0009,	1.1694 s / batch. (data: 2.75e-04). ETA=6 days, 2:36:55, max mem: 15.1 GB 
[06/17 18:21:19][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0009,	1.1713 s / batch. (data: 3.38e-04). ETA=6 days, 2:49:25, max mem: 15.1 GB 
[06/17 18:23:16][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0015,	1.1711 s / batch. (data: 2.95e-04). ETA=6 days, 2:45:47, max mem: 15.1 GB 
[06/17 18:25:13][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0012,	1.1759 s / batch. (data: 3.55e-04). ETA=6 days, 3:20:21, max mem: 15.1 GB 
[06/17 18:27:11][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0009,	1.1718 s / batch. (data: 3.98e-04). ETA=6 days, 2:47:23, max mem: 15.1 GB 
[06/17 18:29:08][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0009,	1.1692 s / batch. (data: 2.89e-04). ETA=6 days, 2:25:49, max mem: 15.1 GB 
[06/17 18:31:05][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0033,	1.1719 s / batch. (data: 2.84e-04). ETA=6 days, 2:44:24, max mem: 15.1 GB 
[06/17 18:33:02][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0018,	1.1726 s / batch. (data: 3.76e-04). ETA=6 days, 2:47:29, max mem: 15.1 GB 
[06/17 18:34:59][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0001,	1.1766 s / batch. (data: 3.56e-04). ETA=6 days, 3:15:41, max mem: 15.1 GB 
[06/17 18:36:57][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0011,	1.1736 s / batch. (data: 4.10e-04). ETA=6 days, 2:51:00, max mem: 15.1 GB 
[06/17 18:38:54][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0022,	1.1650 s / batch. (data: 1.19e-04). ETA=6 days, 1:44:22, max mem: 15.1 GB 
[06/17 18:39:00][INFO] visual_prompt:  319: Epoch 10 / 100: avg data time: 4.25e-03, avg batch time: 1.1819, average train loss: 0.0012
[06/17 18:47:55][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.2181 s / batch. (data: 1.57e-04)max mem: 15.06516 GB 
[06/17 18:56:11][INFO] visual_prompt:  476: Inference (val):avg data time: 1.68e-04, avg batch time: 5.1719, average loss: 0.0003
[06/17 18:56:11][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/17 18:56:11][INFO] visual_prompt:  257: Training 11 / 100 epoch, with learning rate 1.0
[06/17 18:58:50][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0001,	1.1711 s / batch. (data: 3.94e-04). ETA=6 days, 2:28:24, max mem: 15.1 GB 
[06/17 19:00:47][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0002,	1.1708 s / batch. (data: 2.89e-04). ETA=6 days, 2:23:52, max mem: 15.1 GB 
[06/17 19:02:45][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0015,	1.1809 s / batch. (data: 3.61e-04). ETA=6 days, 3:37:38, max mem: 15.1 GB 
[06/17 19:04:42][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0014,	1.1783 s / batch. (data: 3.05e-04). ETA=6 days, 3:16:34, max mem: 15.1 GB 
[06/17 19:06:40][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0002,	1.1716 s / batch. (data: 3.19e-04). ETA=6 days, 2:24:35, max mem: 15.1 GB 
[06/17 19:08:37][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0016,	1.1841 s / batch. (data: 3.24e-04). ETA=6 days, 3:55:56, max mem: 15.1 GB 
[06/17 19:10:34][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0013,	1.1738 s / batch. (data: 3.47e-04). ETA=6 days, 2:37:00, max mem: 15.1 GB 
[06/17 19:12:32][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0018,	1.1747 s / batch. (data: 3.17e-04). ETA=6 days, 2:41:32, max mem: 15.1 GB 
[06/17 19:14:29][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0018,	1.1786 s / batch. (data: 3.68e-04). ETA=6 days, 3:08:58, max mem: 15.1 GB 
[06/17 19:16:27][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0004,	1.1750 s / batch. (data: 2.93e-04). ETA=6 days, 2:39:45, max mem: 15.1 GB 
[06/17 19:18:24][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0049,	1.1721 s / batch. (data: 5.47e-04). ETA=6 days, 2:16:36, max mem: 15.1 GB 
[06/17 19:20:21][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0002,	1.1727 s / batch. (data: 3.34e-04). ETA=6 days, 2:18:36, max mem: 15.1 GB 
[06/17 19:22:18][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0002,	1.1724 s / batch. (data: 2.97e-04). ETA=6 days, 2:14:51, max mem: 15.1 GB 
[06/17 19:24:16][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0003,	1.1703 s / batch. (data: 3.09e-04). ETA=6 days, 1:57:16, max mem: 15.1 GB 
[06/17 19:26:13][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0010,	1.1723 s / batch. (data: 3.12e-04). ETA=6 days, 2:09:46, max mem: 15.1 GB 
[06/17 19:28:10][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0013,	1.1696 s / batch. (data: 3.07e-04). ETA=6 days, 1:47:33, max mem: 15.1 GB 
[06/17 19:30:07][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0010,	1.1738 s / batch. (data: 2.80e-04). ETA=6 days, 2:17:18, max mem: 15.1 GB 
[06/17 19:32:05][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0011,	1.1755 s / batch. (data: 4.44e-04). ETA=6 days, 2:27:45, max mem: 15.1 GB 
[06/17 19:34:02][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0009,	1.1780 s / batch. (data: 4.30e-04). ETA=6 days, 2:44:56, max mem: 15.1 GB 
[06/17 19:35:59][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0002,	1.1724 s / batch. (data: 3.53e-04). ETA=6 days, 2:01:08, max mem: 15.1 GB 
[06/17 19:37:57][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0015,	1.1697 s / batch. (data: 3.05e-04). ETA=6 days, 1:39:08, max mem: 15.1 GB 
[06/17 19:39:54][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0002,	1.1756 s / batch. (data: 2.61e-04). ETA=6 days, 2:20:46, max mem: 15.1 GB 
[06/17 19:41:51][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0013,	1.1721 s / batch. (data: 3.03e-04). ETA=6 days, 1:52:44, max mem: 15.1 GB 
[06/17 19:43:48][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0037,	1.1684 s / batch. (data: 3.11e-04). ETA=6 days, 1:23:00, max mem: 15.1 GB 
[06/17 19:45:46][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0015,	1.1721 s / batch. (data: 3.41e-04). ETA=6 days, 1:48:42, max mem: 15.1 GB 
[06/17 19:47:43][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0002,	1.1681 s / batch. (data: 2.14e-04). ETA=6 days, 1:17:13, max mem: 15.1 GB 
[06/17 19:49:40][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0029,	1.1751 s / batch. (data: 3.26e-04). ETA=6 days, 2:07:35, max mem: 15.1 GB 
[06/17 19:51:38][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0003,	1.1758 s / batch. (data: 3.73e-04). ETA=6 days, 2:11:01, max mem: 15.1 GB 
[06/17 19:53:35][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0014,	1.1733 s / batch. (data: 3.40e-04). ETA=6 days, 1:49:58, max mem: 15.1 GB 
[06/17 19:55:32][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0017,	1.1751 s / batch. (data: 3.52e-04). ETA=6 days, 2:01:24, max mem: 15.1 GB 
[06/17 19:57:30][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0002,	1.1730 s / batch. (data: 3.82e-04). ETA=6 days, 1:44:09, max mem: 15.1 GB 
[06/17 19:59:27][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0012,	1.1698 s / batch. (data: 3.61e-04). ETA=6 days, 1:17:57, max mem: 15.1 GB 
[06/17 20:01:25][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0001,	1.1750 s / batch. (data: 3.96e-04). ETA=6 days, 1:55:15, max mem: 15.1 GB 
[06/17 20:03:22][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0002,	1.1777 s / batch. (data: 2.95e-04). ETA=6 days, 2:13:15, max mem: 15.1 GB 
[06/17 20:05:19][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0019,	1.1728 s / batch. (data: 3.01e-04). ETA=6 days, 1:34:53, max mem: 15.1 GB 
[06/17 20:07:17][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0001,	1.1723 s / batch. (data: 3.51e-04). ETA=6 days, 1:28:54, max mem: 15.1 GB 
[06/17 20:09:14][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0002,	1.1687 s / batch. (data: 3.40e-04). ETA=6 days, 0:59:53, max mem: 15.1 GB 
[06/17 20:11:11][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0001,	1.1752 s / batch. (data: 3.37e-04). ETA=6 days, 1:46:20, max mem: 15.1 GB 
[06/17 20:13:08][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0013,	1.1759 s / batch. (data: 3.55e-04). ETA=6 days, 1:49:56, max mem: 15.1 GB 
[06/17 20:15:06][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0010,	1.1710 s / batch. (data: 4.41e-04). ETA=6 days, 1:11:16, max mem: 15.1 GB 
[06/17 20:17:03][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0055,	1.1651 s / batch. (data: 3.45e-04). ETA=6 days, 0:25:19, max mem: 15.1 GB 
[06/17 20:19:00][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0040,	1.1734 s / batch. (data: 3.78e-04). ETA=6 days, 1:25:04, max mem: 15.1 GB 
[06/17 20:20:57][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0002,	1.1766 s / batch. (data: 2.85e-04). ETA=6 days, 1:47:31, max mem: 15.1 GB 
[06/17 20:22:54][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0002,	1.1654 s / batch. (data: 3.54e-04). ETA=6 days, 0:22:15, max mem: 15.1 GB 
[06/17 20:24:52][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0001,	1.1721 s / batch. (data: 3.69e-04). ETA=6 days, 1:10:10, max mem: 15.1 GB 
[06/17 20:26:49][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0033,	1.1711 s / batch. (data: 3.42e-04). ETA=6 days, 1:00:24, max mem: 15.1 GB 
[06/17 20:28:46][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0001,	1.1760 s / batch. (data: 3.71e-04). ETA=6 days, 1:34:48, max mem: 15.1 GB 
[06/17 20:30:43][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0009,	1.1711 s / batch. (data: 3.35e-04). ETA=6 days, 0:56:37, max mem: 15.1 GB 
[06/17 20:32:40][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0003,	1.1687 s / batch. (data: 3.32e-04). ETA=6 days, 0:36:33, max mem: 15.1 GB 
[06/17 20:34:37][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0002,	1.1735 s / batch. (data: 1.95e-04). ETA=6 days, 1:10:45, max mem: 15.1 GB 
[06/17 20:34:43][INFO] visual_prompt:  319: Epoch 11 / 100: avg data time: 4.36e-03, avg batch time: 1.1812, average train loss: 0.0013
[06/17 20:43:38][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1646 s / batch. (data: 1.93e-04)max mem: 15.06516 GB 
[06/17 20:51:54][INFO] visual_prompt:  476: Inference (val):avg data time: 1.73e-04, avg batch time: 5.1619, average loss: 0.0002
[06/17 20:51:54][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/17 20:51:54][INFO] visual_prompt:  257: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[06/17 20:54:31][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0002,	1.1738 s / batch. (data: 3.58e-04). ETA=6 days, 1:11:03, max mem: 15.1 GB 
[06/17 20:56:29][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0008,	1.1673 s / batch. (data: 3.55e-04). ETA=6 days, 0:20:47, max mem: 15.1 GB 
[06/17 20:58:25][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0022,	1.1690 s / batch. (data: 2.92e-04). ETA=6 days, 0:30:55, max mem: 15.1 GB 
[06/17 21:00:23][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0011,	1.1724 s / batch. (data: 3.70e-04). ETA=6 days, 0:54:21, max mem: 15.1 GB 
[06/17 21:02:21][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0030,	1.1734 s / batch. (data: 4.05e-04). ETA=6 days, 0:59:43, max mem: 15.1 GB 
[06/17 21:04:18][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0014,	1.1698 s / batch. (data: 3.54e-04). ETA=6 days, 0:31:25, max mem: 15.1 GB 
[06/17 21:06:15][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0030,	1.1642 s / batch. (data: 3.37e-04). ETA=5 days, 23:47:42, max mem: 15.1 GB 
[06/17 21:08:12][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0015,	1.1712 s / batch. (data: 3.71e-04). ETA=6 days, 0:37:54, max mem: 15.1 GB 
[06/17 21:10:10][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0040,	1.1715 s / batch. (data: 3.40e-04). ETA=6 days, 0:38:08, max mem: 15.1 GB 
[06/17 21:12:07][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0001,	1.1671 s / batch. (data: 3.40e-04). ETA=6 days, 0:03:07, max mem: 15.1 GB 
[06/17 21:14:04][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0015,	1.1713 s / batch. (data: 3.53e-04). ETA=6 days, 0:32:32, max mem: 15.1 GB 
[06/17 21:16:01][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0003,	1.1766 s / batch. (data: 3.07e-04). ETA=6 days, 1:10:14, max mem: 15.1 GB 
[06/17 21:17:59][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0014,	1.1726 s / batch. (data: 3.37e-04). ETA=6 days, 0:38:37, max mem: 15.1 GB 
[06/17 21:19:56][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0003,	1.1714 s / batch. (data: 3.99e-04). ETA=6 days, 0:27:38, max mem: 15.1 GB 
[06/17 21:21:53][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0001,	1.1712 s / batch. (data: 4.15e-04). ETA=6 days, 0:23:43, max mem: 15.1 GB 
[06/17 21:23:50][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0012,	1.1776 s / batch. (data: 3.41e-04). ETA=6 days, 1:09:49, max mem: 15.1 GB 
[06/17 21:25:47][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0034,	1.1768 s / batch. (data: 3.30e-04). ETA=6 days, 1:01:43, max mem: 15.1 GB 
[06/17 21:27:44][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0084,	1.1751 s / batch. (data: 3.62e-04). ETA=6 days, 0:46:49, max mem: 15.1 GB 
[06/17 21:29:42][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0002,	1.1815 s / batch. (data: 3.60e-04). ETA=6 days, 1:32:24, max mem: 15.1 GB 
[06/17 21:31:39][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0024,	1.1682 s / batch. (data: 3.46e-04). ETA=5 days, 23:52:29, max mem: 15.1 GB 
[06/17 21:33:36][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0035,	1.1704 s / batch. (data: 4.85e-04). ETA=6 days, 0:06:06, max mem: 15.1 GB 
[06/17 21:35:33][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0012,	1.1653 s / batch. (data: 3.19e-04). ETA=5 days, 23:26:43, max mem: 15.1 GB 
[06/17 21:37:31][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0003,	1.1700 s / batch. (data: 3.11e-04). ETA=5 days, 23:59:45, max mem: 15.1 GB 
[06/17 21:39:28][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0026,	1.1733 s / batch. (data: 3.25e-04). ETA=6 days, 0:21:49, max mem: 15.1 GB 
[06/17 21:41:25][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0003,	1.1747 s / batch. (data: 3.36e-04). ETA=6 days, 0:30:37, max mem: 15.1 GB 
[06/17 21:43:22][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0004,	1.1725 s / batch. (data: 3.35e-04). ETA=6 days, 0:11:59, max mem: 15.1 GB 
[06/17 21:45:19][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0009,	1.1694 s / batch. (data: 3.64e-04). ETA=5 days, 23:47:41, max mem: 15.1 GB 
[06/17 21:47:16][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0008,	1.1689 s / batch. (data: 3.45e-04). ETA=5 days, 23:41:41, max mem: 15.1 GB 
[06/17 21:49:13][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0016,	1.1634 s / batch. (data: 3.42e-04). ETA=5 days, 22:59:14, max mem: 15.1 GB 
[06/17 21:51:10][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0018,	1.1721 s / batch. (data: 3.65e-04). ETA=6 days, 0:01:39, max mem: 15.1 GB 
[06/17 21:53:07][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0003,	1.1718 s / batch. (data: 2.58e-04). ETA=5 days, 23:57:30, max mem: 15.1 GB 
[06/17 21:55:05][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0003,	1.1694 s / batch. (data: 3.70e-04). ETA=5 days, 23:37:50, max mem: 15.1 GB 
[06/17 21:57:02][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0039,	1.1722 s / batch. (data: 4.20e-04). ETA=5 days, 23:56:12, max mem: 15.1 GB 
[06/17 21:58:59][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0017,	1.1759 s / batch. (data: 3.65e-04). ETA=6 days, 0:21:39, max mem: 15.1 GB 
[06/17 22:00:56][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0005,	1.1635 s / batch. (data: 3.36e-04). ETA=5 days, 22:48:22, max mem: 15.1 GB 
[06/17 22:02:53][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0018,	1.1702 s / batch. (data: 3.75e-04). ETA=5 days, 23:35:21, max mem: 15.1 GB 
[06/17 22:04:51][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0011,	1.1754 s / batch. (data: 3.44e-04). ETA=6 days, 0:12:23, max mem: 15.1 GB 
[06/17 22:06:48][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0002,	1.1749 s / batch. (data: 3.27e-04). ETA=6 days, 0:06:38, max mem: 15.1 GB 
[06/17 22:08:45][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0006,	1.1670 s / batch. (data: 3.36e-04). ETA=5 days, 23:06:08, max mem: 15.1 GB 
[06/17 22:10:43][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0033,	1.1734 s / batch. (data: 3.14e-04). ETA=5 days, 23:51:25, max mem: 15.1 GB 
[06/17 22:12:40][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0006,	1.1677 s / batch. (data: 3.24e-04). ETA=5 days, 23:07:27, max mem: 15.1 GB 
[06/17 22:14:36][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0001,	1.1690 s / batch. (data: 3.47e-04). ETA=5 days, 23:15:25, max mem: 15.1 GB 
[06/17 22:16:33][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0006,	1.1710 s / batch. (data: 3.34e-04). ETA=5 days, 23:27:50, max mem: 15.1 GB 
[06/17 22:18:31][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0002,	1.1753 s / batch. (data: 3.63e-04). ETA=5 days, 23:57:13, max mem: 15.1 GB 
[06/17 22:20:28][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0003,	1.1740 s / batch. (data: 3.81e-04). ETA=5 days, 23:45:52, max mem: 15.1 GB 
[06/17 22:22:25][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0003,	1.1698 s / batch. (data: 3.28e-04). ETA=5 days, 23:13:34, max mem: 15.1 GB 
[06/17 22:24:22][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0024,	1.1667 s / batch. (data: 2.91e-04). ETA=5 days, 22:48:48, max mem: 15.1 GB 
[06/17 22:26:19][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0004,	1.1733 s / batch. (data: 3.14e-04). ETA=5 days, 23:35:13, max mem: 15.1 GB 
[06/17 22:28:16][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0003,	1.1729 s / batch. (data: 3.23e-04). ETA=5 days, 23:30:03, max mem: 15.1 GB 
[06/17 22:30:13][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0002,	1.1722 s / batch. (data: 1.21e-04). ETA=5 days, 23:23:02, max mem: 15.1 GB 
[06/17 22:30:19][INFO] visual_prompt:  319: Epoch 12 / 100: avg data time: 4.38e-03, avg batch time: 1.1799, average train loss: 0.0014
[06/17 22:39:12][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1593 s / batch. (data: 1.34e-04)max mem: 15.06516 GB 
[06/17 22:47:28][INFO] visual_prompt:  476: Inference (val):avg data time: 1.46e-04, avg batch time: 5.1596, average loss: 0.0003
[06/17 22:47:28][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/17 22:47:28][INFO] visual_prompt:  257: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[06/17 22:50:05][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0004,	1.1766 s / batch. (data: 5.48e-04). ETA=5 days, 23:53:23, max mem: 15.1 GB 
[06/17 22:52:02][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0013,	1.1706 s / batch. (data: 3.35e-04). ETA=5 days, 23:07:06, max mem: 15.1 GB 
[06/17 22:53:59][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0010,	1.1705 s / batch. (data: 3.33e-04). ETA=5 days, 23:04:20, max mem: 15.1 GB 
[06/17 22:55:57][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0003,	1.1798 s / batch. (data: 3.38e-04). ETA=6 days, 0:10:48, max mem: 15.1 GB 
[06/17 22:57:54][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0003,	1.1722 s / batch. (data: 3.39e-04). ETA=5 days, 23:13:14, max mem: 15.1 GB 
[06/17 22:59:51][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0017,	1.1723 s / batch. (data: 3.14e-04). ETA=5 days, 23:12:22, max mem: 15.1 GB 
[06/17 23:01:48][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0003,	1.1645 s / batch. (data: 2.86e-04). ETA=5 days, 22:12:42, max mem: 15.1 GB 
[06/17 23:03:45][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0002,	1.1730 s / batch. (data: 2.88e-04). ETA=5 days, 23:13:19, max mem: 15.1 GB 
[06/17 23:05:42][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0016,	1.1696 s / batch. (data: 3.73e-04). ETA=5 days, 22:46:37, max mem: 15.1 GB 
[06/17 23:07:39][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0015,	1.1711 s / batch. (data: 3.58e-04). ETA=5 days, 22:55:27, max mem: 15.1 GB 
[06/17 23:09:37][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0026,	1.1784 s / batch. (data: 3.55e-04). ETA=5 days, 23:47:11, max mem: 15.1 GB 
[06/17 23:11:34][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0002,	1.1663 s / batch. (data: 3.55e-04). ETA=5 days, 22:16:17, max mem: 15.1 GB 
[06/17 23:13:31][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0001,	1.1702 s / batch. (data: 3.17e-04). ETA=5 days, 22:42:46, max mem: 15.1 GB 
[06/17 23:15:28][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0001,	1.1761 s / batch. (data: 3.70e-04). ETA=5 days, 23:24:19, max mem: 15.1 GB 
[06/17 23:17:25][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0014,	1.1738 s / batch. (data: 2.95e-04). ETA=5 days, 23:05:12, max mem: 15.1 GB 
[06/17 23:19:22][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0012,	1.1733 s / batch. (data: 3.34e-04). ETA=5 days, 22:59:38, max mem: 15.1 GB 
[06/17 23:21:20][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0018,	1.1663 s / batch. (data: 2.88e-04). ETA=5 days, 22:06:25, max mem: 15.1 GB 
[06/17 23:23:17][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0044,	1.1680 s / batch. (data: 3.42e-04). ETA=5 days, 22:17:08, max mem: 15.1 GB 
[06/17 23:25:14][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0032,	1.1699 s / batch. (data: 3.31e-04). ETA=5 days, 22:29:07, max mem: 15.1 GB 
[06/17 23:27:11][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0001,	1.1658 s / batch. (data: 2.91e-04). ETA=5 days, 21:57:32, max mem: 15.1 GB 
[06/17 23:29:08][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0011,	1.1697 s / batch. (data: 4.13e-04). ETA=5 days, 22:23:58, max mem: 15.1 GB 
[06/17 23:31:05][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0026,	1.1698 s / batch. (data: 3.63e-04). ETA=5 days, 22:22:36, max mem: 15.1 GB 
[06/17 23:33:02][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0010,	1.1728 s / batch. (data: 3.24e-04). ETA=5 days, 22:42:29, max mem: 15.1 GB 
[06/17 23:34:59][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0002,	1.1714 s / batch. (data: 3.98e-04). ETA=5 days, 22:29:57, max mem: 15.1 GB 
[06/17 23:36:56][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0003,	1.1674 s / batch. (data: 3.39e-04). ETA=5 days, 21:59:05, max mem: 15.1 GB 
[06/17 23:38:54][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0013,	1.1769 s / batch. (data: 3.40e-04). ETA=5 days, 23:06:28, max mem: 15.1 GB 
[06/17 23:40:51][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0003,	1.1742 s / batch. (data: 3.74e-04). ETA=5 days, 22:44:39, max mem: 15.1 GB 
[06/17 23:42:48][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0011,	1.1654 s / batch. (data: 2.91e-04). ETA=5 days, 21:38:32, max mem: 15.1 GB 
[06/17 23:44:45][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0003,	1.1732 s / batch. (data: 2.66e-04). ETA=5 days, 22:33:37, max mem: 15.1 GB 
[06/17 23:46:42][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0002,	1.1713 s / batch. (data: 3.09e-04). ETA=5 days, 22:17:44, max mem: 15.1 GB 
[06/17 23:48:40][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0003,	1.1722 s / batch. (data: 3.41e-04). ETA=5 days, 22:22:45, max mem: 15.1 GB 
[06/17 23:50:37][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0004,	1.1767 s / batch. (data: 3.15e-04). ETA=5 days, 22:53:36, max mem: 15.1 GB 
[06/17 23:52:34][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0012,	1.1735 s / batch. (data: 3.91e-04). ETA=5 days, 22:28:06, max mem: 15.1 GB 
[06/17 23:54:31][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0081,	1.1788 s / batch. (data: 2.98e-04). ETA=5 days, 23:04:18, max mem: 15.1 GB 
[06/17 23:56:29][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0003,	1.1737 s / batch. (data: 4.46e-04). ETA=5 days, 22:25:32, max mem: 15.1 GB 
[06/17 23:58:26][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0024,	1.1699 s / batch. (data: 3.48e-04). ETA=5 days, 21:56:11, max mem: 15.1 GB 
[06/18 00:00:23][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0013,	1.1706 s / batch. (data: 3.49e-04). ETA=5 days, 21:58:56, max mem: 15.1 GB 
[06/18 00:02:20][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0003,	1.1702 s / batch. (data: 3.18e-04). ETA=5 days, 21:54:27, max mem: 15.1 GB 
[06/18 00:04:18][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0035,	1.1709 s / batch. (data: 4.00e-04). ETA=5 days, 21:57:27, max mem: 15.1 GB 
[06/18 00:06:15][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0021,	1.1729 s / batch. (data: 3.07e-04). ETA=5 days, 22:09:54, max mem: 15.1 GB 
[06/18 00:08:12][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0024,	1.1761 s / batch. (data: 3.13e-04). ETA=5 days, 22:31:09, max mem: 15.1 GB 
[06/18 00:10:09][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0021,	1.1763 s / batch. (data: 4.85e-04). ETA=5 days, 22:30:25, max mem: 15.1 GB 
[06/18 00:12:07][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0003,	1.1799 s / batch. (data: 3.72e-04). ETA=5 days, 22:54:58, max mem: 15.1 GB 
[06/18 00:14:05][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0003,	1.1738 s / batch. (data: 3.04e-04). ETA=5 days, 22:08:46, max mem: 15.1 GB 
[06/18 00:16:02][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0003,	1.1691 s / batch. (data: 3.14e-04). ETA=5 days, 21:32:55, max mem: 15.1 GB 
[06/18 00:17:59][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0027,	1.1667 s / batch. (data: 2.91e-04). ETA=5 days, 21:13:15, max mem: 15.1 GB 
[06/18 00:19:56][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0003,	1.1785 s / batch. (data: 3.59e-04). ETA=5 days, 22:36:53, max mem: 15.1 GB 
[06/18 00:21:53][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0002,	1.1729 s / batch. (data: 3.08e-04). ETA=5 days, 21:54:40, max mem: 15.1 GB 
[06/18 00:23:51][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0001,	1.1765 s / batch. (data: 2.53e-04). ETA=5 days, 22:18:19, max mem: 15.1 GB 
[06/18 00:25:48][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0011,	1.1670 s / batch. (data: 1.21e-04). ETA=5 days, 21:07:37, max mem: 15.1 GB 
[06/18 00:25:54][INFO] visual_prompt:  319: Epoch 13 / 100: avg data time: 4.20e-03, avg batch time: 1.1799, average train loss: 0.0012
[06/18 00:34:47][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1730 s / batch. (data: 6.03e-05)max mem: 15.06516 GB 
[06/18 00:43:01][INFO] visual_prompt:  476: Inference (val):avg data time: 1.52e-04, avg batch time: 5.1504, average loss: 0.0002
[06/18 00:43:01][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/18 00:43:01][INFO] visual_prompt:  257: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[06/18 00:45:41][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0010,	1.1738 s / batch. (data: 3.61e-04). ETA=5 days, 21:54:38, max mem: 15.1 GB 
[06/18 00:47:38][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0004,	1.1802 s / batch. (data: 3.98e-04). ETA=5 days, 22:39:03, max mem: 15.1 GB 
[06/18 00:49:36][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0016,	1.1706 s / batch. (data: 4.16e-04). ETA=5 days, 21:27:57, max mem: 15.1 GB 
[06/18 00:51:33][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0015,	1.1745 s / batch. (data: 4.60e-04). ETA=5 days, 21:53:48, max mem: 15.1 GB 
[06/18 00:53:30][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0002,	1.1724 s / batch. (data: 3.41e-04). ETA=5 days, 21:37:12, max mem: 15.1 GB 
[06/18 00:55:28][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0027,	1.1706 s / batch. (data: 3.42e-04). ETA=5 days, 21:21:46, max mem: 15.1 GB 
[06/18 00:57:25][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0002,	1.1691 s / batch. (data: 3.70e-04). ETA=5 days, 21:09:18, max mem: 15.1 GB 
[06/18 00:59:22][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0003,	1.1791 s / batch. (data: 4.89e-04). ETA=5 days, 22:19:23, max mem: 15.1 GB 
[06/18 01:01:19][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0003,	1.1730 s / batch. (data: 4.02e-04). ETA=5 days, 21:33:21, max mem: 15.1 GB 
[06/18 01:03:16][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0003,	1.1709 s / batch. (data: 3.63e-04). ETA=5 days, 21:16:34, max mem: 15.1 GB 
[06/18 01:05:14][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0012,	1.1713 s / batch. (data: 3.40e-04). ETA=5 days, 21:17:33, max mem: 15.1 GB 
[06/18 01:07:11][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0001,	1.1645 s / batch. (data: 2.85e-04). ETA=5 days, 20:26:00, max mem: 15.1 GB 
[06/18 01:09:08][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0012,	1.1718 s / batch. (data: 3.23e-04). ETA=5 days, 21:16:39, max mem: 15.1 GB 
[06/18 01:11:05][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0020,	1.1727 s / batch. (data: 3.19e-04). ETA=5 days, 21:21:16, max mem: 15.1 GB 
[06/18 01:13:02][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0003,	1.1716 s / batch. (data: 3.41e-04). ETA=5 days, 21:11:56, max mem: 15.1 GB 
[06/18 01:15:00][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0023,	1.1683 s / batch. (data: 3.26e-04). ETA=5 days, 20:45:51, max mem: 15.1 GB 
[06/18 01:16:57][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0012,	1.1691 s / batch. (data: 3.19e-04). ETA=5 days, 20:49:35, max mem: 15.1 GB 
[06/18 01:18:54][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0010,	1.1740 s / batch. (data: 3.40e-04). ETA=5 days, 21:23:23, max mem: 15.1 GB 
[06/18 01:20:51][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0003,	1.1725 s / batch. (data: 3.67e-04). ETA=5 days, 21:10:39, max mem: 15.1 GB 
[06/18 01:22:48][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0001,	1.1708 s / batch. (data: 3.85e-04). ETA=5 days, 20:56:23, max mem: 15.1 GB 
[06/18 01:24:46][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0001,	1.1725 s / batch. (data: 3.18e-04). ETA=5 days, 21:06:19, max mem: 15.1 GB 
[06/18 01:26:43][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0003,	1.1691 s / batch. (data: 3.42e-04). ETA=5 days, 20:40:14, max mem: 15.1 GB 
[06/18 01:28:40][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0002,	1.1691 s / batch. (data: 3.66e-04). ETA=5 days, 20:37:46, max mem: 15.1 GB 
[06/18 01:30:37][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0014,	1.1692 s / batch. (data: 3.39e-04). ETA=5 days, 20:36:42, max mem: 15.1 GB 
[06/18 01:32:34][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0001,	1.1680 s / batch. (data: 3.08e-04). ETA=5 days, 20:25:51, max mem: 15.1 GB 
[06/18 01:34:31][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0004,	1.1691 s / batch. (data: 3.08e-04). ETA=5 days, 20:32:26, max mem: 15.1 GB 
[06/18 01:36:29][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0003,	1.1714 s / batch. (data: 2.97e-04). ETA=5 days, 20:46:54, max mem: 15.1 GB 
[06/18 01:38:26][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0023,	1.1770 s / batch. (data: 3.02e-04). ETA=5 days, 21:25:03, max mem: 15.1 GB 
[06/18 01:40:23][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0001,	1.1719 s / batch. (data: 3.38e-04). ETA=5 days, 20:46:09, max mem: 15.1 GB 
[06/18 01:42:21][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0002,	1.1687 s / batch. (data: 3.46e-04). ETA=5 days, 20:21:31, max mem: 15.1 GB 
[06/18 01:44:18][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0035,	1.1825 s / batch. (data: 3.81e-04). ETA=5 days, 21:58:33, max mem: 15.1 GB 
[06/18 01:46:15][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0002,	1.1694 s / batch. (data: 4.76e-04). ETA=5 days, 20:22:46, max mem: 15.1 GB 
[06/18 01:48:12][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0001,	1.1774 s / batch. (data: 4.30e-04). ETA=5 days, 21:18:14, max mem: 15.1 GB 
[06/18 01:50:10][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0005,	1.1803 s / batch. (data: 3.03e-04). ETA=5 days, 21:36:58, max mem: 15.1 GB 
[06/18 01:52:08][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0017,	1.1743 s / batch. (data: 5.33e-04). ETA=5 days, 20:51:55, max mem: 15.1 GB 
[06/18 01:54:05][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0036,	1.1763 s / batch. (data: 3.49e-04). ETA=5 days, 21:04:39, max mem: 15.1 GB 
[06/18 01:56:02][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0001,	1.1699 s / batch. (data: 4.22e-04). ETA=5 days, 20:16:10, max mem: 15.1 GB 
[06/18 01:58:00][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0012,	1.1652 s / batch. (data: 4.30e-04). ETA=5 days, 19:40:50, max mem: 15.1 GB 
[06/18 01:59:57][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0022,	1.1706 s / batch. (data: 3.27e-04). ETA=5 days, 20:17:25, max mem: 15.1 GB 
[06/18 02:01:54][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0027,	1.1764 s / batch. (data: 3.69e-04). ETA=5 days, 20:57:17, max mem: 15.1 GB 
[06/18 02:03:51][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0002,	1.1715 s / batch. (data: 3.45e-04). ETA=5 days, 20:20:17, max mem: 15.1 GB 
[06/18 02:05:48][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0002,	1.1643 s / batch. (data: 3.42e-04). ETA=5 days, 19:26:38, max mem: 15.1 GB 
[06/18 02:07:45][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0023,	1.1830 s / batch. (data: 3.81e-04). ETA=5 days, 21:38:51, max mem: 15.1 GB 
[06/18 02:09:42][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0014,	1.1682 s / batch. (data: 3.07e-04). ETA=5 days, 19:50:48, max mem: 15.1 GB 
[06/18 02:11:39][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0001,	1.1650 s / batch. (data: 3.23e-04). ETA=5 days, 19:25:47, max mem: 15.1 GB 
[06/18 02:13:36][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0002,	1.1634 s / batch. (data: 2.89e-04). ETA=5 days, 19:12:22, max mem: 15.1 GB 
[06/18 02:15:33][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0002,	1.1638 s / batch. (data: 2.71e-04). ETA=5 days, 19:13:10, max mem: 15.1 GB 
[06/18 02:17:29][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0001,	1.1663 s / batch. (data: 3.66e-04). ETA=5 days, 19:29:14, max mem: 15.1 GB 
[06/18 02:19:26][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0011,	1.1768 s / batch. (data: 3.44e-04). ETA=5 days, 20:42:37, max mem: 15.1 GB 
[06/18 02:21:24][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0002,	1.1642 s / batch. (data: 1.35e-04). ETA=5 days, 19:10:26, max mem: 15.1 GB 
[06/18 02:21:29][INFO] visual_prompt:  319: Epoch 14 / 100: avg data time: 4.12e-03, avg batch time: 1.1804, average train loss: 0.0010
[06/18 02:30:23][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1759 s / batch. (data: 2.56e-04)max mem: 15.06516 GB 
[06/18 02:38:38][INFO] visual_prompt:  476: Inference (val):avg data time: 1.53e-04, avg batch time: 5.1529, average loss: 0.0002
[06/18 02:38:38][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/18 02:38:39][INFO] visual_prompt:  257: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[06/18 02:41:21][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0003,	1.1730 s / batch. (data: 3.28e-04). ETA=5 days, 20:11:24, max mem: 15.1 GB 
[06/18 02:43:18][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0003,	1.1679 s / batch. (data: 3.53e-04). ETA=5 days, 19:32:45, max mem: 15.1 GB 
[06/18 02:45:15][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0015,	1.1679 s / batch. (data: 3.29e-04). ETA=5 days, 19:30:46, max mem: 15.1 GB 
[06/18 02:47:12][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0002,	1.1697 s / batch. (data: 5.90e-04). ETA=5 days, 19:41:29, max mem: 15.1 GB 
[06/18 02:49:09][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0002,	1.1697 s / batch. (data: 3.42e-04). ETA=5 days, 19:39:41, max mem: 15.1 GB 
[06/18 02:51:06][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0003,	1.1720 s / batch. (data: 3.41e-04). ETA=5 days, 19:54:23, max mem: 15.1 GB 
[06/18 02:53:03][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0002,	1.1707 s / batch. (data: 4.32e-04). ETA=5 days, 19:42:58, max mem: 15.1 GB 
[06/18 02:55:00][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0017,	1.1705 s / batch. (data: 2.99e-04). ETA=5 days, 19:39:30, max mem: 15.1 GB 
[06/18 02:56:57][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0001,	1.1699 s / batch. (data: 4.46e-04). ETA=5 days, 19:33:32, max mem: 15.1 GB 
[06/18 02:58:53][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0001,	1.1725 s / batch. (data: 3.33e-04). ETA=5 days, 19:50:22, max mem: 15.1 GB 
[06/18 03:00:51][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0001,	1.1717 s / batch. (data: 4.03e-04). ETA=5 days, 19:42:11, max mem: 15.1 GB 
[06/18 03:02:48][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0008,	1.1712 s / batch. (data: 3.17e-04). ETA=5 days, 19:37:00, max mem: 15.1 GB 
[06/18 03:04:45][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0021,	1.1694 s / batch. (data: 4.85e-04). ETA=5 days, 19:21:52, max mem: 15.1 GB 
[06/18 03:06:43][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0003,	1.1782 s / batch. (data: 3.02e-04). ETA=5 days, 20:22:53, max mem: 15.1 GB 
[06/18 03:08:40][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0018,	1.1753 s / batch. (data: 3.75e-04). ETA=5 days, 20:00:35, max mem: 15.1 GB 
[06/18 03:10:37][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0002,	1.1758 s / batch. (data: 3.53e-04). ETA=5 days, 20:01:48, max mem: 15.1 GB 
[06/18 03:12:35][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0002,	1.1745 s / batch. (data: 3.26e-04). ETA=5 days, 19:50:24, max mem: 15.1 GB 
[06/18 03:14:32][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0002,	1.1731 s / batch. (data: 2.91e-04). ETA=5 days, 19:38:48, max mem: 15.1 GB 
[06/18 03:16:29][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0001,	1.1701 s / batch. (data: 3.34e-04). ETA=5 days, 19:15:19, max mem: 15.1 GB 
[06/18 03:18:26][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0003,	1.1749 s / batch. (data: 4.34e-04). ETA=5 days, 19:47:47, max mem: 15.1 GB 
[06/18 03:20:24][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0002,	1.1741 s / batch. (data: 3.96e-04). ETA=5 days, 19:40:19, max mem: 15.1 GB 
[06/18 03:22:21][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0002,	1.1750 s / batch. (data: 3.43e-04). ETA=5 days, 19:44:46, max mem: 15.1 GB 
[06/18 03:24:18][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0002,	1.1696 s / batch. (data: 3.49e-04). ETA=5 days, 19:04:13, max mem: 15.1 GB 
[06/18 03:26:15][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0011,	1.1723 s / batch. (data: 3.35e-04). ETA=5 days, 19:21:24, max mem: 15.1 GB 
[06/18 03:28:12][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0026,	1.1687 s / batch. (data: 3.56e-04). ETA=5 days, 18:53:46, max mem: 15.1 GB 
[06/18 03:30:09][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0003,	1.1701 s / batch. (data: 3.26e-04). ETA=5 days, 19:01:42, max mem: 15.1 GB 
[06/18 03:32:06][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0001,	1.1710 s / batch. (data: 3.50e-04). ETA=5 days, 19:05:50, max mem: 15.1 GB 
[06/18 03:34:03][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0002,	1.1693 s / batch. (data: 3.53e-04). ETA=5 days, 18:52:00, max mem: 15.1 GB 
[06/18 03:36:00][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0002,	1.1688 s / batch. (data: 3.25e-04). ETA=5 days, 18:46:38, max mem: 15.1 GB 
[06/18 03:37:57][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0004,	1.1682 s / batch. (data: 3.17e-04). ETA=5 days, 18:40:37, max mem: 15.1 GB 
[06/18 03:39:54][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0016,	1.1733 s / batch. (data: 3.24e-04). ETA=5 days, 19:14:55, max mem: 15.1 GB 
[06/18 03:41:51][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0001,	1.1725 s / batch. (data: 3.23e-04). ETA=5 days, 19:07:20, max mem: 15.1 GB 
[06/18 03:43:48][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0003,	1.1689 s / batch. (data: 3.20e-04). ETA=5 days, 18:39:30, max mem: 15.1 GB 
[06/18 03:45:45][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0003,	1.1723 s / batch. (data: 2.87e-04). ETA=5 days, 19:01:40, max mem: 15.1 GB 
[06/18 03:47:43][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0001,	1.1783 s / batch. (data: 3.12e-04). ETA=5 days, 19:42:32, max mem: 15.1 GB 
[06/18 03:49:40][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0003,	1.1749 s / batch. (data: 3.80e-04). ETA=5 days, 19:16:25, max mem: 15.1 GB 
[06/18 03:51:37][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0001,	1.1778 s / batch. (data: 3.45e-04). ETA=5 days, 19:34:54, max mem: 15.1 GB 
[06/18 03:53:35][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0015,	1.1791 s / batch. (data: 3.17e-04). ETA=5 days, 19:42:33, max mem: 15.1 GB 
[06/18 03:55:32][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0029,	1.1792 s / batch. (data: 3.01e-04). ETA=5 days, 19:40:52, max mem: 15.1 GB 
[06/18 03:57:29][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0042,	1.1738 s / batch. (data: 3.06e-04). ETA=5 days, 19:00:50, max mem: 15.1 GB 
[06/18 03:59:27][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0001,	1.1752 s / batch. (data: 3.62e-04). ETA=5 days, 19:09:01, max mem: 15.1 GB 
[06/18 04:01:24][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0001,	1.1674 s / batch. (data: 3.25e-04). ETA=5 days, 18:11:30, max mem: 15.1 GB 
[06/18 04:03:21][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0001,	1.1752 s / batch. (data: 4.12e-04). ETA=5 days, 19:04:50, max mem: 15.1 GB 
[06/18 04:05:19][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0021,	1.1747 s / batch. (data: 3.27e-04). ETA=5 days, 18:59:13, max mem: 15.1 GB 
[06/18 04:07:16][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0046,	1.1692 s / batch. (data: 3.43e-04). ETA=5 days, 18:18:33, max mem: 15.1 GB 
[06/18 04:09:13][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0002,	1.1717 s / batch. (data: 3.25e-04). ETA=5 days, 18:34:16, max mem: 15.1 GB 
[06/18 04:11:10][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0017,	1.1763 s / batch. (data: 3.22e-04). ETA=5 days, 19:04:54, max mem: 15.1 GB 
[06/18 04:13:07][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0002,	1.1653 s / batch. (data: 3.13e-04). ETA=5 days, 17:45:06, max mem: 15.1 GB 
[06/18 04:15:04][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0053,	1.1707 s / batch. (data: 3.11e-04). ETA=5 days, 18:21:23, max mem: 15.1 GB 
[06/18 04:17:01][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0023,	1.1658 s / batch. (data: 1.29e-04). ETA=5 days, 17:44:43, max mem: 15.1 GB 
[06/18 04:17:06][INFO] visual_prompt:  319: Epoch 15 / 100: avg data time: 4.30e-03, avg batch time: 1.1804, average train loss: 0.0010
[06/18 04:25:59][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1780 s / batch. (data: 1.83e-04)max mem: 15.06516 GB 
[06/18 04:34:14][INFO] visual_prompt:  476: Inference (val):avg data time: 1.65e-04, avg batch time: 5.1536, average loss: 0.0002
[06/18 04:34:14][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/18 04:34:15][INFO] visual_prompt:  257: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[06/18 04:36:57][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0062,	1.1755 s / batch. (data: 2.83e-04). ETA=5 days, 18:51:20, max mem: 15.1 GB 
[06/18 04:38:54][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0004,	1.1757 s / batch. (data: 4.20e-04). ETA=5 days, 18:50:18, max mem: 15.1 GB 
[06/18 04:40:51][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0003,	1.1683 s / batch. (data: 3.42e-04). ETA=5 days, 17:56:34, max mem: 15.1 GB 
[06/18 04:42:48][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0002,	1.1728 s / batch. (data: 3.21e-04). ETA=5 days, 18:26:18, max mem: 15.1 GB 
[06/18 04:44:45][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0002,	1.1750 s / batch. (data: 3.76e-04). ETA=5 days, 18:39:39, max mem: 15.1 GB 
[06/18 04:46:43][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0012,	1.1754 s / batch. (data: 3.95e-04). ETA=5 days, 18:40:56, max mem: 15.1 GB 
[06/18 04:48:40][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0001,	1.1698 s / batch. (data: 3.88e-04). ETA=5 days, 17:59:20, max mem: 15.1 GB 
[06/18 04:50:37][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0004,	1.1734 s / batch. (data: 5.15e-04). ETA=5 days, 18:22:36, max mem: 15.1 GB 
[06/18 04:52:34][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0042,	1.1721 s / batch. (data: 3.33e-04). ETA=5 days, 18:11:05, max mem: 15.1 GB 
[06/18 04:54:32][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0058,	1.1701 s / batch. (data: 3.40e-04). ETA=5 days, 17:55:41, max mem: 15.1 GB 
[06/18 04:56:29][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0032,	1.1757 s / batch. (data: 3.22e-04). ETA=5 days, 18:33:18, max mem: 15.1 GB 
[06/18 04:58:25][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0003,	1.1723 s / batch. (data: 3.31e-04). ETA=5 days, 18:06:47, max mem: 15.1 GB 
[06/18 05:00:22][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0002,	1.1673 s / batch. (data: 3.04e-04). ETA=5 days, 17:29:38, max mem: 15.1 GB 
[06/18 05:02:19][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0001,	1.1733 s / batch. (data: 3.51e-04). ETA=5 days, 18:10:00, max mem: 15.1 GB 
[06/18 05:04:17][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0044,	1.1781 s / batch. (data: 3.82e-04). ETA=5 days, 18:42:17, max mem: 15.1 GB 
[06/18 05:06:14][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0022,	1.1700 s / batch. (data: 4.67e-04). ETA=5 days, 17:43:06, max mem: 15.1 GB 
[06/18 05:08:11][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0001,	1.1705 s / batch. (data: 3.45e-04). ETA=5 days, 17:44:31, max mem: 15.1 GB 
[06/18 05:10:08][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0001,	1.1714 s / batch. (data: 3.73e-04). ETA=5 days, 17:49:01, max mem: 15.1 GB 
[06/18 05:12:05][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0050,	1.1717 s / batch. (data: 4.62e-04). ETA=5 days, 17:48:56, max mem: 15.1 GB 
[06/18 05:14:03][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0013,	1.1735 s / batch. (data: 3.39e-04). ETA=5 days, 18:00:04, max mem: 15.1 GB 
[06/18 05:16:00][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0002,	1.1699 s / batch. (data: 3.57e-04). ETA=5 days, 17:32:42, max mem: 15.1 GB 
[06/18 05:17:57][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0003,	1.1712 s / batch. (data: 3.32e-04). ETA=5 days, 17:39:36, max mem: 15.1 GB 
[06/18 05:19:54][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0005,	1.1737 s / batch. (data: 3.52e-04). ETA=5 days, 17:55:34, max mem: 15.1 GB 
[06/18 05:21:51][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0003,	1.1699 s / batch. (data: 3.57e-04). ETA=5 days, 17:26:45, max mem: 15.1 GB 
[06/18 05:23:48][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0017,	1.1620 s / batch. (data: 3.37e-04). ETA=5 days, 16:28:39, max mem: 15.1 GB 
[06/18 05:25:46][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0027,	1.1755 s / batch. (data: 2.98e-04). ETA=5 days, 18:02:30, max mem: 15.1 GB 
[06/18 05:27:43][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0003,	1.1801 s / batch. (data: 3.74e-04). ETA=5 days, 18:32:36, max mem: 15.1 GB 
[06/18 05:29:41][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0031,	1.1710 s / batch. (data: 3.03e-04). ETA=5 days, 17:26:23, max mem: 15.1 GB 
[06/18 05:31:38][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0002,	1.1700 s / batch. (data: 3.15e-04). ETA=5 days, 17:17:20, max mem: 15.1 GB 
[06/18 05:33:35][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0003,	1.1702 s / batch. (data: 3.34e-04). ETA=5 days, 17:16:57, max mem: 15.1 GB 
[06/18 05:35:32][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0003,	1.1724 s / batch. (data: 4.72e-04). ETA=5 days, 17:30:36, max mem: 15.1 GB 
[06/18 05:37:29][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0007,	1.1711 s / batch. (data: 3.12e-04). ETA=5 days, 17:19:33, max mem: 15.1 GB 
[06/18 05:39:26][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0003,	1.1759 s / batch. (data: 2.75e-04). ETA=5 days, 17:51:19, max mem: 15.1 GB 
[06/18 05:41:24][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0003,	1.1689 s / batch. (data: 3.28e-04). ETA=5 days, 17:00:14, max mem: 15.1 GB 
[06/18 05:43:21][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0031,	1.1654 s / batch. (data: 3.15e-04). ETA=5 days, 16:33:12, max mem: 15.1 GB 
[06/18 05:45:18][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0002,	1.1760 s / batch. (data: 3.39e-04). ETA=5 days, 17:46:06, max mem: 15.1 GB 
[06/18 05:47:15][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0001,	1.1754 s / batch. (data: 3.09e-04). ETA=5 days, 17:39:40, max mem: 15.1 GB 
[06/18 05:49:13][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0003,	1.1744 s / batch. (data: 3.35e-04). ETA=5 days, 17:31:02, max mem: 15.1 GB 
[06/18 05:51:11][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0002,	1.1744 s / batch. (data: 4.19e-04). ETA=5 days, 17:28:42, max mem: 15.1 GB 
[06/18 05:53:08][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0025,	1.1733 s / batch. (data: 3.40e-04). ETA=5 days, 17:19:38, max mem: 15.1 GB 
[06/18 05:55:06][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0014,	1.1779 s / batch. (data: 3.36e-04). ETA=5 days, 17:49:28, max mem: 15.1 GB 
[06/18 05:57:04][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0001,	1.1793 s / batch. (data: 4.10e-04). ETA=5 days, 17:57:17, max mem: 15.1 GB 
[06/18 05:59:02][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0002,	1.1739 s / batch. (data: 3.65e-04). ETA=5 days, 17:17:44, max mem: 15.1 GB 
[06/18 06:00:59][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0003,	1.1742 s / batch. (data: 3.74e-04). ETA=5 days, 17:17:45, max mem: 15.1 GB 
[06/18 06:02:57][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0002,	1.1803 s / batch. (data: 3.52e-04). ETA=5 days, 17:58:19, max mem: 15.1 GB 
[06/18 06:04:54][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0012,	1.1673 s / batch. (data: 2.99e-04). ETA=5 days, 16:25:18, max mem: 15.1 GB 
[06/18 06:06:52][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0002,	1.1746 s / batch. (data: 4.22e-04). ETA=5 days, 17:14:56, max mem: 15.1 GB 
[06/18 06:08:49][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0013,	1.1690 s / batch. (data: 5.59e-04). ETA=5 days, 16:33:25, max mem: 15.1 GB 
[06/18 06:10:47][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0002,	1.1680 s / batch. (data: 4.19e-04). ETA=5 days, 16:24:49, max mem: 15.1 GB 
[06/18 06:12:44][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0011,	1.1626 s / batch. (data: 1.21e-04). ETA=5 days, 15:44:38, max mem: 15.1 GB 
[06/18 06:12:50][INFO] visual_prompt:  319: Epoch 16 / 100: avg data time: 4.32e-03, avg batch time: 1.1818, average train loss: 0.0010
[06/18 06:21:44][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1884 s / batch. (data: 1.86e-04)max mem: 15.06516 GB 
[06/18 06:30:01][INFO] visual_prompt:  476: Inference (val):avg data time: 2.09e-04, avg batch time: 5.1608, average loss: 0.0002
[06/18 06:30:01][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/18 06:30:01][INFO] visual_prompt:  257: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[06/18 06:32:38][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0013,	1.1732 s / batch. (data: 4.23e-04). ETA=5 days, 16:56:50, max mem: 15.1 GB 
[06/18 06:34:35][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0001,	1.1693 s / batch. (data: 3.63e-04). ETA=5 days, 16:27:43, max mem: 15.1 GB 
[06/18 06:36:33][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0010,	1.1725 s / batch. (data: 4.90e-04). ETA=5 days, 16:48:17, max mem: 15.1 GB 
[06/18 06:38:30][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0001,	1.1723 s / batch. (data: 3.45e-04). ETA=5 days, 16:44:43, max mem: 15.1 GB 
[06/18 06:40:27][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0003,	1.1710 s / batch. (data: 3.68e-04). ETA=5 days, 16:33:38, max mem: 15.1 GB 
[06/18 06:42:24][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0004,	1.1734 s / batch. (data: 3.65e-04). ETA=5 days, 16:48:41, max mem: 15.1 GB 
[06/18 06:44:21][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0002,	1.1704 s / batch. (data: 4.23e-04). ETA=5 days, 16:25:26, max mem: 15.1 GB 
[06/18 06:46:18][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0021,	1.1811 s / batch. (data: 3.33e-04). ETA=5 days, 17:38:14, max mem: 15.1 GB 
[06/18 06:48:16][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0004,	1.1710 s / batch. (data: 3.44e-04). ETA=5 days, 16:26:06, max mem: 15.1 GB 
[06/18 06:50:13][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0001,	1.1708 s / batch. (data: 2.90e-04). ETA=5 days, 16:22:52, max mem: 15.1 GB 
[06/18 06:52:10][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0015,	1.1722 s / batch. (data: 3.04e-04). ETA=5 days, 16:30:17, max mem: 15.1 GB 
[06/18 06:54:07][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0004,	1.1636 s / batch. (data: 3.62e-04). ETA=5 days, 15:28:46, max mem: 15.1 GB 
[06/18 06:56:04][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0005,	1.1703 s / batch. (data: 3.65e-04). ETA=5 days, 16:13:28, max mem: 15.1 GB 
[06/18 06:58:01][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0002,	1.1668 s / batch. (data: 2.89e-04). ETA=5 days, 15:46:45, max mem: 15.1 GB 
[06/18 06:59:58][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0002,	1.1741 s / batch. (data: 5.96e-04). ETA=5 days, 16:36:15, max mem: 15.1 GB 
[06/18 07:01:55][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0003,	1.1652 s / batch. (data: 4.12e-04). ETA=5 days, 15:31:51, max mem: 15.1 GB 
[06/18 07:03:52][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0018,	1.1651 s / batch. (data: 3.56e-04). ETA=5 days, 15:29:21, max mem: 15.1 GB 
[06/18 07:05:49][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0001,	1.1681 s / batch. (data: 3.50e-04). ETA=5 days, 15:48:00, max mem: 15.1 GB 
[06/18 07:07:46][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0013,	1.1647 s / batch. (data: 6.12e-04). ETA=5 days, 15:22:42, max mem: 15.1 GB 
[06/18 07:09:42][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0001,	1.1699 s / batch. (data: 3.64e-04). ETA=5 days, 15:57:11, max mem: 15.1 GB 
[06/18 07:11:39][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0014,	1.1680 s / batch. (data: 3.26e-04). ETA=5 days, 15:41:49, max mem: 15.1 GB 
[06/18 07:13:36][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0003,	1.1694 s / batch. (data: 3.03e-04). ETA=5 days, 15:49:12, max mem: 15.1 GB 
[06/18 07:15:33][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0002,	1.1752 s / batch. (data: 3.67e-04). ETA=5 days, 16:28:12, max mem: 15.1 GB 
[06/18 07:17:30][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0002,	1.1708 s / batch. (data: 3.32e-04). ETA=5 days, 15:55:27, max mem: 15.1 GB 
[06/18 07:19:28][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0002,	1.1747 s / batch. (data: 3.52e-04). ETA=5 days, 16:20:18, max mem: 15.1 GB 
[06/18 07:21:26][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0001,	1.1795 s / batch. (data: 4.01e-04). ETA=5 days, 16:52:07, max mem: 15.1 GB 
[06/18 07:23:23][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0003,	1.1681 s / batch. (data: 3.30e-04). ETA=5 days, 15:30:44, max mem: 15.1 GB 
[06/18 07:25:20][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0002,	1.1764 s / batch. (data: 3.46e-04). ETA=5 days, 16:26:36, max mem: 15.1 GB 
[06/18 07:27:17][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0015,	1.1753 s / batch. (data: 3.08e-04). ETA=5 days, 16:16:32, max mem: 15.1 GB 
[06/18 07:29:14][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0002,	1.1760 s / batch. (data: 3.83e-04). ETA=5 days, 16:19:38, max mem: 15.1 GB 
[06/18 07:31:12][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0043,	1.1694 s / batch. (data: 3.93e-04). ETA=5 days, 15:32:09, max mem: 15.1 GB 
[06/18 07:33:09][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0017,	1.1698 s / batch. (data: 3.58e-04). ETA=5 days, 15:33:05, max mem: 15.1 GB 
[06/18 07:35:06][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0002,	1.1681 s / batch. (data: 3.79e-04). ETA=5 days, 15:18:55, max mem: 15.1 GB 
[06/18 07:37:03][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0013,	1.1799 s / batch. (data: 3.59e-04). ETA=5 days, 16:39:08, max mem: 15.1 GB 
[06/18 07:39:00][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0001,	1.1681 s / batch. (data: 2.99e-04). ETA=5 days, 15:15:24, max mem: 15.1 GB 
[06/18 07:40:57][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0005,	1.1683 s / batch. (data: 4.49e-04). ETA=5 days, 15:14:48, max mem: 15.1 GB 
[06/18 07:42:53][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0021,	1.1609 s / batch. (data: 2.80e-04). ETA=5 days, 14:21:17, max mem: 15.1 GB 
[06/18 07:44:50][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0001,	1.1599 s / batch. (data: 3.34e-04). ETA=5 days, 14:12:12, max mem: 15.1 GB 
[06/18 07:46:47][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0002,	1.1674 s / batch. (data: 2.66e-04). ETA=5 days, 15:02:44, max mem: 15.1 GB 
[06/18 07:48:43][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0001,	1.1698 s / batch. (data: 2.98e-04). ETA=5 days, 15:17:26, max mem: 15.1 GB 
[06/18 07:50:40][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0002,	1.1656 s / batch. (data: 3.01e-04). ETA=5 days, 14:45:46, max mem: 15.1 GB 
[06/18 07:52:37][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0003,	1.1750 s / batch. (data: 2.92e-04). ETA=5 days, 15:49:24, max mem: 15.1 GB 
[06/18 07:54:33][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0001,	1.1629 s / batch. (data: 3.09e-04). ETA=5 days, 14:23:46, max mem: 15.1 GB 
[06/18 07:56:30][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0002,	1.1718 s / batch. (data: 3.02e-04). ETA=5 days, 15:23:30, max mem: 15.1 GB 
[06/18 07:58:27][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0033,	1.1715 s / batch. (data: 3.02e-04). ETA=5 days, 15:19:02, max mem: 15.1 GB 
[06/18 08:00:24][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0002,	1.1671 s / batch. (data: 2.75e-04). ETA=5 days, 14:46:50, max mem: 15.1 GB 
[06/18 08:02:20][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0001,	1.1687 s / batch. (data: 3.18e-04). ETA=5 days, 14:56:10, max mem: 15.1 GB 
[06/18 08:04:17][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0009,	1.1635 s / batch. (data: 2.58e-04). ETA=5 days, 14:17:36, max mem: 15.1 GB 
[06/18 08:06:14][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0003,	1.1667 s / batch. (data: 3.21e-04). ETA=5 days, 14:38:08, max mem: 15.1 GB 
[06/18 08:08:11][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0001,	1.1642 s / batch. (data: 9.82e-05). ETA=5 days, 14:19:04, max mem: 15.1 GB 
[06/18 08:08:16][INFO] visual_prompt:  319: Epoch 17 / 100: avg data time: 4.24e-03, avg batch time: 1.1778, average train loss: 0.0007
[06/18 08:17:05][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1420 s / batch. (data: 1.26e-04)max mem: 15.06516 GB 
[06/18 08:25:17][INFO] visual_prompt:  476: Inference (val):avg data time: 1.43e-04, avg batch time: 5.1230, average loss: 0.0002
[06/18 08:25:17][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/18 08:25:17][INFO] visual_prompt:  257: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[06/18 08:27:58][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0003,	1.1670 s / batch. (data: 2.92e-04). ETA=5 days, 14:35:57, max mem: 15.1 GB 
[06/18 08:29:55][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0002,	1.1696 s / batch. (data: 1.65e-04). ETA=5 days, 14:52:20, max mem: 15.1 GB 
[06/18 08:31:53][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0010,	1.1688 s / batch. (data: 2.93e-04). ETA=5 days, 14:44:39, max mem: 15.1 GB 
[06/18 08:33:50][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0003,	1.1740 s / batch. (data: 3.90e-04). ETA=5 days, 15:18:36, max mem: 15.1 GB 
[06/18 08:35:47][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0003,	1.1640 s / batch. (data: 3.35e-04). ETA=5 days, 14:07:39, max mem: 15.1 GB 
[06/18 08:37:44][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0009,	1.1713 s / batch. (data: 3.14e-04). ETA=5 days, 14:56:05, max mem: 15.1 GB 
[06/18 08:39:40][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0017,	1.1667 s / batch. (data: 3.34e-04). ETA=5 days, 14:22:20, max mem: 15.1 GB 
[06/18 08:41:37][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0031,	1.1651 s / batch. (data: 2.92e-04). ETA=5 days, 14:09:21, max mem: 15.1 GB 
[06/18 08:43:34][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0002,	1.1682 s / batch. (data: 3.32e-04). ETA=5 days, 14:28:43, max mem: 15.1 GB 
[06/18 08:45:30][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0002,	1.1640 s / batch. (data: 2.56e-04). ETA=5 days, 13:58:09, max mem: 15.1 GB 
[06/18 08:47:27][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0003,	1.1650 s / batch. (data: 2.95e-04). ETA=5 days, 14:03:12, max mem: 15.1 GB 
[06/18 08:49:24][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0002,	1.1697 s / batch. (data: 2.97e-04). ETA=5 days, 14:33:31, max mem: 15.1 GB 
[06/18 08:51:21][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0003,	1.1613 s / batch. (data: 3.09e-04). ETA=5 days, 13:33:48, max mem: 15.1 GB 
[06/18 08:53:17][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0001,	1.1715 s / batch. (data: 3.16e-04). ETA=5 days, 14:42:18, max mem: 15.1 GB 
[06/18 08:55:14][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0015,	1.1704 s / batch. (data: 3.09e-04). ETA=5 days, 14:32:36, max mem: 15.1 GB 
[06/18 08:57:11][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0004,	1.1703 s / batch. (data: 2.82e-04). ETA=5 days, 14:29:37, max mem: 15.1 GB 
[06/18 08:59:08][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0014,	1.1693 s / batch. (data: 2.76e-04). ETA=5 days, 14:20:51, max mem: 15.1 GB 
[06/18 09:01:04][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0019,	1.1682 s / batch. (data: 2.76e-04). ETA=5 days, 14:11:32, max mem: 15.1 GB 
[06/18 09:03:01][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0002,	1.1693 s / batch. (data: 3.29e-04). ETA=5 days, 14:17:26, max mem: 15.1 GB 
[06/18 09:04:58][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0021,	1.1681 s / batch. (data: 2.79e-04). ETA=5 days, 14:06:46, max mem: 15.1 GB 
[06/18 09:06:55][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0010,	1.1670 s / batch. (data: 3.12e-04). ETA=5 days, 13:57:20, max mem: 15.1 GB 
[06/18 09:08:52][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0010,	1.1721 s / batch. (data: 2.38e-04). ETA=5 days, 14:30:46, max mem: 15.1 GB 
[06/18 09:10:48][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0010,	1.1654 s / batch. (data: 2.86e-04). ETA=5 days, 13:42:27, max mem: 15.1 GB 
[06/18 09:12:45][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0003,	1.1654 s / batch. (data: 2.97e-04). ETA=5 days, 13:40:14, max mem: 15.1 GB 
[06/18 09:14:42][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0014,	1.1642 s / batch. (data: 3.21e-04). ETA=5 days, 13:30:21, max mem: 15.1 GB 
[06/18 09:16:38][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0003,	1.1706 s / batch. (data: 2.35e-04). ETA=5 days, 14:12:41, max mem: 15.1 GB 
[06/18 09:18:35][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0001,	1.1701 s / batch. (data: 2.88e-04). ETA=5 days, 14:06:49, max mem: 15.1 GB 
[06/18 09:20:32][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0010,	1.1673 s / batch. (data: 3.17e-04). ETA=5 days, 13:46:02, max mem: 15.1 GB 
[06/18 09:22:29][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0002,	1.1667 s / batch. (data: 2.95e-04). ETA=5 days, 13:39:44, max mem: 15.1 GB 
[06/18 09:24:25][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0003,	1.1682 s / batch. (data: 3.02e-04). ETA=5 days, 13:48:12, max mem: 15.1 GB 
[06/18 09:26:22][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0024,	1.1679 s / batch. (data: 3.43e-04). ETA=5 days, 13:44:02, max mem: 15.1 GB 
[06/18 09:28:19][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0002,	1.1686 s / batch. (data: 3.29e-04). ETA=5 days, 13:46:38, max mem: 15.1 GB 
[06/18 09:30:16][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0003,	1.1688 s / batch. (data: 3.08e-04). ETA=5 days, 13:46:03, max mem: 15.1 GB 
[06/18 09:32:12][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0010,	1.1671 s / batch. (data: 4.05e-04). ETA=5 days, 13:32:34, max mem: 15.1 GB 
[06/18 09:34:09][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0003,	1.1700 s / batch. (data: 3.13e-04). ETA=5 days, 13:50:24, max mem: 15.1 GB 
[06/18 09:36:06][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0002,	1.1682 s / batch. (data: 2.65e-04). ETA=5 days, 13:36:44, max mem: 15.1 GB 
[06/18 09:38:02][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0043,	1.1635 s / batch. (data: 3.36e-04). ETA=5 days, 13:02:23, max mem: 15.1 GB 
[06/18 09:39:59][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0003,	1.1639 s / batch. (data: 2.80e-04). ETA=5 days, 13:02:59, max mem: 15.1 GB 
[06/18 09:41:56][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0026,	1.1676 s / batch. (data: 2.82e-04). ETA=5 days, 13:26:36, max mem: 15.1 GB 
[06/18 09:43:52][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0003,	1.1650 s / batch. (data: 3.09e-04). ETA=5 days, 13:06:59, max mem: 15.1 GB 
[06/18 09:45:49][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0002,	1.1703 s / batch. (data: 3.09e-04). ETA=5 days, 13:41:03, max mem: 15.1 GB 
[06/18 09:47:46][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0002,	1.1659 s / batch. (data: 3.23e-04). ETA=5 days, 13:09:15, max mem: 15.1 GB 
[06/18 09:49:42][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0002,	1.1701 s / batch. (data: 2.58e-04). ETA=5 days, 13:35:41, max mem: 15.1 GB 
[06/18 09:51:39][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0002,	1.1625 s / batch. (data: 2.51e-04). ETA=5 days, 12:41:46, max mem: 15.1 GB 
[06/18 09:53:36][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0002,	1.1679 s / batch. (data: 2.71e-04). ETA=5 days, 13:16:50, max mem: 15.1 GB 
[06/18 09:55:32][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0003,	1.1708 s / batch. (data: 2.91e-04). ETA=5 days, 13:34:46, max mem: 15.1 GB 
[06/18 09:57:29][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0019,	1.1671 s / batch. (data: 2.74e-04). ETA=5 days, 13:07:14, max mem: 15.1 GB 
[06/18 09:59:26][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0003,	1.1681 s / batch. (data: 2.24e-04). ETA=5 days, 13:12:02, max mem: 15.1 GB 
[06/18 10:01:22][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0022,	1.1661 s / batch. (data: 2.53e-04). ETA=5 days, 12:57:04, max mem: 15.1 GB 
[06/18 10:03:19][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0001,	1.1598 s / batch. (data: 9.18e-05). ETA=5 days, 12:12:02, max mem: 15.1 GB 
[06/18 10:03:25][INFO] visual_prompt:  319: Epoch 18 / 100: avg data time: 3.86e-03, avg batch time: 1.1763, average train loss: 0.0006
[06/18 10:12:13][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1368 s / batch. (data: 2.02e-04)max mem: 15.06516 GB 
[06/18 10:20:25][INFO] visual_prompt:  476: Inference (val):avg data time: 1.43e-04, avg batch time: 5.1195, average loss: 0.0002
[06/18 10:20:25][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/18 10:20:25][INFO] visual_prompt:  257: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[06/18 10:23:05][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0001,	1.1653 s / batch. (data: 4.44e-04). ETA=5 days, 12:47:22, max mem: 15.1 GB 
[06/18 10:25:02][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0006,	1.1591 s / batch. (data: 2.82e-04). ETA=5 days, 12:03:17, max mem: 15.1 GB 
[06/18 10:26:58][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0001,	1.1661 s / batch. (data: 3.42e-04). ETA=5 days, 12:48:34, max mem: 15.1 GB 
[06/18 10:28:54][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0001,	1.1647 s / batch. (data: 3.05e-04). ETA=5 days, 12:37:32, max mem: 15.1 GB 
[06/18 10:30:51][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0002,	1.1631 s / batch. (data: 3.14e-04). ETA=5 days, 12:24:34, max mem: 15.1 GB 
[06/18 10:32:47][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0002,	1.1705 s / batch. (data: 2.74e-04). ETA=5 days, 13:13:06, max mem: 15.1 GB 
[06/18 10:34:44][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0004,	1.1673 s / batch. (data: 2.75e-04). ETA=5 days, 12:49:27, max mem: 15.1 GB 
[06/18 10:36:40][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0009,	1.1604 s / batch. (data: 4.32e-04). ETA=5 days, 12:00:16, max mem: 15.1 GB 
[06/18 10:38:37][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0002,	1.1657 s / batch. (data: 3.18e-04). ETA=5 days, 12:34:21, max mem: 15.1 GB 
[06/18 10:40:33][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0003,	1.1665 s / batch. (data: 3.08e-04). ETA=5 days, 12:38:18, max mem: 15.1 GB 
[06/18 10:42:30][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0002,	1.1692 s / batch. (data: 3.21e-04). ETA=5 days, 12:54:46, max mem: 15.1 GB 
[06/18 10:44:26][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0002,	1.1688 s / batch. (data: 2.89e-04). ETA=5 days, 12:49:45, max mem: 15.1 GB 
[06/18 10:46:23][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0003,	1.1656 s / batch. (data: 2.98e-04). ETA=5 days, 12:25:42, max mem: 15.1 GB 
[06/18 10:48:19][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0001,	1.1657 s / batch. (data: 3.21e-04). ETA=5 days, 12:25:04, max mem: 15.1 GB 
[06/18 10:50:16][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0013,	1.1627 s / batch. (data: 3.15e-04). ETA=5 days, 12:02:42, max mem: 15.1 GB 
[06/18 10:52:13][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0002,	1.1674 s / batch. (data: 3.20e-04). ETA=5 days, 12:32:42, max mem: 15.1 GB 
[06/18 10:54:09][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0012,	1.1646 s / batch. (data: 3.16e-04). ETA=5 days, 12:11:36, max mem: 15.1 GB 
[06/18 10:56:06][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0002,	1.1645 s / batch. (data: 3.52e-04). ETA=5 days, 12:08:53, max mem: 15.1 GB 
[06/18 10:58:03][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0001,	1.1672 s / batch. (data: 2.62e-04). ETA=5 days, 12:25:13, max mem: 15.1 GB 
[06/18 10:59:59][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0002,	1.1622 s / batch. (data: 2.64e-04). ETA=5 days, 11:49:03, max mem: 15.1 GB 
[06/18 11:01:56][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0002,	1.1685 s / batch. (data: 4.59e-04). ETA=5 days, 12:30:03, max mem: 15.1 GB 
[06/18 11:03:52][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0002,	1.1671 s / batch. (data: 2.58e-04). ETA=5 days, 12:19:04, max mem: 15.1 GB 
[06/18 11:05:49][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0004,	1.1658 s / batch. (data: 2.95e-04). ETA=5 days, 12:07:44, max mem: 15.1 GB 
[06/18 11:07:46][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0002,	1.1698 s / batch. (data: 3.60e-04). ETA=5 days, 12:33:33, max mem: 15.1 GB 
[06/18 11:09:43][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0003,	1.1652 s / batch. (data: 3.63e-04). ETA=5 days, 12:00:09, max mem: 15.1 GB 
[06/18 11:11:40][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0002,	1.1716 s / batch. (data: 3.03e-04). ETA=5 days, 12:41:29, max mem: 15.1 GB 
[06/18 11:13:36][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0001,	1.1687 s / batch. (data: 3.76e-04). ETA=5 days, 12:19:51, max mem: 15.1 GB 
[06/18 11:15:33][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0003,	1.1728 s / batch. (data: 3.71e-04). ETA=5 days, 12:46:00, max mem: 15.1 GB 
[06/18 11:17:30][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0023,	1.1732 s / batch. (data: 2.92e-04). ETA=5 days, 12:46:27, max mem: 15.1 GB 
[06/18 11:19:27][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0024,	1.1719 s / batch. (data: 3.26e-04). ETA=5 days, 12:35:57, max mem: 15.1 GB 
[06/18 11:21:24][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0002,	1.1656 s / batch. (data: 3.45e-04). ETA=5 days, 11:51:23, max mem: 15.1 GB 
[06/18 11:23:21][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0003,	1.1670 s / batch. (data: 3.43e-04). ETA=5 days, 11:58:49, max mem: 15.1 GB 
[06/18 11:25:18][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0001,	1.1685 s / batch. (data: 3.34e-04). ETA=5 days, 12:07:05, max mem: 15.1 GB 
[06/18 11:27:15][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0002,	1.1684 s / batch. (data: 2.98e-04). ETA=5 days, 12:04:15, max mem: 15.1 GB 
[06/18 11:29:12][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0002,	1.1683 s / batch. (data: 3.55e-04). ETA=5 days, 12:01:34, max mem: 15.1 GB 
[06/18 11:31:09][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0002,	1.1706 s / batch. (data: 2.96e-04). ETA=5 days, 12:15:11, max mem: 15.1 GB 
[06/18 11:33:06][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0019,	1.1726 s / batch. (data: 3.31e-04). ETA=5 days, 12:26:39, max mem: 15.1 GB 
[06/18 11:35:03][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0003,	1.1713 s / batch. (data: 3.57e-04). ETA=5 days, 12:15:58, max mem: 15.1 GB 
[06/18 11:37:00][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0038,	1.1682 s / batch. (data: 4.91e-04). ETA=5 days, 11:53:18, max mem: 15.1 GB 
[06/18 11:38:57][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0025,	1.1695 s / batch. (data: 3.16e-04). ETA=5 days, 11:59:49, max mem: 15.1 GB 
[06/18 11:40:54][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0002,	1.1781 s / batch. (data: 3.86e-04). ETA=5 days, 12:56:23, max mem: 15.1 GB 
[06/18 11:42:51][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0001,	1.1684 s / batch. (data: 4.18e-04). ETA=5 days, 11:48:26, max mem: 15.1 GB 
[06/18 11:44:48][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0003,	1.1707 s / batch. (data: 3.32e-04). ETA=5 days, 12:02:32, max mem: 15.1 GB 
[06/18 11:46:46][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0004,	1.1709 s / batch. (data: 3.48e-04). ETA=5 days, 12:01:55, max mem: 15.1 GB 
[06/18 11:48:43][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0017,	1.1669 s / batch. (data: 3.64e-04). ETA=5 days, 11:32:25, max mem: 15.1 GB 
[06/18 11:50:40][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0002,	1.1694 s / batch. (data: 3.87e-04). ETA=5 days, 11:47:34, max mem: 15.1 GB 
[06/18 11:52:37][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0002,	1.1724 s / batch. (data: 3.40e-04). ETA=5 days, 12:06:17, max mem: 15.1 GB 
[06/18 11:54:34][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0002,	1.1732 s / batch. (data: 3.44e-04). ETA=5 days, 12:09:25, max mem: 15.1 GB 
[06/18 11:56:31][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0001,	1.1724 s / batch. (data: 3.38e-04). ETA=5 days, 12:01:51, max mem: 15.1 GB 
[06/18 11:58:28][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0003,	1.1698 s / batch. (data: 1.47e-04). ETA=5 days, 11:42:46, max mem: 15.1 GB 
[06/18 11:58:34][INFO] visual_prompt:  319: Epoch 19 / 100: avg data time: 3.93e-03, avg batch time: 1.1765, average train loss: 0.0004
[06/18 12:07:25][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1626 s / batch. (data: 2.82e-04)max mem: 15.06516 GB 
[06/18 12:15:40][INFO] visual_prompt:  476: Inference (val):avg data time: 1.79e-04, avg batch time: 5.1478, average loss: 0.0001
[06/18 12:15:40][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/18 12:15:41][INFO] visual_prompt:  257: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[06/18 12:18:22][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0003,	1.1723 s / batch. (data: 3.10e-04). ETA=5 days, 11:57:25, max mem: 15.1 GB 
[06/18 12:20:19][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0004,	1.1693 s / batch. (data: 3.16e-04). ETA=5 days, 11:34:52, max mem: 15.1 GB 
[06/18 12:22:17][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0002,	1.1623 s / batch. (data: 5.18e-04). ETA=5 days, 10:45:45, max mem: 15.1 GB 
[06/18 12:24:14][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0002,	1.1671 s / batch. (data: 2.88e-04). ETA=5 days, 11:16:10, max mem: 15.1 GB 
[06/18 12:26:11][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0002,	1.1752 s / batch. (data: 3.27e-04). ETA=5 days, 12:09:13, max mem: 15.1 GB 
[06/18 12:28:08][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0001,	1.1735 s / batch. (data: 4.05e-04). ETA=5 days, 11:55:39, max mem: 15.1 GB 
[06/18 12:30:05][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0003,	1.1708 s / batch. (data: 3.59e-04). ETA=5 days, 11:35:35, max mem: 15.1 GB 
[06/18 12:32:02][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0001,	1.1717 s / batch. (data: 2.89e-04). ETA=5 days, 11:39:54, max mem: 15.1 GB 
[06/18 12:33:59][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0001,	1.1687 s / batch. (data: 2.83e-04). ETA=5 days, 11:17:26, max mem: 15.1 GB 
[06/18 12:35:56][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0001,	1.1693 s / batch. (data: 3.15e-04). ETA=5 days, 11:19:20, max mem: 15.1 GB 
[06/18 12:37:53][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0003,	1.1665 s / batch. (data: 3.20e-04). ETA=5 days, 10:58:50, max mem: 15.1 GB 
[06/18 12:39:50][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0003,	1.1722 s / batch. (data: 3.18e-04). ETA=5 days, 11:35:10, max mem: 15.1 GB 
[06/18 12:41:47][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0003,	1.1688 s / batch. (data: 3.38e-04). ETA=5 days, 11:10:30, max mem: 15.1 GB 
[06/18 12:43:44][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0016,	1.1677 s / batch. (data: 3.26e-04). ETA=5 days, 11:00:50, max mem: 15.1 GB 
[06/18 12:45:41][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0001,	1.1695 s / batch. (data: 3.09e-04). ETA=5 days, 11:11:06, max mem: 15.1 GB 
[06/18 12:47:37][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0001,	1.1718 s / batch. (data: 3.57e-04). ETA=5 days, 11:25:02, max mem: 15.1 GB 
[06/18 12:49:34][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0003,	1.1679 s / batch. (data: 3.78e-04). ETA=5 days, 10:56:36, max mem: 15.1 GB 
[06/18 12:51:31][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0003,	1.1687 s / batch. (data: 4.60e-04). ETA=5 days, 11:00:09, max mem: 15.1 GB 
[06/18 12:53:28][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0003,	1.1695 s / batch. (data: 4.57e-04). ETA=5 days, 11:03:11, max mem: 15.1 GB 
[06/18 12:55:25][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0003,	1.1718 s / batch. (data: 2.93e-04). ETA=5 days, 11:17:00, max mem: 15.1 GB 
[06/18 12:57:22][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0003,	1.1675 s / batch. (data: 4.22e-04). ETA=5 days, 10:46:00, max mem: 15.1 GB 
[06/18 12:59:19][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0002,	1.1656 s / batch. (data: 3.65e-04). ETA=5 days, 10:31:22, max mem: 15.1 GB 
[06/18 13:01:16][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0007,	1.1702 s / batch. (data: 3.51e-04). ETA=5 days, 11:00:02, max mem: 15.1 GB 
[06/18 13:03:14][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0003,	1.1668 s / batch. (data: 3.44e-04). ETA=5 days, 10:35:13, max mem: 15.1 GB 
[06/18 13:05:11][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0009,	1.1700 s / batch. (data: 3.43e-04). ETA=5 days, 10:54:51, max mem: 15.1 GB 
[06/18 13:07:08][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0002,	1.1753 s / batch. (data: 3.37e-04). ETA=5 days, 11:28:28, max mem: 15.1 GB 
[06/18 13:09:05][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0002,	1.1789 s / batch. (data: 3.81e-04). ETA=5 days, 11:51:09, max mem: 15.1 GB 
[06/18 13:11:03][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0001,	1.1743 s / batch. (data: 3.78e-04). ETA=5 days, 11:18:14, max mem: 15.1 GB 
[06/18 13:13:00][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0002,	1.1731 s / batch. (data: 3.42e-04). ETA=5 days, 11:08:17, max mem: 15.1 GB 
[06/18 13:14:57][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0002,	1.1750 s / batch. (data: 3.22e-04). ETA=5 days, 11:19:10, max mem: 15.1 GB 
[06/18 13:16:54][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0002,	1.1764 s / batch. (data: 3.75e-04). ETA=5 days, 11:26:14, max mem: 15.1 GB 
[06/18 13:18:51][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0003,	1.1687 s / batch. (data: 3.49e-04). ETA=5 days, 10:32:41, max mem: 15.1 GB 
[06/18 13:20:48][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0016,	1.1659 s / batch. (data: 3.39e-04). ETA=5 days, 10:11:58, max mem: 15.1 GB 
[06/18 13:22:45][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0002,	1.1709 s / batch. (data: 3.26e-04). ETA=5 days, 10:43:26, max mem: 15.1 GB 
[06/18 13:24:42][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0004,	1.1687 s / batch. (data: 4.09e-04). ETA=5 days, 10:27:07, max mem: 15.1 GB 
[06/18 13:26:39][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0001,	1.1621 s / batch. (data: 3.29e-04). ETA=5 days, 9:40:41, max mem: 15.1 GB 
[06/18 13:28:36][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0002,	1.1710 s / batch. (data: 3.39e-04). ETA=5 days, 10:38:10, max mem: 15.1 GB 
[06/18 13:30:33][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0003,	1.1683 s / batch. (data: 3.43e-04). ETA=5 days, 10:18:27, max mem: 15.1 GB 
[06/18 13:32:29][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0002,	1.1712 s / batch. (data: 5.40e-04). ETA=5 days, 10:35:38, max mem: 15.1 GB 
[06/18 13:34:26][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0002,	1.1649 s / batch. (data: 3.50e-04). ETA=5 days, 9:51:46, max mem: 15.1 GB 
[06/18 13:36:23][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0003,	1.1687 s / batch. (data: 3.57e-04). ETA=5 days, 10:15:29, max mem: 15.1 GB 
[06/18 13:38:20][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0002,	1.1644 s / batch. (data: 3.11e-04). ETA=5 days, 9:44:39, max mem: 15.1 GB 
[06/18 13:40:17][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0003,	1.1678 s / batch. (data: 3.22e-04). ETA=5 days, 10:05:00, max mem: 15.1 GB 
[06/18 13:42:14][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0003,	1.1761 s / batch. (data: 3.38e-04). ETA=5 days, 10:59:02, max mem: 15.1 GB 
[06/18 13:44:11][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0018,	1.1703 s / batch. (data: 3.46e-04). ETA=5 days, 10:17:47, max mem: 15.1 GB 
[06/18 13:46:09][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0032,	1.1714 s / batch. (data: 3.10e-04). ETA=5 days, 10:23:46, max mem: 15.1 GB 
[06/18 13:48:06][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0012,	1.1695 s / batch. (data: 3.22e-04). ETA=5 days, 10:08:36, max mem: 15.1 GB 
[06/18 13:50:03][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0003,	1.1680 s / batch. (data: 3.86e-04). ETA=5 days, 9:57:03, max mem: 15.1 GB 
[06/18 13:52:00][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0003,	1.1698 s / batch. (data: 3.51e-04). ETA=5 days, 10:06:58, max mem: 15.1 GB 
[06/18 13:53:57][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0031,	1.1641 s / batch. (data: 1.05e-04). ETA=5 days, 9:27:01, max mem: 15.1 GB 
[06/18 13:54:03][INFO] visual_prompt:  319: Epoch 20 / 100: avg data time: 4.36e-03, avg batch time: 1.1793, average train loss: 0.0004
[06/18 14:02:55][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1760 s / batch. (data: 7.82e-05)max mem: 15.06516 GB 
[06/18 14:11:10][INFO] visual_prompt:  476: Inference (val):avg data time: 1.85e-04, avg batch time: 5.1481, average loss: 0.0001
[06/18 14:11:10][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/18 14:11:10][INFO] visual_prompt:  257: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[06/18 14:13:50][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0003,	1.1682 s / batch. (data: 3.47e-04). ETA=5 days, 9:52:21, max mem: 15.1 GB 
[06/18 14:15:47][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0001,	1.1646 s / batch. (data: 2.68e-04). ETA=5 days, 9:26:14, max mem: 15.1 GB 
[06/18 14:17:43][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0002,	1.1676 s / batch. (data: 2.81e-04). ETA=5 days, 9:44:37, max mem: 15.1 GB 
[06/18 14:19:40][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0001,	1.1681 s / batch. (data: 2.93e-04). ETA=5 days, 9:45:46, max mem: 15.1 GB 
[06/18 14:21:37][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0023,	1.1689 s / batch. (data: 3.46e-04). ETA=5 days, 9:49:23, max mem: 15.1 GB 
[06/18 14:23:34][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0002,	1.1705 s / batch. (data: 3.19e-04). ETA=5 days, 9:58:08, max mem: 15.1 GB 
[06/18 14:25:31][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0004,	1.1673 s / batch. (data: 3.25e-04). ETA=5 days, 9:34:55, max mem: 15.1 GB 
[06/18 14:27:28][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0001,	1.1721 s / batch. (data: 3.59e-04). ETA=5 days, 10:04:46, max mem: 15.1 GB 
[06/18 14:29:25][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0002,	1.1717 s / batch. (data: 3.94e-04). ETA=5 days, 10:00:03, max mem: 15.1 GB 
[06/18 14:31:23][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0002,	1.1724 s / batch. (data: 3.64e-04). ETA=5 days, 10:02:25, max mem: 15.1 GB 
[06/18 14:33:20][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0003,	1.1700 s / batch. (data: 3.21e-04). ETA=5 days, 9:45:06, max mem: 15.1 GB 
[06/18 14:35:17][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0002,	1.1783 s / batch. (data: 3.47e-04). ETA=5 days, 10:38:04, max mem: 15.1 GB 
[06/18 14:37:14][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0010,	1.1767 s / batch. (data: 5.06e-04). ETA=5 days, 10:25:38, max mem: 15.1 GB 
[06/18 14:39:12][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0002,	1.1666 s / batch. (data: 2.87e-04). ETA=5 days, 9:16:15, max mem: 15.1 GB 
[06/18 14:41:09][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0003,	1.1697 s / batch. (data: 3.49e-04). ETA=5 days, 9:35:18, max mem: 15.1 GB 
[06/18 14:43:06][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0001,	1.1668 s / batch. (data: 3.14e-04). ETA=5 days, 9:13:59, max mem: 15.1 GB 
[06/18 14:45:03][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0004,	1.1705 s / batch. (data: 3.10e-04). ETA=5 days, 9:36:44, max mem: 15.1 GB 
[06/18 14:46:59][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0002,	1.1673 s / batch. (data: 3.64e-04). ETA=5 days, 9:13:26, max mem: 15.1 GB 
[06/18 14:48:56][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0001,	1.1668 s / batch. (data: 3.70e-04). ETA=5 days, 9:08:06, max mem: 15.1 GB 
[06/18 14:50:53][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0001,	1.1675 s / batch. (data: 3.12e-04). ETA=5 days, 9:10:57, max mem: 15.1 GB 
[06/18 14:52:50][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0003,	1.1725 s / batch. (data: 4.20e-04). ETA=5 days, 9:41:44, max mem: 15.1 GB 
[06/18 14:54:46][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0001,	1.1653 s / batch. (data: 3.13e-04). ETA=5 days, 8:52:04, max mem: 15.1 GB 
[06/18 14:56:43][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0005,	1.1645 s / batch. (data: 2.97e-04). ETA=5 days, 8:45:06, max mem: 15.1 GB 
[06/18 14:58:40][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0003,	1.1723 s / batch. (data: 4.01e-04). ETA=5 days, 9:34:40, max mem: 15.1 GB 
[06/18 15:00:37][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0010,	1.1719 s / batch. (data: 3.15e-04). ETA=5 days, 9:30:12, max mem: 15.1 GB 
[06/18 15:02:33][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0003,	1.1673 s / batch. (data: 4.17e-04). ETA=5 days, 8:57:27, max mem: 15.1 GB 
[06/18 15:04:30][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0001,	1.1699 s / batch. (data: 2.84e-04). ETA=5 days, 9:12:56, max mem: 15.1 GB 
[06/18 15:06:27][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0005,	1.1695 s / batch. (data: 3.29e-04). ETA=5 days, 9:08:27, max mem: 15.1 GB 
[06/18 15:08:24][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0003,	1.1709 s / batch. (data: 3.47e-04). ETA=5 days, 9:15:23, max mem: 15.1 GB 
[06/18 15:10:21][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0005,	1.1746 s / batch. (data: 3.34e-04). ETA=5 days, 9:38:28, max mem: 15.1 GB 
[06/18 15:12:18][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0003,	1.1702 s / batch. (data: 3.71e-04). ETA=5 days, 9:07:21, max mem: 15.1 GB 
[06/18 15:14:15][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0002,	1.1725 s / batch. (data: 3.38e-04). ETA=5 days, 9:20:06, max mem: 15.1 GB 
[06/18 15:16:12][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0002,	1.1737 s / batch. (data: 3.19e-04). ETA=5 days, 9:26:05, max mem: 15.1 GB 
[06/18 15:18:09][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0003,	1.1763 s / batch. (data: 3.58e-04). ETA=5 days, 9:41:43, max mem: 15.1 GB 
[06/18 15:20:06][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0002,	1.1688 s / batch. (data: 3.89e-04). ETA=5 days, 8:50:18, max mem: 15.1 GB 
[06/18 15:22:03][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0005,	1.1714 s / batch. (data: 3.48e-04). ETA=5 days, 9:05:26, max mem: 15.1 GB 
[06/18 15:24:00][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0003,	1.1679 s / batch. (data: 3.13e-04). ETA=5 days, 8:40:25, max mem: 15.1 GB 
[06/18 15:25:57][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0003,	1.1690 s / batch. (data: 3.81e-04). ETA=5 days, 8:45:32, max mem: 15.1 GB 
[06/18 15:27:54][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0003,	1.1788 s / batch. (data: 3.19e-04). ETA=5 days, 9:48:17, max mem: 15.1 GB 
[06/18 15:29:51][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0002,	1.1707 s / batch. (data: 2.78e-04). ETA=5 days, 8:52:38, max mem: 15.1 GB 
[06/18 15:31:48][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0003,	1.1656 s / batch. (data: 3.11e-04). ETA=5 days, 8:17:13, max mem: 15.1 GB 
[06/18 15:33:45][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0002,	1.1652 s / batch. (data: 3.46e-04). ETA=5 days, 8:12:25, max mem: 15.1 GB 
[06/18 15:35:42][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0004,	1.1697 s / batch. (data: 3.41e-04). ETA=5 days, 8:40:05, max mem: 15.1 GB 
[06/18 15:37:38][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0002,	1.1680 s / batch. (data: 3.18e-04). ETA=5 days, 8:27:18, max mem: 15.1 GB 
[06/18 15:39:35][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0002,	1.1681 s / batch. (data: 3.43e-04). ETA=5 days, 8:25:58, max mem: 15.1 GB 
[06/18 15:41:32][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0001,	1.1687 s / batch. (data: 3.51e-04). ETA=5 days, 8:27:38, max mem: 15.1 GB 
[06/18 15:43:29][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0001,	1.1705 s / batch. (data: 3.11e-04). ETA=5 days, 8:38:09, max mem: 15.1 GB 
[06/18 15:45:25][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0003,	1.1682 s / batch. (data: 3.35e-04). ETA=5 days, 8:20:50, max mem: 15.1 GB 
[06/18 15:47:22][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0002,	1.1747 s / batch. (data: 3.51e-04). ETA=5 days, 9:01:41, max mem: 15.1 GB 
[06/18 15:49:20][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0002,	1.1623 s / batch. (data: 1.14e-04). ETA=5 days, 7:37:45, max mem: 15.1 GB 
[06/18 15:49:26][INFO] visual_prompt:  319: Epoch 21 / 100: avg data time: 4.36e-03, avg batch time: 1.1779, average train loss: 0.0003
[06/18 15:58:19][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.2200 s / batch. (data: 1.40e-04)max mem: 15.06516 GB 
[06/18 16:06:35][INFO] visual_prompt:  476: Inference (val):avg data time: 1.81e-04, avg batch time: 5.1592, average loss: 0.0001
[06/18 16:06:35][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/18 16:06:35][INFO] visual_prompt:  257: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[06/18 16:09:14][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0001,	1.1747 s / batch. (data: 3.25e-04). ETA=5 days, 8:57:22, max mem: 15.1 GB 
[06/18 16:11:11][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0002,	1.1668 s / batch. (data: 3.64e-04). ETA=5 days, 8:04:00, max mem: 15.1 GB 
[06/18 16:13:08][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0003,	1.1688 s / batch. (data: 3.78e-04). ETA=5 days, 8:14:55, max mem: 15.1 GB 
[06/18 16:15:05][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0002,	1.1684 s / batch. (data: 2.96e-04). ETA=5 days, 8:10:27, max mem: 15.1 GB 
[06/18 16:17:01][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0003,	1.1709 s / batch. (data: 3.47e-04). ETA=5 days, 8:25:00, max mem: 15.1 GB 
[06/18 16:18:58][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0035,	1.1663 s / batch. (data: 3.41e-04). ETA=5 days, 7:52:36, max mem: 15.1 GB 
[06/18 16:20:55][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0001,	1.1682 s / batch. (data: 3.22e-04). ETA=5 days, 8:03:27, max mem: 15.1 GB 
[06/18 16:22:52][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0002,	1.1692 s / batch. (data: 3.14e-04). ETA=5 days, 8:07:41, max mem: 15.1 GB 
[06/18 16:24:48][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0005,	1.1693 s / batch. (data: 4.85e-04). ETA=5 days, 8:06:42, max mem: 15.1 GB 
[06/18 16:26:45][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0024,	1.1655 s / batch. (data: 3.03e-04). ETA=5 days, 7:39:49, max mem: 15.1 GB 
[06/18 16:28:42][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0002,	1.1711 s / batch. (data: 3.30e-04). ETA=5 days, 8:14:06, max mem: 15.1 GB 
[06/18 16:30:39][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0002,	1.1626 s / batch. (data: 3.73e-04). ETA=5 days, 7:16:54, max mem: 15.1 GB 
[06/18 16:32:36][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0002,	1.1719 s / batch. (data: 3.36e-04). ETA=5 days, 8:15:42, max mem: 15.1 GB 
[06/18 16:34:33][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0003,	1.1662 s / batch. (data: 2.74e-04). ETA=5 days, 7:36:21, max mem: 15.1 GB 
[06/18 16:36:30][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0002,	1.1704 s / batch. (data: 4.40e-04). ETA=5 days, 8:02:13, max mem: 15.1 GB 
[06/18 16:38:27][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0002,	1.1692 s / batch. (data: 4.59e-04). ETA=5 days, 7:52:12, max mem: 15.1 GB 
[06/18 16:40:24][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0002,	1.1690 s / batch. (data: 3.58e-04). ETA=5 days, 7:48:42, max mem: 15.1 GB 
[06/18 16:42:21][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0003,	1.1688 s / batch. (data: 3.14e-04). ETA=5 days, 7:45:42, max mem: 15.1 GB 
[06/18 16:44:18][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0002,	1.1708 s / batch. (data: 3.35e-04). ETA=5 days, 7:56:36, max mem: 15.1 GB 
[06/18 16:46:15][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0002,	1.1737 s / batch. (data: 5.24e-04). ETA=5 days, 8:13:59, max mem: 15.1 GB 
[06/18 16:48:12][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0004,	1.1727 s / batch. (data: 2.92e-04). ETA=5 days, 8:05:15, max mem: 15.1 GB 
[06/18 16:50:09][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0056,	1.1679 s / batch. (data: 3.42e-04). ETA=5 days, 7:32:17, max mem: 15.1 GB 
[06/18 16:52:06][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0002,	1.1755 s / batch. (data: 3.61e-04). ETA=5 days, 8:19:58, max mem: 15.1 GB 
[06/18 16:54:03][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0002,	1.1678 s / batch. (data: 3.57e-04). ETA=5 days, 7:27:37, max mem: 15.1 GB 
[06/18 16:56:00][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0003,	1.1683 s / batch. (data: 3.27e-04). ETA=5 days, 7:28:36, max mem: 15.1 GB 
[06/18 16:57:57][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0003,	1.1725 s / batch. (data: 3.31e-04). ETA=5 days, 7:54:11, max mem: 15.1 GB 
[06/18 16:59:54][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0001,	1.1684 s / batch. (data: 3.18e-04). ETA=5 days, 7:25:25, max mem: 15.1 GB 
[06/18 17:01:50][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0004,	1.1706 s / batch. (data: 3.21e-04). ETA=5 days, 7:38:03, max mem: 15.1 GB 
[06/18 17:03:47][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0003,	1.1660 s / batch. (data: 3.18e-04). ETA=5 days, 7:06:07, max mem: 15.1 GB 
[06/18 17:05:44][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0001,	1.1714 s / batch. (data: 3.69e-04). ETA=5 days, 7:39:17, max mem: 15.1 GB 
[06/18 17:07:41][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0001,	1.1675 s / batch. (data: 3.26e-04). ETA=5 days, 7:12:09, max mem: 15.1 GB 
[06/18 17:09:37][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0002,	1.1680 s / batch. (data: 3.74e-04). ETA=5 days, 7:12:56, max mem: 15.1 GB 
[06/18 17:11:34][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0001,	1.1673 s / batch. (data: 3.33e-04). ETA=5 days, 7:06:44, max mem: 15.1 GB 
[06/18 17:13:31][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0005,	1.1718 s / batch. (data: 3.48e-04). ETA=5 days, 7:34:16, max mem: 15.1 GB 
[06/18 17:15:28][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0002,	1.1702 s / batch. (data: 3.07e-04). ETA=5 days, 7:21:44, max mem: 15.1 GB 
[06/18 17:17:25][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0003,	1.1717 s / batch. (data: 4.09e-04). ETA=5 days, 7:29:24, max mem: 15.1 GB 
[06/18 17:19:22][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0003,	1.1729 s / batch. (data: 3.60e-04). ETA=5 days, 7:35:31, max mem: 15.1 GB 
[06/18 17:21:19][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0003,	1.1776 s / batch. (data: 3.68e-04). ETA=5 days, 8:04:23, max mem: 15.1 GB 
[06/18 17:23:16][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0002,	1.1744 s / batch. (data: 3.08e-04). ETA=5 days, 7:41:27, max mem: 15.1 GB 
[06/18 17:25:13][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0027,	1.1737 s / batch. (data: 3.22e-04). ETA=5 days, 7:34:30, max mem: 15.1 GB 
[06/18 17:27:10][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0003,	1.1722 s / batch. (data: 3.02e-04). ETA=5 days, 7:23:16, max mem: 15.1 GB 
[06/18 17:29:07][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0001,	1.1708 s / batch. (data: 3.39e-04). ETA=5 days, 7:12:11, max mem: 15.1 GB 
[06/18 17:31:04][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0001,	1.1686 s / batch. (data: 3.14e-04). ETA=5 days, 6:55:30, max mem: 15.1 GB 
[06/18 17:33:01][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0002,	1.1666 s / batch. (data: 2.55e-04). ETA=5 days, 6:40:23, max mem: 15.1 GB 
[06/18 17:34:58][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0038,	1.1694 s / batch. (data: 3.49e-04). ETA=5 days, 6:56:49, max mem: 15.1 GB 
[06/18 17:36:55][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0015,	1.1680 s / batch. (data: 3.52e-04). ETA=5 days, 6:45:54, max mem: 15.1 GB 
[06/18 17:38:52][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0002,	1.1676 s / batch. (data: 3.39e-04). ETA=5 days, 6:41:09, max mem: 15.1 GB 
[06/18 17:40:48][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0021,	1.1686 s / batch. (data: 3.69e-04). ETA=5 days, 6:46:03, max mem: 15.1 GB 
[06/18 17:42:45][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0001,	1.1671 s / batch. (data: 3.02e-04). ETA=5 days, 6:34:04, max mem: 15.1 GB 
[06/18 17:44:42][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0001,	1.1624 s / batch. (data: 1.68e-04). ETA=5 days, 6:01:52, max mem: 15.1 GB 
[06/18 17:44:47][INFO] visual_prompt:  319: Epoch 22 / 100: avg data time: 4.44e-03, avg batch time: 1.1773, average train loss: 0.0005
[06/18 17:53:40][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1658 s / batch. (data: 1.07e-04)max mem: 15.06516 GB 
[06/18 18:01:55][INFO] visual_prompt:  476: Inference (val):avg data time: 2.03e-04, avg batch time: 5.1523, average loss: 0.0001
[06/18 18:01:55][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/18 18:01:56][INFO] visual_prompt:  257: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[06/18 18:04:37][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0003,	1.1703 s / batch. (data: 4.06e-04). ETA=5 days, 6:50:59, max mem: 15.1 GB 
[06/18 18:06:34][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0001,	1.1681 s / batch. (data: 3.27e-04). ETA=5 days, 6:34:57, max mem: 15.1 GB 
[06/18 18:08:31][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0002,	1.1667 s / batch. (data: 3.54e-04). ETA=5 days, 6:23:47, max mem: 15.1 GB 
[06/18 18:10:27][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0005,	1.1740 s / batch. (data: 3.64e-04). ETA=5 days, 7:09:22, max mem: 15.1 GB 
[06/18 18:12:24][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0005,	1.1683 s / batch. (data: 3.44e-04). ETA=5 days, 6:30:08, max mem: 15.1 GB 
[06/18 18:14:21][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0003,	1.1725 s / batch. (data: 3.41e-04). ETA=5 days, 6:55:40, max mem: 15.1 GB 
[06/18 18:16:18][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0001,	1.1655 s / batch. (data: 2.93e-04). ETA=5 days, 6:08:15, max mem: 15.1 GB 
[06/18 18:18:15][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0001,	1.1698 s / batch. (data: 3.02e-04). ETA=5 days, 6:34:11, max mem: 15.1 GB 
[06/18 18:20:12][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0001,	1.1637 s / batch. (data: 3.42e-04). ETA=5 days, 5:52:51, max mem: 15.1 GB 
[06/18 18:22:08][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0002,	1.1654 s / batch. (data: 3.12e-04). ETA=5 days, 6:01:31, max mem: 15.1 GB 
[06/18 18:24:05][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0002,	1.1658 s / batch. (data: 3.00e-04). ETA=5 days, 6:02:21, max mem: 15.1 GB 
[06/18 18:26:02][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0002,	1.1674 s / batch. (data: 3.17e-04). ETA=5 days, 6:10:52, max mem: 15.1 GB 
[06/18 18:27:58][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0002,	1.1714 s / batch. (data: 3.59e-04). ETA=5 days, 6:34:57, max mem: 15.1 GB 
[06/18 18:29:55][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0003,	1.1692 s / batch. (data: 3.33e-04). ETA=5 days, 6:18:38, max mem: 15.1 GB 
[06/18 18:31:52][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0002,	1.1710 s / batch. (data: 3.24e-04). ETA=5 days, 6:28:36, max mem: 15.1 GB 
[06/18 18:33:48][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0002,	1.1669 s / batch. (data: 3.39e-04). ETA=5 days, 5:59:48, max mem: 15.1 GB 
[06/18 18:35:45][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0002,	1.1662 s / batch. (data: 2.89e-04). ETA=5 days, 5:53:22, max mem: 15.1 GB 
[06/18 18:37:42][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0002,	1.1732 s / batch. (data: 3.56e-04). ETA=5 days, 6:36:27, max mem: 15.1 GB 
[06/18 18:39:39][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0001,	1.1681 s / batch. (data: 3.48e-04). ETA=5 days, 6:01:50, max mem: 15.1 GB 
[06/18 18:41:36][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0003,	1.1679 s / batch. (data: 2.74e-04). ETA=5 days, 5:58:40, max mem: 15.1 GB 
[06/18 18:43:33][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0005,	1.1702 s / batch. (data: 2.98e-04). ETA=5 days, 6:11:37, max mem: 15.1 GB 
[06/18 18:45:30][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0001,	1.1698 s / batch. (data: 3.10e-04). ETA=5 days, 6:06:39, max mem: 15.1 GB 
[06/18 18:47:27][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0001,	1.1661 s / batch. (data: 3.98e-04). ETA=5 days, 5:40:59, max mem: 15.1 GB 
[06/18 18:49:24][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0003,	1.1706 s / batch. (data: 3.96e-04). ETA=5 days, 6:08:28, max mem: 15.1 GB 
[06/18 18:51:21][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0003,	1.1761 s / batch. (data: 3.37e-04). ETA=5 days, 6:41:49, max mem: 15.1 GB 
[06/18 18:53:18][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0003,	1.1761 s / batch. (data: 3.08e-04). ETA=5 days, 6:39:31, max mem: 15.1 GB 
[06/18 18:55:15][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0004,	1.1706 s / batch. (data: 4.14e-04). ETA=5 days, 6:02:07, max mem: 15.1 GB 
[06/18 18:57:12][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0003,	1.1717 s / batch. (data: 3.56e-04). ETA=5 days, 6:07:27, max mem: 15.1 GB 
[06/18 18:59:09][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0001,	1.1670 s / batch. (data: 3.45e-04). ETA=5 days, 5:35:12, max mem: 15.1 GB 
[06/18 19:01:05][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0004,	1.1679 s / batch. (data: 2.89e-04). ETA=5 days, 5:38:46, max mem: 15.1 GB 
[06/18 19:03:02][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0002,	1.1677 s / batch. (data: 3.34e-04). ETA=5 days, 5:35:44, max mem: 15.1 GB 
[06/18 19:04:59][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0002,	1.1683 s / batch. (data: 3.23e-04). ETA=5 days, 5:38:00, max mem: 15.1 GB 
[06/18 19:06:55][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0002,	1.1662 s / batch. (data: 3.30e-04). ETA=5 days, 5:22:03, max mem: 15.1 GB 
[06/18 19:08:52][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0003,	1.1672 s / batch. (data: 3.17e-04). ETA=5 days, 5:26:34, max mem: 15.1 GB 
[06/18 19:10:48][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0002,	1.1688 s / batch. (data: 3.38e-04). ETA=5 days, 5:35:20, max mem: 15.1 GB 
[06/18 19:12:45][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0001,	1.1679 s / batch. (data: 3.50e-04). ETA=5 days, 5:27:30, max mem: 15.1 GB 
[06/18 19:14:42][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0002,	1.1705 s / batch. (data: 3.09e-04). ETA=5 days, 5:41:56, max mem: 15.1 GB 
[06/18 19:16:38][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0001,	1.1735 s / batch. (data: 3.65e-04). ETA=5 days, 5:59:27, max mem: 15.1 GB 
[06/18 19:18:35][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0002,	1.1677 s / batch. (data: 3.87e-04). ETA=5 days, 5:20:32, max mem: 15.1 GB 
[06/18 19:20:32][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0001,	1.1706 s / batch. (data: 3.73e-04). ETA=5 days, 5:37:05, max mem: 15.1 GB 
[06/18 19:22:29][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0002,	1.1693 s / batch. (data: 3.48e-04). ETA=5 days, 5:26:33, max mem: 15.1 GB 
[06/18 19:24:26][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0002,	1.1683 s / batch. (data: 3.31e-04). ETA=5 days, 5:18:12, max mem: 15.1 GB 
[06/18 19:26:23][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0002,	1.1682 s / batch. (data: 3.29e-04). ETA=5 days, 5:15:33, max mem: 15.1 GB 
[06/18 19:28:20][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0005,	1.1698 s / batch. (data: 4.48e-04). ETA=5 days, 5:24:19, max mem: 15.1 GB 
[06/18 19:30:17][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0002,	1.1673 s / batch. (data: 3.77e-04). ETA=5 days, 5:05:56, max mem: 15.1 GB 
[06/18 19:32:14][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0001,	1.1679 s / batch. (data: 3.60e-04). ETA=5 days, 5:07:43, max mem: 15.1 GB 
[06/18 19:34:11][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0001,	1.1720 s / batch. (data: 2.94e-04). ETA=5 days, 5:31:59, max mem: 15.1 GB 
[06/18 19:36:08][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0003,	1.1714 s / batch. (data: 3.28e-04). ETA=5 days, 5:26:41, max mem: 15.1 GB 
[06/18 19:38:05][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0002,	1.1709 s / batch. (data: 3.74e-04). ETA=5 days, 5:21:17, max mem: 15.1 GB 
[06/18 19:40:02][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0001,	1.1673 s / batch. (data: 1.39e-04). ETA=5 days, 4:56:01, max mem: 15.1 GB 
[06/18 19:40:08][INFO] visual_prompt:  319: Epoch 23 / 100: avg data time: 4.41e-03, avg batch time: 1.1773, average train loss: 0.0002
[06/18 19:49:00][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1616 s / batch. (data: 1.31e-04)max mem: 15.06516 GB 
[06/18 19:57:14][INFO] visual_prompt:  476: Inference (val):avg data time: 1.59e-04, avg batch time: 5.1401, average loss: 0.0001
[06/18 19:57:14][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/18 19:57:14][INFO] visual_prompt:  257: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[06/18 19:59:56][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0001,	1.1662 s / batch. (data: 3.65e-04). ETA=5 days, 4:47:18, max mem: 15.1 GB 
[06/18 20:01:53][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0001,	1.1723 s / batch. (data: 3.18e-04). ETA=5 days, 5:24:37, max mem: 15.1 GB 
[06/18 20:03:50][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0002,	1.1709 s / batch. (data: 3.15e-04). ETA=5 days, 5:13:41, max mem: 15.1 GB 
[06/18 20:05:47][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0003,	1.1672 s / batch. (data: 3.11e-04). ETA=5 days, 4:47:39, max mem: 15.1 GB 
[06/18 20:07:44][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0002,	1.1704 s / batch. (data: 2.66e-04). ETA=5 days, 5:06:16, max mem: 15.1 GB 
[06/18 20:09:41][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0002,	1.1702 s / batch. (data: 3.09e-04). ETA=5 days, 5:02:56, max mem: 15.1 GB 
[06/18 20:11:39][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0001,	1.1681 s / batch. (data: 3.02e-04). ETA=5 days, 4:47:33, max mem: 15.1 GB 
[06/18 20:13:36][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0001,	1.1687 s / batch. (data: 3.04e-04). ETA=5 days, 4:49:28, max mem: 15.1 GB 
[06/18 20:15:33][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0003,	1.1723 s / batch. (data: 3.19e-04). ETA=5 days, 5:10:28, max mem: 15.1 GB 
[06/18 20:17:30][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0002,	1.1694 s / batch. (data: 3.57e-04). ETA=5 days, 4:49:58, max mem: 15.1 GB 
[06/18 20:19:27][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0002,	1.1725 s / batch. (data: 3.23e-04). ETA=5 days, 5:08:19, max mem: 15.1 GB 
[06/18 20:21:24][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0002,	1.1662 s / batch. (data: 3.18e-04). ETA=5 days, 4:25:50, max mem: 15.1 GB 
[06/18 20:23:21][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0004,	1.1665 s / batch. (data: 2.95e-04). ETA=5 days, 4:25:31, max mem: 15.1 GB 
[06/18 20:25:18][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0002,	1.1712 s / batch. (data: 3.18e-04). ETA=5 days, 4:54:05, max mem: 15.1 GB 
[06/18 20:27:15][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0001,	1.1723 s / batch. (data: 3.47e-04). ETA=5 days, 4:59:12, max mem: 15.1 GB 
[06/18 20:29:12][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0002,	1.1685 s / batch. (data: 3.43e-04). ETA=5 days, 4:32:47, max mem: 15.1 GB 
[06/18 20:31:09][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0003,	1.1740 s / batch. (data: 3.19e-04). ETA=5 days, 5:05:58, max mem: 15.1 GB 
[06/18 20:33:06][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0002,	1.1689 s / batch. (data: 3.05e-04). ETA=5 days, 4:31:37, max mem: 15.1 GB 
[06/18 20:35:02][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0002,	1.1677 s / batch. (data: 3.31e-04). ETA=5 days, 4:22:03, max mem: 15.1 GB 
[06/18 20:36:59][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0003,	1.1661 s / batch. (data: 3.20e-04). ETA=5 days, 4:09:32, max mem: 15.1 GB 
[06/18 20:38:56][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0001,	1.1663 s / batch. (data: 3.44e-04). ETA=5 days, 4:09:03, max mem: 15.1 GB 
[06/18 20:40:53][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0004,	1.1683 s / batch. (data: 3.13e-04). ETA=5 days, 4:19:40, max mem: 15.1 GB 
[06/18 20:42:50][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0002,	1.1724 s / batch. (data: 3.81e-04). ETA=5 days, 4:43:42, max mem: 15.1 GB 
[06/18 20:44:47][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0002,	1.1683 s / batch. (data: 3.16e-04). ETA=5 days, 4:15:42, max mem: 15.1 GB 
[06/18 20:46:44][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0001,	1.1687 s / batch. (data: 3.28e-04). ETA=5 days, 4:16:41, max mem: 15.1 GB 
[06/18 20:48:41][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0002,	1.1713 s / batch. (data: 2.70e-04). ETA=5 days, 4:30:53, max mem: 15.1 GB 
[06/18 20:50:38][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0002,	1.1780 s / batch. (data: 2.98e-04). ETA=5 days, 5:11:48, max mem: 15.1 GB 
[06/18 20:52:35][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0003,	1.1729 s / batch. (data: 3.12e-04). ETA=5 days, 4:37:08, max mem: 15.1 GB 
[06/18 20:54:32][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0003,	1.1716 s / batch. (data: 3.69e-04). ETA=5 days, 4:27:12, max mem: 15.1 GB 
[06/18 20:56:30][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0003,	1.1751 s / batch. (data: 3.35e-04). ETA=5 days, 4:47:49, max mem: 15.1 GB 
[06/18 20:58:27][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0003,	1.1726 s / batch. (data: 3.61e-04). ETA=5 days, 4:29:48, max mem: 15.1 GB 
[06/18 21:00:24][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0003,	1.1685 s / batch. (data: 3.43e-04). ETA=5 days, 4:01:18, max mem: 15.1 GB 
[06/18 21:02:21][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0003,	1.1803 s / batch. (data: 3.24e-04). ETA=5 days, 5:14:46, max mem: 15.1 GB 
[06/18 21:04:18][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0002,	1.1656 s / batch. (data: 3.23e-04). ETA=5 days, 3:39:02, max mem: 15.1 GB 
[06/18 21:06:15][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0001,	1.1662 s / batch. (data: 3.49e-04). ETA=5 days, 3:41:09, max mem: 15.1 GB 
[06/18 21:08:12][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0003,	1.1686 s / batch. (data: 3.26e-04). ETA=5 days, 3:54:35, max mem: 15.1 GB 
[06/18 21:10:08][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0001,	1.1682 s / batch. (data: 3.55e-04). ETA=5 days, 3:49:36, max mem: 15.1 GB 
[06/18 21:12:05][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0002,	1.1755 s / batch. (data: 3.26e-04). ETA=5 days, 4:34:11, max mem: 15.1 GB 
[06/18 21:14:03][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0001,	1.1762 s / batch. (data: 3.44e-04). ETA=5 days, 4:36:54, max mem: 15.1 GB 
[06/18 21:16:00][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0002,	1.1726 s / batch. (data: 3.10e-04). ETA=5 days, 4:12:12, max mem: 15.1 GB 
[06/18 21:17:58][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0002,	1.1710 s / batch. (data: 3.25e-04). ETA=5 days, 3:59:40, max mem: 15.1 GB 
[06/18 21:19:55][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0002,	1.1702 s / batch. (data: 3.27e-04). ETA=5 days, 3:52:57, max mem: 15.1 GB 
[06/18 21:21:52][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0005,	1.1700 s / batch. (data: 3.35e-04). ETA=5 days, 3:49:52, max mem: 15.1 GB 
[06/18 21:23:49][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0004,	1.1762 s / batch. (data: 3.53e-04). ETA=5 days, 4:27:18, max mem: 15.1 GB 
[06/18 21:25:46][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0003,	1.1782 s / batch. (data: 2.99e-04). ETA=5 days, 4:38:04, max mem: 15.1 GB 
[06/18 21:27:44][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0001,	1.1687 s / batch. (data: 3.61e-04). ETA=5 days, 3:35:24, max mem: 15.1 GB 
[06/18 21:29:41][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0002,	1.1713 s / batch. (data: 3.45e-04). ETA=5 days, 3:50:00, max mem: 15.1 GB 
[06/18 21:31:38][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0005,	1.1693 s / batch. (data: 3.57e-04). ETA=5 days, 3:35:36, max mem: 15.1 GB 
[06/18 21:33:35][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0003,	1.1691 s / batch. (data: 4.21e-04). ETA=5 days, 3:32:16, max mem: 15.1 GB 
[06/18 21:35:32][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0002,	1.1713 s / batch. (data: 1.47e-04). ETA=5 days, 3:44:25, max mem: 15.1 GB 
[06/18 21:35:38][INFO] visual_prompt:  319: Epoch 24 / 100: avg data time: 4.08e-03, avg batch time: 1.1797, average train loss: 0.0002
[06/18 21:44:32][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1549 s / batch. (data: 2.13e-04)max mem: 15.06516 GB 
[06/18 21:52:45][INFO] visual_prompt:  476: Inference (val):avg data time: 1.58e-04, avg batch time: 5.1423, average loss: 0.0001
[06/18 21:52:45][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/18 21:52:45][INFO] visual_prompt:  257: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[06/18 21:55:24][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0003,	1.1766 s / batch. (data: 3.10e-04). ETA=5 days, 4:15:57, max mem: 15.1 GB 
[06/18 21:57:21][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0004,	1.1719 s / batch. (data: 3.36e-04). ETA=5 days, 3:43:50, max mem: 15.1 GB 
[06/18 21:59:18][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0001,	1.1651 s / batch. (data: 3.23e-04). ETA=5 days, 2:58:48, max mem: 15.1 GB 
[06/18 22:01:15][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0002,	1.1694 s / batch. (data: 2.71e-04). ETA=5 days, 3:24:19, max mem: 15.1 GB 
[06/18 22:03:12][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0003,	1.1727 s / batch. (data: 3.34e-04). ETA=5 days, 3:43:33, max mem: 15.1 GB 
[06/18 22:05:09][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0002,	1.1683 s / batch. (data: 3.65e-04). ETA=5 days, 3:13:34, max mem: 15.1 GB 
[06/18 22:07:06][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0003,	1.1705 s / batch. (data: 3.16e-04). ETA=5 days, 3:25:32, max mem: 15.1 GB 
[06/18 22:09:04][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0002,	1.1745 s / batch. (data: 2.96e-04). ETA=5 days, 3:48:48, max mem: 15.1 GB 
[06/18 22:11:01][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0001,	1.1644 s / batch. (data: 2.73e-04). ETA=5 days, 2:43:11, max mem: 15.1 GB 
[06/18 22:12:58][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0002,	1.1648 s / batch. (data: 3.44e-04). ETA=5 days, 2:43:16, max mem: 15.1 GB 
[06/18 22:14:55][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0002,	1.1782 s / batch. (data: 2.92e-04). ETA=5 days, 4:06:29, max mem: 15.1 GB 
[06/18 22:16:52][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0002,	1.1752 s / batch. (data: 3.84e-04). ETA=5 days, 3:45:18, max mem: 15.1 GB 
[06/18 22:18:49][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0001,	1.1726 s / batch. (data: 3.04e-04). ETA=5 days, 3:26:53, max mem: 15.1 GB 
[06/18 22:20:46][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0002,	1.1713 s / batch. (data: 3.35e-04). ETA=5 days, 3:16:45, max mem: 15.1 GB 
[06/18 22:22:43][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0002,	1.1671 s / batch. (data: 2.77e-04). ETA=5 days, 2:48:25, max mem: 15.1 GB 
[06/18 22:24:40][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0003,	1.1689 s / batch. (data: 3.30e-04). ETA=5 days, 2:57:39, max mem: 15.1 GB 
[06/18 22:26:38][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0009,	1.1688 s / batch. (data: 3.63e-04). ETA=5 days, 2:55:29, max mem: 15.1 GB 
[06/18 22:28:35][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0001,	1.1657 s / batch. (data: 3.58e-04). ETA=5 days, 2:33:26, max mem: 15.1 GB 
[06/18 22:30:32][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0004,	1.1697 s / batch. (data: 4.34e-04). ETA=5 days, 2:57:03, max mem: 15.1 GB 
[06/18 22:32:29][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0002,	1.1685 s / batch. (data: 3.79e-04). ETA=5 days, 2:47:11, max mem: 15.1 GB 
[06/18 22:34:26][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0006,	1.1697 s / batch. (data: 4.07e-04). ETA=5 days, 2:52:59, max mem: 15.1 GB 
[06/18 22:36:23][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0001,	1.1696 s / batch. (data: 4.66e-04). ETA=5 days, 2:50:42, max mem: 15.1 GB 
[06/18 22:38:20][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0003,	1.1816 s / batch. (data: 3.66e-04). ETA=5 days, 4:04:26, max mem: 15.1 GB 
[06/18 22:40:17][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0003,	1.1712 s / batch. (data: 3.71e-04). ETA=5 days, 2:56:22, max mem: 15.1 GB 
[06/18 22:42:14][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0001,	1.1752 s / batch. (data: 3.55e-04). ETA=5 days, 3:19:54, max mem: 15.1 GB 
[06/18 22:44:11][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0003,	1.1651 s / batch. (data: 3.04e-04). ETA=5 days, 2:14:20, max mem: 15.1 GB 
[06/18 22:46:08][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0001,	1.1684 s / batch. (data: 3.84e-04). ETA=5 days, 2:33:10, max mem: 15.1 GB 
[06/18 22:48:05][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0001,	1.1694 s / batch. (data: 4.56e-04). ETA=5 days, 2:37:27, max mem: 15.1 GB 
[06/18 22:50:02][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0003,	1.1729 s / batch. (data: 4.10e-04). ETA=5 days, 2:57:53, max mem: 15.1 GB 
[06/18 22:51:59][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0002,	1.1688 s / batch. (data: 3.76e-04). ETA=5 days, 2:29:41, max mem: 15.1 GB 
[06/18 22:53:56][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0001,	1.1669 s / batch. (data: 4.52e-04). ETA=5 days, 2:16:08, max mem: 15.1 GB 
[06/18 22:55:53][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0002,	1.1696 s / batch. (data: 3.77e-04). ETA=5 days, 2:30:47, max mem: 15.1 GB 
[06/18 22:57:51][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0003,	1.1704 s / batch. (data: 3.02e-04). ETA=5 days, 2:34:15, max mem: 15.1 GB 
[06/18 22:59:48][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0002,	1.1667 s / batch. (data: 3.84e-04). ETA=5 days, 2:08:36, max mem: 15.1 GB 
[06/18 23:01:45][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0003,	1.1736 s / batch. (data: 3.59e-04). ETA=5 days, 2:49:58, max mem: 15.1 GB 
[06/18 23:03:42][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0004,	1.1711 s / batch. (data: 3.59e-04). ETA=5 days, 2:32:27, max mem: 15.1 GB 
[06/18 23:05:39][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0001,	1.1744 s / batch. (data: 4.54e-04). ETA=5 days, 2:51:30, max mem: 15.1 GB 
[06/18 23:07:36][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0003,	1.1753 s / batch. (data: 3.57e-04). ETA=5 days, 2:54:53, max mem: 15.1 GB 
[06/18 23:09:34][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0003,	1.1747 s / batch. (data: 3.66e-04). ETA=5 days, 2:49:11, max mem: 15.1 GB 
[06/18 23:11:31][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0001,	1.1657 s / batch. (data: 3.87e-04). ETA=5 days, 1:50:41, max mem: 15.1 GB 
[06/18 23:13:27][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0003,	1.1688 s / batch. (data: 3.87e-04). ETA=5 days, 2:08:34, max mem: 15.1 GB 
[06/18 23:15:24][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0002,	1.1703 s / batch. (data: 4.12e-04). ETA=5 days, 2:15:47, max mem: 15.1 GB 
[06/18 23:17:21][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0003,	1.1706 s / batch. (data: 3.57e-04). ETA=5 days, 2:16:05, max mem: 15.1 GB 
[06/18 23:19:18][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0003,	1.1667 s / batch. (data: 3.24e-04). ETA=5 days, 1:49:25, max mem: 15.1 GB 
[06/18 23:21:15][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0003,	1.1704 s / batch. (data: 3.86e-04). ETA=5 days, 2:10:51, max mem: 15.1 GB 
[06/18 23:23:12][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0003,	1.1677 s / batch. (data: 7.84e-04). ETA=5 days, 1:52:07, max mem: 15.1 GB 
[06/18 23:25:09][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0002,	1.1740 s / batch. (data: 3.52e-04). ETA=5 days, 2:29:14, max mem: 15.1 GB 
[06/18 23:27:06][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0003,	1.1654 s / batch. (data: 3.39e-04). ETA=5 days, 1:33:15, max mem: 15.1 GB 
[06/18 23:29:03][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0003,	1.1671 s / batch. (data: 3.45e-04). ETA=5 days, 1:42:31, max mem: 15.1 GB 
[06/18 23:31:01][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0002,	1.1715 s / batch. (data: 1.37e-04). ETA=5 days, 2:07:32, max mem: 15.1 GB 
[06/18 23:31:06][INFO] visual_prompt:  319: Epoch 25 / 100: avg data time: 4.23e-03, avg batch time: 1.1791, average train loss: 0.0003
[06/18 23:40:03][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1702 s / batch. (data: 2.58e-04)max mem: 15.06516 GB 
[06/18 23:48:21][INFO] visual_prompt:  476: Inference (val):avg data time: 2.12e-04, avg batch time: 5.1786, average loss: 0.0001
[06/18 23:48:21][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/18 23:48:21][INFO] visual_prompt:  257: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[06/18 23:50:53][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0001,	1.1734 s / batch. (data: 3.44e-04). ETA=5 days, 2:17:48, max mem: 15.1 GB 
[06/18 23:52:50][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0001,	1.1665 s / batch. (data: 3.72e-04). ETA=5 days, 1:32:40, max mem: 15.1 GB 
[06/18 23:54:47][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0002,	1.1671 s / batch. (data: 3.18e-04). ETA=5 days, 1:34:34, max mem: 15.1 GB 
[06/18 23:56:44][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0001,	1.1677 s / batch. (data: 3.68e-04). ETA=5 days, 1:36:03, max mem: 15.1 GB 
[06/18 23:58:40][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0002,	1.1657 s / batch. (data: 4.56e-04). ETA=5 days, 1:21:32, max mem: 15.1 GB 
[06/19 00:00:37][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0002,	1.1661 s / batch. (data: 3.43e-04). ETA=5 days, 1:22:20, max mem: 15.1 GB 
[06/19 00:02:34][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0002,	1.1657 s / batch. (data: 3.37e-04). ETA=5 days, 1:17:57, max mem: 15.1 GB 
[06/19 00:04:31][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0003,	1.1691 s / batch. (data: 3.24e-04). ETA=5 days, 1:36:58, max mem: 15.1 GB 
[06/19 00:06:28][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0002,	1.1660 s / batch. (data: 3.50e-04). ETA=5 days, 1:15:40, max mem: 15.1 GB 
[06/19 00:08:24][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0005,	1.1685 s / batch. (data: 3.15e-04). ETA=5 days, 1:29:44, max mem: 15.1 GB 
[06/19 00:10:21][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0001,	1.1726 s / batch. (data: 4.66e-04). ETA=5 days, 1:52:52, max mem: 15.1 GB 
[06/19 00:12:18][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0004,	1.1702 s / batch. (data: 3.21e-04). ETA=5 days, 1:36:22, max mem: 15.1 GB 
[06/19 00:14:15][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0001,	1.1712 s / batch. (data: 3.39e-04). ETA=5 days, 1:40:33, max mem: 15.1 GB 
[06/19 00:16:12][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0001,	1.1684 s / batch. (data: 4.03e-04). ETA=5 days, 1:20:59, max mem: 15.1 GB 
[06/19 00:18:09][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0010,	1.1693 s / batch. (data: 3.58e-04). ETA=5 days, 1:24:40, max mem: 15.1 GB 
[06/19 00:20:06][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0002,	1.1736 s / batch. (data: 3.29e-04). ETA=5 days, 1:49:17, max mem: 15.1 GB 
[06/19 00:22:03][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0001,	1.1705 s / batch. (data: 3.37e-04). ETA=5 days, 1:28:02, max mem: 15.1 GB 
[06/19 00:24:00][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0003,	1.1711 s / batch. (data: 4.35e-04). ETA=5 days, 1:29:55, max mem: 15.1 GB 
[06/19 00:25:57][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0002,	1.1734 s / batch. (data: 4.13e-04). ETA=5 days, 1:42:30, max mem: 15.1 GB 
[06/19 00:27:54][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0002,	1.1743 s / batch. (data: 3.31e-04). ETA=5 days, 1:46:13, max mem: 15.1 GB 
[06/19 00:29:51][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0003,	1.1725 s / batch. (data: 3.18e-04). ETA=5 days, 1:33:10, max mem: 15.1 GB 
[06/19 00:31:48][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0002,	1.1730 s / batch. (data: 3.58e-04). ETA=5 days, 1:34:08, max mem: 15.1 GB 
[06/19 00:33:45][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0029,	1.1677 s / batch. (data: 3.29e-04). ETA=5 days, 0:59:14, max mem: 15.1 GB 
[06/19 00:35:42][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0001,	1.1699 s / batch. (data: 3.49e-04). ETA=5 days, 1:10:46, max mem: 15.1 GB 
[06/19 00:37:39][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0001,	1.1697 s / batch. (data: 3.30e-04). ETA=5 days, 1:07:56, max mem: 15.1 GB 
[06/19 00:39:36][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0002,	1.1721 s / batch. (data: 3.06e-04). ETA=5 days, 1:20:56, max mem: 15.1 GB 
[06/19 00:41:33][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0022,	1.1650 s / batch. (data: 3.55e-04). ETA=5 days, 0:34:41, max mem: 15.1 GB 
[06/19 00:43:29][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0003,	1.1675 s / batch. (data: 3.24e-04). ETA=5 days, 0:48:01, max mem: 15.1 GB 
[06/19 00:45:26][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0002,	1.1671 s / batch. (data: 3.06e-04). ETA=5 days, 0:43:39, max mem: 15.1 GB 
[06/19 00:47:23][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0001,	1.1673 s / batch. (data: 3.38e-04). ETA=5 days, 0:42:47, max mem: 15.1 GB 
[06/19 00:49:20][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0003,	1.1657 s / batch. (data: 4.23e-04). ETA=5 days, 0:31:25, max mem: 15.1 GB 
[06/19 00:51:16][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0002,	1.1681 s / batch. (data: 3.19e-04). ETA=5 days, 0:44:01, max mem: 15.1 GB 
[06/19 00:53:13][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0003,	1.1733 s / batch. (data: 4.10e-04). ETA=5 days, 1:14:09, max mem: 15.1 GB 
[06/19 00:55:10][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0002,	1.1716 s / batch. (data: 4.89e-04). ETA=5 days, 1:02:04, max mem: 15.1 GB 
[06/19 00:57:07][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0002,	1.1699 s / batch. (data: 3.21e-04). ETA=5 days, 0:49:40, max mem: 15.1 GB 
[06/19 00:59:04][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0002,	1.1713 s / batch. (data: 3.25e-04). ETA=5 days, 0:56:15, max mem: 15.1 GB 
[06/19 01:01:01][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0035,	1.1750 s / batch. (data: 3.23e-04). ETA=5 days, 1:17:27, max mem: 15.1 GB 
[06/19 01:02:58][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0002,	1.1651 s / batch. (data: 3.32e-04). ETA=5 days, 0:13:49, max mem: 15.1 GB 
[06/19 01:04:55][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0001,	1.1752 s / batch. (data: 3.17e-04). ETA=5 days, 1:14:47, max mem: 15.1 GB 
[06/19 01:06:52][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0001,	1.1664 s / batch. (data: 3.56e-04). ETA=5 days, 0:18:05, max mem: 15.1 GB 
[06/19 01:08:49][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0001,	1.1700 s / batch. (data: 2.71e-04). ETA=5 days, 0:38:26, max mem: 15.1 GB 
[06/19 01:10:46][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0003,	1.1711 s / batch. (data: 3.15e-04). ETA=5 days, 0:43:10, max mem: 15.1 GB 
[06/19 01:12:43][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0002,	1.1731 s / batch. (data: 3.01e-04). ETA=5 days, 0:53:35, max mem: 15.1 GB 
[06/19 01:14:40][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0003,	1.1794 s / batch. (data: 3.09e-04). ETA=5 days, 1:30:54, max mem: 15.1 GB 
[06/19 01:16:38][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0003,	1.1799 s / batch. (data: 3.19e-04). ETA=5 days, 1:31:54, max mem: 15.1 GB 
[06/19 01:18:36][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0002,	1.1786 s / batch. (data: 3.29e-04). ETA=5 days, 1:22:05, max mem: 15.1 GB 
[06/19 01:20:34][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0004,	1.1744 s / batch. (data: 2.85e-04). ETA=5 days, 0:53:54, max mem: 15.1 GB 
[06/19 01:22:31][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0002,	1.1750 s / batch. (data: 3.05e-04). ETA=5 days, 0:55:22, max mem: 15.1 GB 
[06/19 01:24:29][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0003,	1.1734 s / batch. (data: 3.51e-04). ETA=5 days, 0:43:58, max mem: 15.1 GB 
[06/19 01:26:26][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0002,	1.1671 s / batch. (data: 1.21e-04). ETA=5 days, 0:02:46, max mem: 15.1 GB 
[06/19 01:26:32][INFO] visual_prompt:  319: Epoch 26 / 100: avg data time: 4.43e-03, avg batch time: 1.1770, average train loss: 0.0004
[06/19 01:35:24][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.2039 s / batch. (data: 3.06e-04)max mem: 15.06516 GB 
[06/19 01:43:40][INFO] visual_prompt:  476: Inference (val):avg data time: 1.93e-04, avg batch time: 5.1554, average loss: 0.0001
[06/19 01:43:40][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/19 01:43:40][INFO] visual_prompt:  257: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[06/19 01:46:20][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0002,	1.1730 s / batch. (data: 3.42e-04). ETA=5 days, 0:37:03, max mem: 15.1 GB 
[06/19 01:48:17][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0003,	1.1711 s / batch. (data: 4.04e-04). ETA=5 days, 0:23:55, max mem: 15.1 GB 
[06/19 01:50:14][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0028,	1.1687 s / batch. (data: 3.49e-04). ETA=5 days, 0:06:39, max mem: 15.1 GB 
[06/19 01:52:11][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0002,	1.1695 s / batch. (data: 3.47e-04). ETA=5 days, 0:10:00, max mem: 15.1 GB 
[06/19 01:54:08][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0002,	1.1767 s / batch. (data: 4.14e-04). ETA=5 days, 0:52:22, max mem: 15.1 GB 
[06/19 01:56:05][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0003,	1.1664 s / batch. (data: 3.19e-04). ETA=4 days, 23:46:47, max mem: 15.1 GB 
[06/19 01:58:02][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0002,	1.1717 s / batch. (data: 3.26e-04). ETA=5 days, 0:17:53, max mem: 15.1 GB 
[06/19 01:59:59][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0003,	1.1711 s / batch. (data: 5.33e-04). ETA=5 days, 0:11:52, max mem: 15.1 GB 
[06/19 02:01:56][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0001,	1.1719 s / batch. (data: 4.27e-04). ETA=5 days, 0:14:39, max mem: 15.1 GB 
[06/19 02:03:53][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0003,	1.1668 s / batch. (data: 3.30e-04). ETA=4 days, 23:41:26, max mem: 15.1 GB 
[06/19 02:05:50][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0002,	1.1707 s / batch. (data: 3.85e-04). ETA=5 days, 0:03:43, max mem: 15.1 GB 
[06/19 02:07:47][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0002,	1.1682 s / batch. (data: 3.30e-04). ETA=4 days, 23:46:26, max mem: 15.1 GB 
[06/19 02:09:44][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0002,	1.1681 s / batch. (data: 3.20e-04). ETA=4 days, 23:43:45, max mem: 15.1 GB 
[06/19 02:11:41][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0001,	1.1679 s / batch. (data: 3.24e-04). ETA=4 days, 23:40:49, max mem: 15.1 GB 
[06/19 02:13:38][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0005,	1.1665 s / batch. (data: 3.83e-04). ETA=4 days, 23:29:48, max mem: 15.1 GB 
[06/19 02:15:35][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0002,	1.1771 s / batch. (data: 2.85e-04). ETA=5 days, 0:32:56, max mem: 15.1 GB 
[06/19 02:17:32][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0003,	1.1753 s / batch. (data: 3.32e-04). ETA=5 days, 0:20:08, max mem: 15.1 GB 
[06/19 02:19:29][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0002,	1.1733 s / batch. (data: 3.44e-04). ETA=5 days, 0:05:58, max mem: 15.1 GB 
[06/19 02:21:26][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0002,	1.1652 s / batch. (data: 4.79e-04). ETA=4 days, 23:14:00, max mem: 15.1 GB 
[06/19 02:23:23][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0003,	1.1752 s / batch. (data: 3.49e-04). ETA=5 days, 0:13:52, max mem: 15.1 GB 
[06/19 02:25:21][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0001,	1.1693 s / batch. (data: 3.09e-04). ETA=4 days, 23:35:47, max mem: 15.1 GB 
[06/19 02:27:18][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0003,	1.1672 s / batch. (data: 3.65e-04). ETA=4 days, 23:20:30, max mem: 15.1 GB 
[06/19 02:29:15][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0001,	1.1699 s / batch. (data: 3.31e-04). ETA=4 days, 23:35:16, max mem: 15.1 GB 
[06/19 02:31:12][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0008,	1.1728 s / batch. (data: 2.71e-04). ETA=4 days, 23:51:12, max mem: 15.1 GB 
[06/19 02:33:10][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0003,	1.1766 s / batch. (data: 3.12e-04). ETA=5 days, 0:12:27, max mem: 15.1 GB 
[06/19 02:35:07][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0002,	1.1720 s / batch. (data: 3.52e-04). ETA=4 days, 23:42:07, max mem: 15.1 GB 
[06/19 02:37:04][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0001,	1.1701 s / batch. (data: 2.85e-04). ETA=4 days, 23:28:51, max mem: 15.1 GB 
[06/19 02:39:01][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0002,	1.1724 s / batch. (data: 2.88e-04). ETA=4 days, 23:41:06, max mem: 15.1 GB 
[06/19 02:40:58][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0003,	1.1669 s / batch. (data: 2.75e-04). ETA=4 days, 23:05:00, max mem: 15.1 GB 
[06/19 02:42:55][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0003,	1.1690 s / batch. (data: 3.38e-04). ETA=4 days, 23:16:05, max mem: 15.1 GB 
[06/19 02:44:52][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0003,	1.1686 s / batch. (data: 3.31e-04). ETA=4 days, 23:11:57, max mem: 15.1 GB 
[06/19 02:46:49][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0003,	1.1705 s / batch. (data: 3.16e-04). ETA=4 days, 23:21:34, max mem: 15.1 GB 
[06/19 02:48:46][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0006,	1.1665 s / batch. (data: 3.27e-04). ETA=4 days, 22:55:09, max mem: 15.1 GB 
[06/19 02:50:43][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0002,	1.1689 s / batch. (data: 3.43e-04). ETA=4 days, 23:07:31, max mem: 15.1 GB 
[06/19 02:52:40][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0001,	1.1742 s / batch. (data: 3.04e-04). ETA=4 days, 23:38:21, max mem: 15.1 GB 
[06/19 02:54:38][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0002,	1.1711 s / batch. (data: 3.23e-04). ETA=4 days, 23:17:22, max mem: 15.1 GB 
[06/19 02:56:34][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0003,	1.1706 s / batch. (data: 3.26e-04). ETA=4 days, 23:12:05, max mem: 15.1 GB 
[06/19 02:58:31][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0003,	1.1717 s / batch. (data: 3.51e-04). ETA=4 days, 23:17:00, max mem: 15.1 GB 
[06/19 03:00:29][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0001,	1.1686 s / batch. (data: 3.63e-04). ETA=4 days, 22:56:20, max mem: 15.1 GB 
[06/19 03:02:26][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0002,	1.1718 s / batch. (data: 3.33e-04). ETA=4 days, 23:13:47, max mem: 15.1 GB 
[06/19 03:04:23][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0002,	1.1722 s / batch. (data: 3.21e-04). ETA=4 days, 23:14:14, max mem: 15.1 GB 
[06/19 03:06:20][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0001,	1.1747 s / batch. (data: 3.10e-04). ETA=4 days, 23:27:33, max mem: 15.1 GB 
[06/19 03:08:17][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0002,	1.1720 s / batch. (data: 3.34e-04). ETA=4 days, 23:09:16, max mem: 15.1 GB 
[06/19 03:10:14][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0001,	1.1689 s / batch. (data: 3.69e-04). ETA=4 days, 22:48:09, max mem: 15.1 GB 
[06/19 03:12:11][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0002,	1.1711 s / batch. (data: 3.39e-04). ETA=4 days, 22:59:29, max mem: 15.1 GB 
[06/19 03:14:08][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0003,	1.1702 s / batch. (data: 3.82e-04). ETA=4 days, 22:52:14, max mem: 15.1 GB 
[06/19 03:16:05][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0004,	1.1674 s / batch. (data: 3.41e-04). ETA=4 days, 22:33:32, max mem: 15.1 GB 
[06/19 03:18:02][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0004,	1.1796 s / batch. (data: 3.36e-04). ETA=4 days, 23:45:40, max mem: 15.1 GB 
[06/19 03:19:59][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0003,	1.1687 s / batch. (data: 4.62e-04). ETA=4 days, 22:37:20, max mem: 15.1 GB 
[06/19 03:21:56][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0002,	1.1693 s / batch. (data: 1.18e-04). ETA=4 days, 22:38:49, max mem: 15.1 GB 
[06/19 03:22:02][INFO] visual_prompt:  319: Epoch 27 / 100: avg data time: 4.34e-03, avg batch time: 1.1792, average train loss: 0.0003
[06/19 03:30:54][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.2051 s / batch. (data: 2.96e-04)max mem: 15.06516 GB 
[06/19 03:39:11][INFO] visual_prompt:  476: Inference (val):avg data time: 1.92e-04, avg batch time: 5.1557, average loss: 0.0002
[06/19 03:39:11][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/19 03:39:11][INFO] visual_prompt:  257: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[06/19 03:41:51][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0002,	1.1726 s / batch. (data: 3.79e-04). ETA=4 days, 22:57:00, max mem: 15.1 GB 
[06/19 03:43:48][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0002,	1.1745 s / batch. (data: 3.15e-04). ETA=4 days, 23:06:36, max mem: 15.1 GB 
[06/19 03:45:45][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0001,	1.1706 s / batch. (data: 3.40e-04). ETA=4 days, 22:40:49, max mem: 15.1 GB 
[06/19 03:47:42][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0002,	1.1671 s / batch. (data: 3.14e-04). ETA=4 days, 22:17:30, max mem: 15.1 GB 
[06/19 03:49:39][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0001,	1.1674 s / batch. (data: 3.67e-04). ETA=4 days, 22:17:33, max mem: 15.1 GB 
[06/19 03:51:36][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0002,	1.1689 s / batch. (data: 3.40e-04). ETA=4 days, 22:24:52, max mem: 15.1 GB 
[06/19 03:53:33][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0001,	1.1708 s / batch. (data: 3.86e-04). ETA=4 days, 22:34:37, max mem: 15.1 GB 
[06/19 03:55:30][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0002,	1.1684 s / batch. (data: 3.45e-04). ETA=4 days, 22:17:41, max mem: 15.1 GB 
[06/19 03:57:27][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0003,	1.1699 s / batch. (data: 4.21e-04). ETA=4 days, 22:24:44, max mem: 15.1 GB 
[06/19 03:59:24][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0003,	1.1693 s / batch. (data: 4.74e-04). ETA=4 days, 22:19:40, max mem: 15.1 GB 
[06/19 04:01:21][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0002,	1.1727 s / batch. (data: 3.48e-04). ETA=4 days, 22:38:14, max mem: 15.1 GB 
[06/19 04:03:19][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0003,	1.1671 s / batch. (data: 2.82e-04). ETA=4 days, 22:02:28, max mem: 15.1 GB 
[06/19 04:05:15][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0003,	1.1693 s / batch. (data: 4.14e-04). ETA=4 days, 22:13:52, max mem: 15.1 GB 
[06/19 04:07:12][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0002,	1.1692 s / batch. (data: 3.29e-04). ETA=4 days, 22:11:03, max mem: 15.1 GB 
[06/19 04:09:09][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0002,	1.1691 s / batch. (data: 3.52e-04). ETA=4 days, 22:08:34, max mem: 15.1 GB 
[06/19 04:11:06][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0004,	1.1689 s / batch. (data: 3.35e-04). ETA=4 days, 22:05:35, max mem: 15.1 GB 
[06/19 04:13:03][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0012,	1.1721 s / batch. (data: 3.37e-04). ETA=4 days, 22:22:50, max mem: 15.1 GB 
[06/19 04:15:00][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0001,	1.1688 s / batch. (data: 3.51e-04). ETA=4 days, 22:00:34, max mem: 15.1 GB 
[06/19 04:16:57][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0001,	1.1666 s / batch. (data: 3.57e-04). ETA=4 days, 21:45:24, max mem: 15.1 GB 
[06/19 04:18:53][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0017,	1.1698 s / batch. (data: 3.05e-04). ETA=4 days, 22:02:51, max mem: 15.1 GB 
[06/19 04:20:50][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0003,	1.1715 s / batch. (data: 3.01e-04). ETA=4 days, 22:11:22, max mem: 15.1 GB 
[06/19 04:22:47][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0001,	1.1705 s / batch. (data: 4.45e-04). ETA=4 days, 22:03:29, max mem: 15.1 GB 
[06/19 04:24:44][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0002,	1.1679 s / batch. (data: 3.43e-04). ETA=4 days, 21:45:49, max mem: 15.1 GB 
[06/19 04:26:42][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0007,	1.1667 s / batch. (data: 3.76e-04). ETA=4 days, 21:36:27, max mem: 15.1 GB 
[06/19 04:28:39][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0012,	1.1738 s / batch. (data: 3.85e-04). ETA=4 days, 22:17:23, max mem: 15.1 GB 
[06/19 04:30:36][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0003,	1.1738 s / batch. (data: 3.93e-04). ETA=4 days, 22:15:36, max mem: 15.1 GB 
[06/19 04:32:33][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0013,	1.1744 s / batch. (data: 3.38e-04). ETA=4 days, 22:17:22, max mem: 15.1 GB 
[06/19 04:34:30][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0010,	1.1672 s / batch. (data: 3.36e-04). ETA=4 days, 21:31:44, max mem: 15.1 GB 
[06/19 04:36:27][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0001,	1.1748 s / batch. (data: 3.14e-04). ETA=4 days, 22:15:31, max mem: 15.1 GB 
[06/19 04:38:25][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0002,	1.1686 s / batch. (data: 3.31e-04). ETA=4 days, 21:36:12, max mem: 15.1 GB 
[06/19 04:40:22][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0006,	1.1708 s / batch. (data: 2.87e-04). ETA=4 days, 21:47:40, max mem: 15.1 GB 
[06/19 04:42:19][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0002,	1.1713 s / batch. (data: 3.21e-04). ETA=4 days, 21:48:53, max mem: 15.1 GB 
[06/19 04:44:16][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0001,	1.1713 s / batch. (data: 3.00e-04). ETA=4 days, 21:46:25, max mem: 15.1 GB 
[06/19 04:46:13][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0003,	1.1678 s / batch. (data: 3.15e-04). ETA=4 days, 21:23:38, max mem: 15.1 GB 
[06/19 04:48:10][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0003,	1.1759 s / batch. (data: 3.03e-04). ETA=4 days, 22:10:44, max mem: 15.1 GB 
[06/19 04:50:08][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0003,	1.1764 s / batch. (data: 3.28e-04). ETA=4 days, 22:11:24, max mem: 15.1 GB 
[06/19 04:52:05][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0002,	1.1669 s / batch. (data: 3.22e-04). ETA=4 days, 21:12:26, max mem: 15.1 GB 
[06/19 04:54:02][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0002,	1.1709 s / batch. (data: 3.30e-04). ETA=4 days, 21:34:19, max mem: 15.1 GB 
[06/19 04:55:59][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0003,	1.1718 s / batch. (data: 3.59e-04). ETA=4 days, 21:37:49, max mem: 15.1 GB 
[06/19 04:57:55][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0003,	1.1675 s / batch. (data: 3.01e-04). ETA=4 days, 21:10:14, max mem: 15.1 GB 
[06/19 04:59:52][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0002,	1.1709 s / batch. (data: 3.68e-04). ETA=4 days, 21:28:32, max mem: 15.1 GB 
[06/19 05:01:49][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0003,	1.1664 s / batch. (data: 4.14e-04). ETA=4 days, 20:59:54, max mem: 15.1 GB 
[06/19 05:03:46][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0002,	1.1695 s / batch. (data: 5.51e-04). ETA=4 days, 21:16:12, max mem: 15.1 GB 
[06/19 05:05:43][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0015,	1.1712 s / batch. (data: 3.57e-04). ETA=4 days, 21:24:42, max mem: 15.1 GB 
[06/19 05:07:41][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0005,	1.1746 s / batch. (data: 3.41e-04). ETA=4 days, 21:43:01, max mem: 15.1 GB 
[06/19 05:09:38][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0006,	1.1709 s / batch. (data: 3.08e-04). ETA=4 days, 21:18:41, max mem: 15.1 GB 
[06/19 05:11:35][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0001,	1.1745 s / batch. (data: 3.60e-04). ETA=4 days, 21:38:51, max mem: 15.1 GB 
[06/19 05:13:32][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0026,	1.1712 s / batch. (data: 4.10e-04). ETA=4 days, 21:16:47, max mem: 15.1 GB 
[06/19 05:15:29][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0002,	1.1661 s / batch. (data: 3.34e-04). ETA=4 days, 20:43:58, max mem: 15.1 GB 
[06/19 05:17:26][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0003,	1.1669 s / batch. (data: 1.20e-04). ETA=4 days, 20:47:15, max mem: 15.1 GB 
[06/19 05:17:32][INFO] visual_prompt:  319: Epoch 28 / 100: avg data time: 4.26e-03, avg batch time: 1.1790, average train loss: 0.0005
[06/19 05:26:26][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1676 s / batch. (data: 1.01e-04)max mem: 15.06516 GB 
[06/19 05:34:40][INFO] visual_prompt:  476: Inference (val):avg data time: 1.84e-04, avg batch time: 5.1515, average loss: 0.0001
[06/19 05:34:40][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/19 05:34:40][INFO] visual_prompt:  257: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[06/19 05:37:19][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0001,	1.1673 s / batch. (data: 5.35e-04). ETA=4 days, 20:47:26, max mem: 15.1 GB 
[06/19 05:39:16][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0003,	1.1643 s / batch. (data: 3.29e-04). ETA=4 days, 20:27:29, max mem: 15.1 GB 
[06/19 05:41:13][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0019,	1.1677 s / batch. (data: 4.47e-04). ETA=4 days, 20:45:47, max mem: 15.1 GB 
[06/19 05:43:10][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0002,	1.1676 s / batch. (data: 3.04e-04). ETA=4 days, 20:43:30, max mem: 15.1 GB 
[06/19 05:45:07][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0003,	1.1700 s / batch. (data: 3.19e-04). ETA=4 days, 20:55:59, max mem: 15.1 GB 
[06/19 05:47:04][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0003,	1.1723 s / batch. (data: 3.81e-04). ETA=4 days, 21:07:40, max mem: 15.1 GB 
[06/19 05:49:01][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0001,	1.1768 s / batch. (data: 3.23e-04). ETA=4 days, 21:32:32, max mem: 15.1 GB 
[06/19 05:50:58][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0002,	1.1762 s / batch. (data: 3.37e-04). ETA=4 days, 21:26:55, max mem: 15.1 GB 
[06/19 05:52:55][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0002,	1.1762 s / batch. (data: 3.17e-04). ETA=4 days, 21:25:02, max mem: 15.1 GB 
[06/19 05:54:52][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0002,	1.1681 s / batch. (data: 3.07e-04). ETA=4 days, 20:34:34, max mem: 15.1 GB 
[06/19 05:56:49][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0002,	1.1731 s / batch. (data: 5.25e-04). ETA=4 days, 21:02:39, max mem: 15.1 GB 
[06/19 05:58:47][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0001,	1.1770 s / batch. (data: 2.65e-04). ETA=4 days, 21:24:11, max mem: 15.1 GB 
[06/19 06:00:44][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0001,	1.1786 s / batch. (data: 3.14e-04). ETA=4 days, 21:31:46, max mem: 15.1 GB 
[06/19 06:02:41][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0002,	1.1807 s / batch. (data: 3.58e-04). ETA=4 days, 21:42:06, max mem: 15.1 GB 
[06/19 06:04:38][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0001,	1.1804 s / batch. (data: 3.28e-04). ETA=4 days, 21:38:31, max mem: 15.1 GB 
[06/19 06:06:35][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0003,	1.1736 s / batch. (data: 2.74e-04). ETA=4 days, 20:56:01, max mem: 15.1 GB 
[06/19 06:08:33][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0001,	1.1729 s / batch. (data: 3.21e-04). ETA=4 days, 20:49:40, max mem: 15.1 GB 
[06/19 06:10:30][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0004,	1.1800 s / batch. (data: 3.95e-04). ETA=4 days, 21:30:04, max mem: 15.1 GB 
[06/19 06:12:27][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0002,	1.1683 s / batch. (data: 3.40e-04). ETA=4 days, 20:18:09, max mem: 15.1 GB 
[06/19 06:14:24][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0002,	1.1689 s / batch. (data: 3.30e-04). ETA=4 days, 20:19:57, max mem: 15.1 GB 
[06/19 06:16:20][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0001,	1.1675 s / batch. (data: 3.52e-04). ETA=4 days, 20:09:32, max mem: 15.1 GB 
[06/19 06:18:17][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0003,	1.1664 s / batch. (data: 3.07e-04). ETA=4 days, 20:01:16, max mem: 15.1 GB 
[06/19 06:20:14][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0002,	1.1660 s / batch. (data: 3.61e-04). ETA=4 days, 19:56:45, max mem: 15.1 GB 
[06/19 06:22:11][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0003,	1.1666 s / batch. (data: 3.24e-04). ETA=4 days, 19:58:24, max mem: 15.1 GB 
[06/19 06:24:08][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0001,	1.1649 s / batch. (data: 3.36e-04). ETA=4 days, 19:46:23, max mem: 15.1 GB 
[06/19 06:26:05][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0010,	1.1668 s / batch. (data: 3.20e-04). ETA=4 days, 19:55:44, max mem: 15.1 GB 
[06/19 06:28:02][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0002,	1.1684 s / batch. (data: 3.73e-04). ETA=4 days, 20:03:17, max mem: 15.1 GB 
[06/19 06:29:59][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0001,	1.1760 s / batch. (data: 3.56e-04). ETA=4 days, 20:46:52, max mem: 15.1 GB 
[06/19 06:31:56][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0028,	1.1711 s / batch. (data: 3.87e-04). ETA=4 days, 20:15:54, max mem: 15.1 GB 
[06/19 06:33:53][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0002,	1.1680 s / batch. (data: 3.33e-04). ETA=4 days, 19:55:18, max mem: 15.1 GB 
[06/19 06:35:51][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0017,	1.1715 s / batch. (data: 3.52e-04). ETA=4 days, 20:14:05, max mem: 15.1 GB 
[06/19 06:37:48][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0002,	1.1749 s / batch. (data: 2.82e-04). ETA=4 days, 20:32:16, max mem: 15.1 GB 
[06/19 06:39:45][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0002,	1.1711 s / batch. (data: 3.16e-04). ETA=4 days, 20:07:30, max mem: 15.1 GB 
[06/19 06:41:42][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0002,	1.1718 s / batch. (data: 3.75e-04). ETA=4 days, 20:10:15, max mem: 15.1 GB 
[06/19 06:43:40][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0003,	1.1724 s / batch. (data: 3.25e-04). ETA=4 days, 20:11:53, max mem: 15.1 GB 
[06/19 06:45:37][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0002,	1.1716 s / batch. (data: 2.89e-04). ETA=4 days, 20:04:43, max mem: 15.1 GB 
[06/19 06:47:34][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0001,	1.1767 s / batch. (data: 3.23e-04). ETA=4 days, 20:33:07, max mem: 15.1 GB 
[06/19 06:49:31][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0002,	1.1700 s / batch. (data: 3.34e-04). ETA=4 days, 19:51:14, max mem: 15.1 GB 
[06/19 06:51:29][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0037,	1.1672 s / batch. (data: 3.09e-04). ETA=4 days, 19:32:44, max mem: 15.1 GB 
[06/19 06:53:25][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0026,	1.1682 s / batch. (data: 3.27e-04). ETA=4 days, 19:36:50, max mem: 15.1 GB 
[06/19 06:55:22][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0003,	1.1701 s / batch. (data: 3.49e-04). ETA=4 days, 19:45:59, max mem: 15.1 GB 
[06/19 06:57:19][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0003,	1.1667 s / batch. (data: 2.84e-04). ETA=4 days, 19:24:20, max mem: 15.1 GB 
[06/19 06:59:16][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0002,	1.1659 s / batch. (data: 3.22e-04). ETA=4 days, 19:17:16, max mem: 15.1 GB 
[06/19 07:01:13][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0003,	1.1646 s / batch. (data: 3.19e-04). ETA=4 days, 19:07:57, max mem: 15.1 GB 
[06/19 07:03:09][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0001,	1.1641 s / batch. (data: 3.21e-04). ETA=4 days, 19:03:01, max mem: 15.1 GB 
[06/19 07:05:06][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0022,	1.1688 s / batch. (data: 3.87e-04). ETA=4 days, 19:28:53, max mem: 15.1 GB 
[06/19 07:07:03][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0003,	1.1676 s / batch. (data: 3.07e-04). ETA=4 days, 19:19:45, max mem: 15.1 GB 
[06/19 07:09:00][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0003,	1.1692 s / batch. (data: 3.25e-04). ETA=4 days, 19:27:34, max mem: 15.1 GB 
[06/19 07:10:57][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0003,	1.1722 s / batch. (data: 3.37e-04). ETA=4 days, 19:42:56, max mem: 15.1 GB 
[06/19 07:12:54][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0015,	1.1663 s / batch. (data: 1.71e-04). ETA=4 days, 19:06:19, max mem: 15.1 GB 
[06/19 07:13:00][INFO] visual_prompt:  319: Epoch 29 / 100: avg data time: 4.23e-03, avg batch time: 1.1787, average train loss: 0.0005
[06/19 07:21:55][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1701 s / batch. (data: 1.55e-04)max mem: 15.06516 GB 
[06/19 07:30:09][INFO] visual_prompt:  476: Inference (val):avg data time: 1.59e-04, avg batch time: 5.1529, average loss: 0.0002
[06/19 07:30:09][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/19 07:30:10][INFO] visual_prompt:  257: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[06/19 07:32:51][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0003,	1.1922 s / batch. (data: 5.85e-04). ETA=4 days, 21:37:42, max mem: 15.1 GB 
[06/19 07:34:48][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0003,	1.1694 s / batch. (data: 4.16e-04). ETA=4 days, 19:20:25, max mem: 15.1 GB 
[06/19 07:36:45][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0009,	1.1704 s / batch. (data: 3.58e-04). ETA=4 days, 19:24:26, max mem: 15.1 GB 
[06/19 07:38:42][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0003,	1.1679 s / batch. (data: 4.82e-04). ETA=4 days, 19:07:39, max mem: 15.1 GB 
[06/19 07:40:39][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0001,	1.1671 s / batch. (data: 3.35e-04). ETA=4 days, 19:00:51, max mem: 15.1 GB 
[06/19 07:42:36][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0001,	1.1670 s / batch. (data: 4.37e-04). ETA=4 days, 18:58:47, max mem: 15.1 GB 
[06/19 07:44:33][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0029,	1.1680 s / batch. (data: 3.20e-04). ETA=4 days, 19:02:32, max mem: 15.1 GB 
[06/19 07:46:30][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0020,	1.1635 s / batch. (data: 3.26e-04). ETA=4 days, 18:34:18, max mem: 15.1 GB 
[06/19 07:48:27][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0002,	1.1676 s / batch. (data: 3.58e-04). ETA=4 days, 18:56:05, max mem: 15.1 GB 
[06/19 07:50:24][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0002,	1.1667 s / batch. (data: 3.49e-04). ETA=4 days, 18:49:15, max mem: 15.1 GB 
[06/19 07:52:21][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0001,	1.1688 s / batch. (data: 3.77e-04). ETA=4 days, 18:59:45, max mem: 15.1 GB 
[06/19 07:54:18][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0003,	1.1711 s / batch. (data: 3.23e-04). ETA=4 days, 19:11:05, max mem: 15.1 GB 
[06/19 07:56:15][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0002,	1.1702 s / batch. (data: 3.50e-04). ETA=4 days, 19:03:35, max mem: 15.1 GB 
[06/19 07:58:12][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0002,	1.1680 s / batch. (data: 3.80e-04). ETA=4 days, 18:48:59, max mem: 15.1 GB 
[06/19 08:00:09][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0001,	1.1756 s / batch. (data: 3.29e-04). ETA=4 days, 19:31:38, max mem: 15.1 GB 
[06/19 08:02:07][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0003,	1.1703 s / batch. (data: 3.17e-04). ETA=4 days, 18:58:53, max mem: 15.1 GB 
[06/19 08:04:04][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0002,	1.1799 s / batch. (data: 3.27e-04). ETA=4 days, 19:52:58, max mem: 15.1 GB 
[06/19 08:06:02][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0014,	1.1777 s / batch. (data: 3.26e-04). ETA=4 days, 19:38:32, max mem: 15.1 GB 
[06/19 08:07:59][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0001,	1.1683 s / batch. (data: 2.97e-04). ETA=4 days, 18:40:59, max mem: 15.1 GB 
[06/19 08:09:56][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0014,	1.1694 s / batch. (data: 4.13e-04). ETA=4 days, 18:45:24, max mem: 15.1 GB 
[06/19 08:11:53][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0002,	1.1747 s / batch. (data: 3.79e-04). ETA=4 days, 19:14:53, max mem: 15.1 GB 
[06/19 08:13:51][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0002,	1.1779 s / batch. (data: 3.08e-04). ETA=4 days, 19:31:52, max mem: 15.1 GB 
[06/19 08:15:48][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0003,	1.1690 s / batch. (data: 2.62e-04). ETA=4 days, 18:37:34, max mem: 15.1 GB 
[06/19 08:17:45][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0018,	1.1695 s / batch. (data: 3.47e-04). ETA=4 days, 18:38:28, max mem: 15.1 GB 
[06/19 08:19:42][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0003,	1.1689 s / batch. (data: 3.48e-04). ETA=4 days, 18:32:58, max mem: 15.1 GB 
[06/19 08:21:39][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0002,	1.1654 s / batch. (data: 3.21e-04). ETA=4 days, 18:10:32, max mem: 15.1 GB 
[06/19 08:23:36][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0003,	1.1688 s / batch. (data: 3.24e-04). ETA=4 days, 18:28:16, max mem: 15.1 GB 
[06/19 08:25:33][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0011,	1.1666 s / batch. (data: 2.85e-04). ETA=4 days, 18:13:40, max mem: 15.1 GB 
[06/19 08:27:30][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0002,	1.1647 s / batch. (data: 5.12e-04). ETA=4 days, 18:00:25, max mem: 15.1 GB 
[06/19 08:29:27][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0002,	1.1671 s / batch. (data: 3.26e-04). ETA=4 days, 18:12:39, max mem: 15.1 GB 
[06/19 08:31:24][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0003,	1.1654 s / batch. (data: 3.13e-04). ETA=4 days, 18:00:24, max mem: 15.1 GB 
[06/19 08:33:21][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0001,	1.1772 s / batch. (data: 3.39e-04). ETA=4 days, 19:07:39, max mem: 15.1 GB 
[06/19 08:35:18][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0002,	1.1711 s / batch. (data: 3.61e-04). ETA=4 days, 18:30:18, max mem: 15.1 GB 
[06/19 08:37:15][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0002,	1.1666 s / batch. (data: 3.34e-04). ETA=4 days, 18:02:01, max mem: 15.1 GB 
[06/19 08:39:12][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0013,	1.1667 s / batch. (data: 2.93e-04). ETA=4 days, 18:00:18, max mem: 15.1 GB 
[06/19 08:41:09][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0002,	1.1751 s / batch. (data: 2.95e-04). ETA=4 days, 18:47:27, max mem: 15.1 GB 
[06/19 08:43:06][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0033,	1.1730 s / batch. (data: 3.12e-04). ETA=4 days, 18:33:38, max mem: 15.1 GB 
[06/19 08:45:04][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0002,	1.1769 s / batch. (data: 3.28e-04). ETA=4 days, 18:54:25, max mem: 15.1 GB 
[06/19 08:47:01][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0001,	1.1716 s / batch. (data: 2.99e-04). ETA=4 days, 18:21:31, max mem: 15.1 GB 
[06/19 08:48:58][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0002,	1.1669 s / batch. (data: 3.19e-04). ETA=4 days, 17:51:40, max mem: 15.1 GB 
[06/19 08:50:55][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0001,	1.1741 s / batch. (data: 4.05e-04). ETA=4 days, 18:32:05, max mem: 15.1 GB 
[06/19 08:52:52][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0002,	1.1720 s / batch. (data: 3.57e-04). ETA=4 days, 18:17:53, max mem: 15.1 GB 
[06/19 08:54:49][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0003,	1.1710 s / batch. (data: 3.60e-04). ETA=4 days, 18:10:06, max mem: 15.1 GB 
[06/19 08:56:46][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0002,	1.1673 s / batch. (data: 3.40e-04). ETA=4 days, 17:46:23, max mem: 15.1 GB 
[06/19 08:58:43][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0003,	1.1664 s / batch. (data: 3.14e-04). ETA=4 days, 17:39:27, max mem: 15.1 GB 
[06/19 09:00:40][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0002,	1.1697 s / batch. (data: 3.15e-04). ETA=4 days, 17:56:21, max mem: 15.1 GB 
[06/19 09:02:37][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0003,	1.1686 s / batch. (data: 3.21e-04). ETA=4 days, 17:48:02, max mem: 15.1 GB 
[06/19 09:04:34][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0042,	1.1675 s / batch. (data: 2.28e-04). ETA=4 days, 17:39:52, max mem: 15.1 GB 
[06/19 09:06:31][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0003,	1.1677 s / batch. (data: 3.36e-04). ETA=4 days, 17:38:54, max mem: 15.1 GB 
[06/19 09:08:28][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0001,	1.1672 s / batch. (data: 1.25e-04). ETA=4 days, 17:34:12, max mem: 15.1 GB 
[06/19 09:08:34][INFO] visual_prompt:  319: Epoch 30 / 100: avg data time: 4.22e-03, avg batch time: 1.1797, average train loss: 0.0006
[06/19 09:17:26][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1820 s / batch. (data: 1.50e-04)max mem: 15.06516 GB 
[06/19 09:25:42][INFO] visual_prompt:  476: Inference (val):avg data time: 1.73e-04, avg batch time: 5.1550, average loss: 0.0002
[06/19 09:25:42][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/19 09:25:42][INFO] visual_prompt:  257: Training 31 / 100 epoch, with learning rate 0.883022221559489
[06/19 09:28:22][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0002,	1.1692 s / batch. (data: 2.99e-04). ETA=4 days, 17:43:49, max mem: 15.1 GB 
[06/19 09:30:20][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0001,	1.1675 s / batch. (data: 3.26e-04). ETA=4 days, 17:31:58, max mem: 15.1 GB 
[06/19 09:32:17][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0011,	1.1681 s / batch. (data: 3.31e-04). ETA=4 days, 17:33:41, max mem: 15.1 GB 
[06/19 09:34:14][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0002,	1.1713 s / batch. (data: 3.62e-04). ETA=4 days, 17:50:15, max mem: 15.1 GB 
[06/19 09:36:11][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0002,	1.1674 s / batch. (data: 3.44e-04). ETA=4 days, 17:25:42, max mem: 15.1 GB 
[06/19 09:38:08][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0002,	1.1708 s / batch. (data: 3.07e-04). ETA=4 days, 17:43:29, max mem: 15.1 GB 
[06/19 09:40:05][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0001,	1.1714 s / batch. (data: 3.37e-04). ETA=4 days, 17:44:44, max mem: 15.1 GB 
[06/19 09:42:02][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0001,	1.1754 s / batch. (data: 3.37e-04). ETA=4 days, 18:06:35, max mem: 15.1 GB 
[06/19 09:44:00][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0035,	1.1752 s / batch. (data: 3.05e-04). ETA=4 days, 18:03:04, max mem: 15.1 GB 
[06/19 09:45:58][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0001,	1.1756 s / batch. (data: 2.93e-04). ETA=4 days, 18:03:43, max mem: 15.1 GB 
[06/19 09:47:55][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0003,	1.1794 s / batch. (data: 3.11e-04). ETA=4 days, 18:23:42, max mem: 15.1 GB 
[06/19 09:49:52][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0007,	1.1718 s / batch. (data: 5.66e-04). ETA=4 days, 17:37:42, max mem: 15.1 GB 
[06/19 09:51:49][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0003,	1.1724 s / batch. (data: 3.45e-04). ETA=4 days, 17:39:08, max mem: 15.1 GB 
[06/19 09:53:46][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0002,	1.1700 s / batch. (data: 3.45e-04). ETA=4 days, 17:23:03, max mem: 15.1 GB 
[06/19 09:55:43][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0001,	1.1681 s / batch. (data: 3.09e-04). ETA=4 days, 17:10:08, max mem: 15.1 GB 
[06/19 09:57:40][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0003,	1.1697 s / batch. (data: 2.91e-04). ETA=4 days, 17:17:31, max mem: 15.1 GB 
[06/19 09:59:37][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.0001,	1.1788 s / batch. (data: 3.42e-04). ETA=4 days, 18:08:43, max mem: 15.1 GB 
[06/19 10:01:34][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.0004,	1.1724 s / batch. (data: 3.72e-04). ETA=4 days, 17:29:28, max mem: 15.1 GB 
[06/19 10:03:32][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0001,	1.1743 s / batch. (data: 3.42e-04). ETA=4 days, 17:38:33, max mem: 15.1 GB 
[06/19 10:05:29][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0001,	1.1747 s / batch. (data: 3.49e-04). ETA=4 days, 17:38:48, max mem: 15.1 GB 
[06/19 10:07:26][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.0002,	1.1823 s / batch. (data: 3.13e-04). ETA=4 days, 18:21:01, max mem: 15.1 GB 
[06/19 10:09:23][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0003,	1.1695 s / batch. (data: 3.29e-04). ETA=4 days, 17:04:55, max mem: 15.1 GB 
[06/19 10:11:21][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.0020,	1.1755 s / batch. (data: 3.26e-04). ETA=4 days, 17:37:44, max mem: 15.1 GB 
[06/19 10:13:18][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.0002,	1.1733 s / batch. (data: 3.27e-04). ETA=4 days, 17:22:33, max mem: 15.1 GB 
[06/19 10:15:16][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.0017,	1.1718 s / batch. (data: 3.26e-04). ETA=4 days, 17:12:11, max mem: 15.1 GB 
[06/19 10:17:13][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0002,	1.1768 s / batch. (data: 2.91e-04). ETA=4 days, 17:38:55, max mem: 15.1 GB 
[06/19 10:19:10][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.0003,	1.1820 s / batch. (data: 3.24e-04). ETA=4 days, 18:07:02, max mem: 15.1 GB 
[06/19 10:21:08][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.0002,	1.1656 s / batch. (data: 2.82e-04). ETA=4 days, 16:30:16, max mem: 15.1 GB 
[06/19 10:23:04][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.0002,	1.1693 s / batch. (data: 2.84e-04). ETA=4 days, 16:49:51, max mem: 15.1 GB 
[06/19 10:25:01][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.0002,	1.1708 s / batch. (data: 4.05e-04). ETA=4 days, 16:56:50, max mem: 15.1 GB 
[06/19 10:26:58][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0025,	1.1664 s / batch. (data: 2.60e-04). ETA=4 days, 16:29:05, max mem: 15.1 GB 
[06/19 10:28:55][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0003,	1.1662 s / batch. (data: 2.93e-04). ETA=4 days, 16:26:17, max mem: 15.1 GB 
[06/19 10:30:51][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0002,	1.1722 s / batch. (data: 2.56e-04). ETA=4 days, 16:59:05, max mem: 15.1 GB 
[06/19 10:32:48][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.0002,	1.1638 s / batch. (data: 2.73e-04). ETA=4 days, 16:08:16, max mem: 15.1 GB 
[06/19 10:34:45][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0002,	1.1692 s / batch. (data: 3.95e-04). ETA=4 days, 16:37:47, max mem: 15.1 GB 
[06/19 10:36:42][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.0044,	1.1680 s / batch. (data: 2.52e-04). ETA=4 days, 16:28:29, max mem: 15.1 GB 
[06/19 10:38:39][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.0001,	1.1673 s / batch. (data: 2.48e-04). ETA=4 days, 16:22:56, max mem: 15.1 GB 
[06/19 10:40:36][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0001,	1.1650 s / batch. (data: 2.38e-04). ETA=4 days, 16:07:34, max mem: 15.1 GB 
[06/19 10:42:33][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0002,	1.1685 s / batch. (data: 3.19e-04). ETA=4 days, 16:25:42, max mem: 15.1 GB 
[06/19 10:44:29][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0001,	1.1693 s / batch. (data: 3.55e-04). ETA=4 days, 16:28:36, max mem: 15.1 GB 
[06/19 10:46:26][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0001,	1.1719 s / batch. (data: 2.98e-04). ETA=4 days, 16:41:37, max mem: 15.1 GB 
[06/19 10:48:23][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0003,	1.1703 s / batch. (data: 3.02e-04). ETA=4 days, 16:30:19, max mem: 15.1 GB 
[06/19 10:50:20][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0021,	1.1698 s / batch. (data: 3.20e-04). ETA=4 days, 16:25:37, max mem: 15.1 GB 
[06/19 10:52:17][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0003,	1.1738 s / batch. (data: 3.01e-04). ETA=4 days, 16:46:19, max mem: 15.1 GB 
[06/19 10:54:15][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0001,	1.1693 s / batch. (data: 2.93e-04). ETA=4 days, 16:18:31, max mem: 15.1 GB 
[06/19 10:56:12][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0036,	1.1711 s / batch. (data: 3.10e-04). ETA=4 days, 16:26:51, max mem: 15.1 GB 
[06/19 10:58:08][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0002,	1.1681 s / batch. (data: 3.22e-04). ETA=4 days, 16:07:39, max mem: 15.1 GB 
[06/19 11:00:05][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.0002,	1.1708 s / batch. (data: 3.38e-04). ETA=4 days, 16:21:15, max mem: 15.1 GB 
[06/19 11:02:02][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.0003,	1.1660 s / batch. (data: 2.74e-04). ETA=4 days, 15:51:49, max mem: 15.1 GB 
[06/19 11:03:59][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0001,	1.1694 s / batch. (data: 1.55e-04). ETA=4 days, 16:09:44, max mem: 15.1 GB 
[06/19 11:04:05][INFO] visual_prompt:  319: Epoch 31 / 100: avg data time: 4.32e-03, avg batch time: 1.1794, average train loss: 0.0006
[06/19 11:12:59][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.000, 5.1565 s / batch. (data: 2.26e-04)max mem: 15.06516 GB 
[06/19 11:21:14][INFO] visual_prompt:  476: Inference (val):avg data time: 1.67e-04, avg batch time: 5.1596, average loss: 0.0002
[06/19 11:21:14][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_deep_cls_reinit_10/imagenet/sup_vitb16_imagenet21k/lr1.0_wd0.001/run1/val_imagenet_invariances.json
[06/19 11:21:15][INFO] visual_prompt:  257: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[06/19 11:23:53][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0001,	1.1741 s / batch. (data: 3.13e-04). ETA=4 days, 16:34:30, max mem: 15.1 GB 
[06/19 11:25:51][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0001,	1.1659 s / batch. (data: 4.61e-04). ETA=4 days, 15:45:17, max mem: 15.1 GB 
[06/19 11:27:48][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0002,	1.1724 s / batch. (data: 3.43e-04). ETA=4 days, 16:21:03, max mem: 15.1 GB 
[06/19 11:29:45][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0001,	1.1694 s / batch. (data: 3.65e-04). ETA=4 days, 16:01:27, max mem: 15.1 GB 
[06/19 11:31:42][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0002,	1.1702 s / batch. (data: 3.16e-04). ETA=4 days, 16:04:30, max mem: 15.1 GB 
[06/19 11:33:39][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0002,	1.1690 s / batch. (data: 3.33e-04). ETA=4 days, 15:55:22, max mem: 15.1 GB 
[06/19 11:35:36][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0005,	1.1681 s / batch. (data: 3.46e-04). ETA=4 days, 15:48:02, max mem: 15.1 GB 
[06/19 11:37:33][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0001,	1.1699 s / batch. (data: 3.08e-04). ETA=4 days, 15:56:58, max mem: 15.1 GB 
