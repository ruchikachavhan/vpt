[06/14 11:19:00][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/14 11:19:00][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,4,5,6
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/14 11:19:00][INFO] visual_prompt:   99: Command line arguments: None
[06/14 11:19:00][INFO] visual_prompt:  108: Training with config:
[06/14 11:19:00][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': False,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_shallow_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.1,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'mse',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/14 11:19:04][INFO] visual_prompt:   56: Total Parameters: 93879568	 Gradient Parameters: 8081680
[06/14 11:19:04][INFO] visual_prompt:   58: tuned percent:8.609
[06/14 11:19:04][INFO] visual_prompt:   44: Device used for model: 0
[06/14 11:19:04][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/14 11:19:04][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/14 11:19:07][INFO] visual_prompt:  158: Number of images: 1281167
[06/14 11:19:07][INFO] visual_prompt:  159: Number of classes: 1000
[06/14 11:19:07][INFO] visual_prompt:   78: Loading validation data...
[06/14 11:19:07][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/14 11:19:07][INFO] visual_prompt:  158: Number of images: 50000
[06/14 11:19:07][INFO] visual_prompt:  159: Number of classes: 1000
[06/14 11:19:07][INFO] visual_prompt:   81: Loading test data...
[06/14 11:19:07][INFO] visual_prompt:   83: ...no test data is constructed
[06/14 11:19:07][INFO] visual_prompt:  111: Constructing models...
[06/14 11:19:07][INFO] visual_prompt:  114: Setting up Evalutator...
[06/14 11:19:07][INFO] visual_prompt:  116: Setting up Trainer...
[06/14 11:19:07][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/14 11:19:07][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/14 11:19:07][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/14 11:19:07][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/14 11:19:07][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/14 11:19:07][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/14 11:19:07][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/14 11:19:07][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/14 11:19:07][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/14 11:19:07][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/14 11:19:07][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/14 11:19:07][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/14 11:19:07][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/14 11:19:07][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/14 11:19:07][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/14 11:19:07][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/14 11:19:07][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/14 11:19:07][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/14 11:19:07][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/14 11:19:07][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/14 11:19:07][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/14 11:19:07][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/14 11:19:07][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/14 11:21:28][INFO] visual_prompt:  301: 	Training 100/5004. train loss: 0.0436,	1.2094 s / batch. (data: 2.80e-04). ETA=7 days, 0:04:07, max mem: 15.0 GB 
[06/14 11:23:29][INFO] visual_prompt:  301: 	Training 200/5004. train loss: 0.1045,	1.2069 s / batch. (data: 3.19e-04). ETA=6 days, 23:41:29, max mem: 15.0 GB 
[06/14 11:25:29][INFO] visual_prompt:  301: 	Training 300/5004. train loss: 0.0249,	1.2064 s / batch. (data: 2.73e-04). ETA=6 days, 23:35:20, max mem: 15.0 GB 
[06/14 11:27:30][INFO] visual_prompt:  301: 	Training 400/5004. train loss: 0.0745,	1.2031 s / batch. (data: 3.16e-04). ETA=6 days, 23:05:32, max mem: 15.0 GB 
[06/14 11:29:31][INFO] visual_prompt:  301: 	Training 500/5004. train loss: 0.1074,	1.2096 s / batch. (data: 3.12e-04). ETA=6 days, 23:57:50, max mem: 15.0 GB 
[06/14 11:31:31][INFO] visual_prompt:  301: 	Training 600/5004. train loss: 0.0498,	1.2115 s / batch. (data: 3.33e-04). ETA=7 days, 0:11:41, max mem: 15.0 GB 
[06/14 11:33:32][INFO] visual_prompt:  301: 	Training 700/5004. train loss: 0.0665,	1.2055 s / batch. (data: 3.07e-04). ETA=6 days, 23:20:11, max mem: 15.0 GB 
[06/14 11:37:40][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/14 11:37:40][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,4,5,6
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/14 11:37:40][INFO] visual_prompt:   99: Command line arguments: None
[06/14 11:37:40][INFO] visual_prompt:  108: Training with config:
[06/14 11:37:40][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': False,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_shallow_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.1,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'mse',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/14 11:37:44][INFO] visual_prompt:   56: Total Parameters: 93879568	 Gradient Parameters: 8081680
[06/14 11:37:44][INFO] visual_prompt:   58: tuned percent:8.609
[06/14 11:37:44][INFO] visual_prompt:   44: Device used for model: 0
[06/14 11:37:44][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/14 11:37:44][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/14 11:37:47][INFO] visual_prompt:  158: Number of images: 1281167
[06/14 11:37:47][INFO] visual_prompt:  159: Number of classes: 1000
[06/14 11:37:47][INFO] visual_prompt:   78: Loading validation data...
[06/14 11:37:47][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/14 11:37:47][INFO] visual_prompt:  158: Number of images: 50000
[06/14 11:37:47][INFO] visual_prompt:  159: Number of classes: 1000
[06/14 11:37:47][INFO] visual_prompt:   81: Loading test data...
[06/14 11:37:47][INFO] visual_prompt:   83: ...no test data is constructed
[06/14 11:37:47][INFO] visual_prompt:  111: Constructing models...
[06/14 11:37:47][INFO] visual_prompt:  114: Setting up Evalutator...
[06/14 11:37:47][INFO] visual_prompt:  116: Setting up Trainer...
[06/14 11:37:47][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/14 11:37:47][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/14 11:37:47][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/14 11:37:47][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/14 11:37:47][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/14 11:37:47][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/14 11:37:47][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/14 11:37:47][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/14 11:37:47][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/14 11:37:47][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/14 11:37:47][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/14 11:37:47][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/14 11:37:47][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/14 11:37:47][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/14 11:37:47][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/14 11:37:47][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/14 11:37:47][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/14 11:37:47][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/14 11:37:47][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/14 11:37:47][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/14 11:37:47][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/14 11:37:47][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/14 11:37:47][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/14 11:40:11][INFO] visual_prompt:  301: 	Training 100/5004. train loss: 0.0819,	1.2058 s / batch. (data: 3.63e-04). ETA=6 days, 23:34:46, max mem: 15.0 GB 
[06/14 11:42:12][INFO] visual_prompt:  301: 	Training 200/5004. train loss: 0.0632,	1.2084 s / batch. (data: 4.30e-04). ETA=6 days, 23:53:41, max mem: 15.0 GB 
[06/14 11:44:13][INFO] visual_prompt:  301: 	Training 300/5004. train loss: 0.0589,	1.2137 s / batch. (data: 3.51e-04). ETA=7 days, 0:36:32, max mem: 15.0 GB 
[06/14 11:46:13][INFO] visual_prompt:  301: 	Training 400/5004. train loss: 0.0246,	1.2051 s / batch. (data: 2.62e-04). ETA=6 days, 23:22:30, max mem: 15.0 GB 
[06/14 11:48:13][INFO] visual_prompt:  301: 	Training 500/5004. train loss: 0.0338,	1.2054 s / batch. (data: 3.48e-04). ETA=6 days, 23:23:06, max mem: 15.0 GB 
[06/14 11:50:14][INFO] visual_prompt:  301: 	Training 600/5004. train loss: 0.0578,	1.2050 s / batch. (data: 3.18e-04). ETA=6 days, 23:17:55, max mem: 15.0 GB 
[06/14 11:52:15][INFO] visual_prompt:  301: 	Training 700/5004. train loss: 0.0397,	1.2064 s / batch. (data: 3.16e-04). ETA=6 days, 23:27:36, max mem: 15.0 GB 
[06/14 11:54:15][INFO] visual_prompt:  301: 	Training 800/5004. train loss: 0.0598,	1.2053 s / batch. (data: 2.68e-04). ETA=6 days, 23:16:02, max mem: 15.0 GB 
[06/14 11:56:16][INFO] visual_prompt:  301: 	Training 900/5004. train loss: 0.0327,	1.2064 s / batch. (data: 2.92e-04). ETA=6 days, 23:23:04, max mem: 15.0 GB 
[06/14 11:58:16][INFO] visual_prompt:  301: 	Training 1000/5004. train loss: 0.0403,	1.2079 s / batch. (data: 3.09e-04). ETA=6 days, 23:33:31, max mem: 15.0 GB 
[06/14 12:00:17][INFO] visual_prompt:  301: 	Training 1100/5004. train loss: 0.0775,	1.2050 s / batch. (data: 3.38e-04). ETA=6 days, 23:07:12, max mem: 15.0 GB 
[06/14 12:02:18][INFO] visual_prompt:  301: 	Training 1200/5004. train loss: 0.0305,	1.2059 s / batch. (data: 3.36e-04). ETA=6 days, 23:12:47, max mem: 15.0 GB 
[06/14 12:04:19][INFO] visual_prompt:  301: 	Training 1300/5004. train loss: 0.0533,	1.2072 s / batch. (data: 3.74e-04). ETA=6 days, 23:21:35, max mem: 15.0 GB 
[06/14 12:06:19][INFO] visual_prompt:  301: 	Training 1400/5004. train loss: 0.0708,	1.2048 s / batch. (data: 3.11e-04). ETA=6 days, 22:59:59, max mem: 15.0 GB 
[06/14 12:08:20][INFO] visual_prompt:  301: 	Training 1500/5004. train loss: 0.0262,	1.2091 s / batch. (data: 3.31e-04). ETA=6 days, 23:33:51, max mem: 15.0 GB 
[06/14 12:10:21][INFO] visual_prompt:  301: 	Training 1600/5004. train loss: 0.0579,	1.2050 s / batch. (data: 3.13e-04). ETA=6 days, 22:57:30, max mem: 15.0 GB 
[06/14 12:12:22][INFO] visual_prompt:  301: 	Training 1700/5004. train loss: 0.0986,	1.2050 s / batch. (data: 2.88e-04). ETA=6 days, 22:55:33, max mem: 15.0 GB 
[06/14 12:14:23][INFO] visual_prompt:  301: 	Training 1800/5004. train loss: 0.0787,	1.2087 s / batch. (data: 4.07e-04). ETA=6 days, 23:24:36, max mem: 15.0 GB 
[06/14 12:16:24][INFO] visual_prompt:  301: 	Training 1900/5004. train loss: 0.0774,	1.2081 s / batch. (data: 3.52e-04). ETA=6 days, 23:17:12, max mem: 15.0 GB 
[06/14 12:18:25][INFO] visual_prompt:  301: 	Training 2000/5004. train loss: 0.1035,	1.2084 s / batch. (data: 3.56e-04). ETA=6 days, 23:18:03, max mem: 15.0 GB 
[06/14 12:20:26][INFO] visual_prompt:  301: 	Training 2100/5004. train loss: 0.0256,	1.2056 s / batch. (data: 3.04e-04). ETA=6 days, 22:52:24, max mem: 15.0 GB 
[06/14 12:22:26][INFO] visual_prompt:  301: 	Training 2200/5004. train loss: 0.0380,	1.2096 s / batch. (data: 3.05e-04). ETA=6 days, 23:23:30, max mem: 15.0 GB 
[06/14 12:24:27][INFO] visual_prompt:  301: 	Training 2300/5004. train loss: 0.0746,	1.2052 s / batch. (data: 3.05e-04). ETA=6 days, 22:45:31, max mem: 15.0 GB 
[06/14 12:26:28][INFO] visual_prompt:  301: 	Training 2400/5004. train loss: 0.0984,	1.2076 s / batch. (data: 3.43e-04). ETA=6 days, 23:02:51, max mem: 15.0 GB 
[06/14 12:28:28][INFO] visual_prompt:  301: 	Training 2500/5004. train loss: 0.0981,	1.2051 s / batch. (data: 3.47e-04). ETA=6 days, 22:40:01, max mem: 15.0 GB 
[06/14 12:30:29][INFO] visual_prompt:  301: 	Training 2600/5004. train loss: 0.0626,	1.2068 s / batch. (data: 4.57e-04). ETA=6 days, 22:52:39, max mem: 15.0 GB 
[06/14 12:32:29][INFO] visual_prompt:  301: 	Training 2700/5004. train loss: 0.0721,	1.2057 s / batch. (data: 3.34e-04). ETA=6 days, 22:40:53, max mem: 15.0 GB 
[06/14 12:34:30][INFO] visual_prompt:  301: 	Training 2800/5004. train loss: 0.0603,	1.2065 s / batch. (data: 3.60e-04). ETA=6 days, 22:45:51, max mem: 15.0 GB 
[06/14 12:36:30][INFO] visual_prompt:  301: 	Training 2900/5004. train loss: 0.0612,	1.2045 s / batch. (data: 3.26e-04). ETA=6 days, 22:27:25, max mem: 15.0 GB 
[06/14 12:38:31][INFO] visual_prompt:  301: 	Training 3000/5004. train loss: 0.0635,	1.2101 s / batch. (data: 2.57e-04). ETA=6 days, 23:11:33, max mem: 15.0 GB 
[06/14 12:40:32][INFO] visual_prompt:  301: 	Training 3100/5004. train loss: 0.0378,	1.2097 s / batch. (data: 3.50e-04). ETA=6 days, 23:06:10, max mem: 15.0 GB 
[06/14 12:42:33][INFO] visual_prompt:  301: 	Training 3200/5004. train loss: 0.0704,	1.2069 s / batch. (data: 3.28e-04). ETA=6 days, 22:41:32, max mem: 15.0 GB 
[06/14 12:44:34][INFO] visual_prompt:  301: 	Training 3300/5004. train loss: 0.0897,	1.2137 s / batch. (data: 3.36e-04). ETA=6 days, 23:35:20, max mem: 15.0 GB 
[06/14 12:46:35][INFO] visual_prompt:  301: 	Training 3400/5004. train loss: 0.0586,	1.2108 s / batch. (data: 3.59e-04). ETA=6 days, 23:09:04, max mem: 15.0 GB 
[06/14 12:48:35][INFO] visual_prompt:  301: 	Training 3500/5004. train loss: 0.0718,	1.2115 s / batch. (data: 3.92e-04). ETA=6 days, 23:12:49, max mem: 15.0 GB 
[06/14 12:50:36][INFO] visual_prompt:  301: 	Training 3600/5004. train loss: 0.0785,	1.2092 s / batch. (data: 3.59e-04). ETA=6 days, 22:51:52, max mem: 15.0 GB 
[06/14 12:52:37][INFO] visual_prompt:  301: 	Training 3700/5004. train loss: 0.0995,	1.2081 s / batch. (data: 3.68e-04). ETA=6 days, 22:41:10, max mem: 15.0 GB 
[06/14 12:54:38][INFO] visual_prompt:  301: 	Training 3800/5004. train loss: 0.0504,	1.2079 s / batch. (data: 3.75e-04). ETA=6 days, 22:37:10, max mem: 15.0 GB 
[06/14 12:56:38][INFO] visual_prompt:  301: 	Training 3900/5004. train loss: 0.0943,	1.2064 s / batch. (data: 2.88e-04). ETA=6 days, 22:23:19, max mem: 15.0 GB 
[06/14 12:58:39][INFO] visual_prompt:  301: 	Training 4000/5004. train loss: 0.0747,	1.2085 s / batch. (data: 3.24e-04). ETA=6 days, 22:38:38, max mem: 15.0 GB 
[06/14 13:00:40][INFO] visual_prompt:  301: 	Training 4100/5004. train loss: 0.0612,	1.2055 s / batch. (data: 3.56e-04). ETA=6 days, 22:11:43, max mem: 15.0 GB 
[06/14 13:02:40][INFO] visual_prompt:  301: 	Training 4200/5004. train loss: 0.0646,	1.2087 s / batch. (data: 3.26e-04). ETA=6 days, 22:35:40, max mem: 15.0 GB 
[06/14 13:04:41][INFO] visual_prompt:  301: 	Training 4300/5004. train loss: 0.0562,	1.2008 s / batch. (data: 3.39e-04). ETA=6 days, 21:28:40, max mem: 15.0 GB 
[06/14 13:06:41][INFO] visual_prompt:  301: 	Training 4400/5004. train loss: 0.0756,	1.2091 s / batch. (data: 3.08e-04). ETA=6 days, 22:35:27, max mem: 15.0 GB 
[06/14 13:08:42][INFO] visual_prompt:  301: 	Training 4500/5004. train loss: 0.0672,	1.2060 s / batch. (data: 5.06e-04). ETA=6 days, 22:07:27, max mem: 15.0 GB 
[06/14 13:10:42][INFO] visual_prompt:  301: 	Training 4600/5004. train loss: 0.0256,	1.2088 s / batch. (data: 3.73e-04). ETA=6 days, 22:28:20, max mem: 15.0 GB 
[06/14 13:12:43][INFO] visual_prompt:  301: 	Training 4700/5004. train loss: 0.0246,	1.2055 s / batch. (data: 3.53e-04). ETA=6 days, 21:59:24, max mem: 15.0 GB 
[06/14 13:14:44][INFO] visual_prompt:  301: 	Training 4800/5004. train loss: 0.0536,	1.2078 s / batch. (data: 3.44e-04). ETA=6 days, 22:16:17, max mem: 15.0 GB 
[06/14 13:16:45][INFO] visual_prompt:  301: 	Training 4900/5004. train loss: 0.0332,	1.2112 s / batch. (data: 3.44e-04). ETA=6 days, 22:42:53, max mem: 15.0 GB 
[06/14 13:18:46][INFO] visual_prompt:  301: 	Training 5000/5004. train loss: 0.0806,	1.2087 s / batch. (data: 1.11e-04). ETA=6 days, 22:19:37, max mem: 15.0 GB 
[06/14 13:18:52][INFO] visual_prompt:  314: Epoch 1 / 100: avg data time: 4.34e-03, avg batch time: 1.2116, average train loss: 0.0616
[06/14 13:27:51][INFO] visual_prompt:  434: 	Test 100/196. loss: 0.060, 5.2005 s / batch. (data: 1.46e-04)max mem: 14.99516 GB 
[06/14 13:36:10][INFO] visual_prompt:  471: Inference (val):avg data time: 1.60e-04, avg batch time: 5.1992, average loss: 0.0605
[06/14 13:36:10][INFO] visual_prompt:  488: Saved invariances for val_imagenet at output_shallow_10/val_imagenet_invariances.json
[06/14 13:36:10][INFO] visual_prompt:  252: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[06/14 13:38:55][INFO] visual_prompt:  301: 	Training 100/5004. train loss: 0.0367,	1.2067 s / batch. (data: 3.42e-04). ETA=6 days, 22:00:54, max mem: 15.0 GB 
[06/14 13:40:56][INFO] visual_prompt:  301: 	Training 200/5004. train loss: 0.1047,	1.2053 s / batch. (data: 3.39e-04). ETA=6 days, 21:48:01, max mem: 15.0 GB 
[06/14 13:42:57][INFO] visual_prompt:  301: 	Training 300/5004. train loss: 0.0618,	1.2075 s / batch. (data: 3.05e-04). ETA=6 days, 22:03:47, max mem: 15.0 GB 
[06/14 13:44:57][INFO] visual_prompt:  301: 	Training 400/5004. train loss: 0.0238,	1.2063 s / batch. (data: 2.38e-04). ETA=6 days, 21:51:50, max mem: 15.0 GB 
[06/14 13:46:58][INFO] visual_prompt:  301: 	Training 500/5004. train loss: 0.0653,	1.2057 s / batch. (data: 3.24e-04). ETA=6 days, 21:44:43, max mem: 15.0 GB 
[06/14 13:48:59][INFO] visual_prompt:  301: 	Training 600/5004. train loss: 0.0740,	1.2076 s / batch. (data: 2.71e-04). ETA=6 days, 21:58:19, max mem: 15.0 GB 
[06/14 13:50:59][INFO] visual_prompt:  301: 	Training 700/5004. train loss: 0.0393,	1.2082 s / batch. (data: 3.71e-04). ETA=6 days, 22:01:14, max mem: 15.0 GB 
[06/14 13:53:00][INFO] visual_prompt:  301: 	Training 800/5004. train loss: 0.0231,	1.2111 s / batch. (data: 3.25e-04). ETA=6 days, 22:23:01, max mem: 15.0 GB 
[06/14 13:55:01][INFO] visual_prompt:  301: 	Training 900/5004. train loss: 0.0908,	1.2120 s / batch. (data: 2.92e-04). ETA=6 days, 22:28:50, max mem: 15.0 GB 
[06/14 13:57:02][INFO] visual_prompt:  301: 	Training 1000/5004. train loss: 0.0947,	1.2136 s / batch. (data: 3.31e-04). ETA=6 days, 22:39:53, max mem: 15.0 GB 
[06/14 13:59:03][INFO] visual_prompt:  301: 	Training 1100/5004. train loss: 0.0664,	1.2096 s / batch. (data: 3.09e-04). ETA=6 days, 22:04:40, max mem: 15.0 GB 
[06/14 14:01:04][INFO] visual_prompt:  301: 	Training 1200/5004. train loss: 0.0398,	1.2136 s / batch. (data: 3.82e-04). ETA=6 days, 22:36:12, max mem: 15.0 GB 
[06/14 14:03:05][INFO] visual_prompt:  301: 	Training 1300/5004. train loss: 0.0320,	1.2051 s / batch. (data: 3.58e-04). ETA=6 days, 21:24:11, max mem: 15.0 GB 
[06/14 14:05:06][INFO] visual_prompt:  301: 	Training 1400/5004. train loss: 0.0562,	1.2052 s / batch. (data: 3.48e-04). ETA=6 days, 21:22:41, max mem: 15.0 GB 
[06/14 14:07:07][INFO] visual_prompt:  301: 	Training 1500/5004. train loss: 0.0604,	1.2081 s / batch. (data: 2.87e-04). ETA=6 days, 21:44:41, max mem: 15.0 GB 
[06/14 14:09:07][INFO] visual_prompt:  301: 	Training 1600/5004. train loss: 0.0807,	1.2052 s / batch. (data: 3.13e-04). ETA=6 days, 21:18:26, max mem: 15.0 GB 
[06/14 14:11:08][INFO] visual_prompt:  301: 	Training 1700/5004. train loss: 0.0943,	1.2063 s / batch. (data: 3.42e-04). ETA=6 days, 21:25:21, max mem: 15.0 GB 
[06/14 14:13:09][INFO] visual_prompt:  301: 	Training 1800/5004. train loss: 0.0241,	1.2072 s / batch. (data: 3.05e-04). ETA=6 days, 21:31:01, max mem: 15.0 GB 
[06/14 14:15:10][INFO] visual_prompt:  301: 	Training 1900/5004. train loss: 0.0619,	1.2073 s / batch. (data: 3.08e-04). ETA=6 days, 21:29:40, max mem: 15.0 GB 
[06/14 14:17:10][INFO] visual_prompt:  301: 	Training 2000/5004. train loss: 0.0227,	1.2092 s / batch. (data: 3.01e-04). ETA=6 days, 21:43:46, max mem: 15.0 GB 
[06/14 14:19:11][INFO] visual_prompt:  301: 	Training 2100/5004. train loss: 0.0658,	1.2085 s / batch. (data: 2.94e-04). ETA=6 days, 21:36:05, max mem: 15.0 GB 
[06/14 14:21:12][INFO] visual_prompt:  301: 	Training 2200/5004. train loss: 0.0363,	1.2054 s / batch. (data: 3.11e-04). ETA=6 days, 21:08:26, max mem: 15.0 GB 
[06/14 14:23:12][INFO] visual_prompt:  301: 	Training 2300/5004. train loss: 0.0516,	1.2108 s / batch. (data: 2.90e-04). ETA=6 days, 21:50:17, max mem: 15.0 GB 
[06/14 14:25:13][INFO] visual_prompt:  301: 	Training 2400/5004. train loss: 0.0521,	1.2082 s / batch. (data: 4.18e-04). ETA=6 days, 21:27:11, max mem: 15.0 GB 
[06/14 14:27:14][INFO] visual_prompt:  301: 	Training 2500/5004. train loss: 0.0322,	1.2100 s / batch. (data: 3.18e-04). ETA=6 days, 21:39:43, max mem: 15.0 GB 
[06/14 14:29:15][INFO] visual_prompt:  301: 	Training 2600/5004. train loss: 0.0566,	1.2102 s / batch. (data: 3.78e-04). ETA=6 days, 21:40:05, max mem: 15.0 GB 
[06/14 14:31:16][INFO] visual_prompt:  301: 	Training 2700/5004. train loss: 0.0410,	1.2085 s / batch. (data: 3.03e-04). ETA=6 days, 21:23:57, max mem: 15.0 GB 
[06/14 14:33:17][INFO] visual_prompt:  301: 	Training 2800/5004. train loss: 0.0513,	1.2106 s / batch. (data: 3.26e-04). ETA=6 days, 21:38:38, max mem: 15.0 GB 
[06/14 14:35:18][INFO] visual_prompt:  301: 	Training 2900/5004. train loss: 0.0608,	1.2068 s / batch. (data: 3.36e-04). ETA=6 days, 21:05:21, max mem: 15.0 GB 
[06/14 14:37:19][INFO] visual_prompt:  301: 	Training 3000/5004. train loss: 0.0580,	1.2055 s / batch. (data: 3.11e-04). ETA=6 days, 20:53:01, max mem: 15.0 GB 
[06/14 14:39:20][INFO] visual_prompt:  301: 	Training 3100/5004. train loss: 0.0203,	1.2084 s / batch. (data: 3.70e-04). ETA=6 days, 21:14:26, max mem: 15.0 GB 
[06/14 14:41:21][INFO] visual_prompt:  301: 	Training 3200/5004. train loss: 0.0233,	1.2043 s / batch. (data: 3.24e-04). ETA=6 days, 20:39:22, max mem: 15.0 GB 
[06/14 14:43:21][INFO] visual_prompt:  301: 	Training 3300/5004. train loss: 0.0588,	1.2043 s / batch. (data: 3.74e-04). ETA=6 days, 20:37:06, max mem: 15.0 GB 
[06/14 14:45:22][INFO] visual_prompt:  301: 	Training 3400/5004. train loss: 0.0581,	1.2047 s / batch. (data: 3.51e-04). ETA=6 days, 20:38:11, max mem: 15.0 GB 
[06/14 14:47:23][INFO] visual_prompt:  301: 	Training 3500/5004. train loss: 0.0207,	1.2063 s / batch. (data: 2.80e-04). ETA=6 days, 20:49:32, max mem: 15.0 GB 
[06/14 14:49:23][INFO] visual_prompt:  301: 	Training 3600/5004. train loss: 0.0414,	1.2076 s / batch. (data: 2.91e-04). ETA=6 days, 20:58:03, max mem: 15.0 GB 
[06/14 14:51:24][INFO] visual_prompt:  301: 	Training 3700/5004. train loss: 0.0505,	1.2113 s / batch. (data: 2.66e-04). ETA=6 days, 21:26:28, max mem: 15.0 GB 
[06/14 14:53:25][INFO] visual_prompt:  301: 	Training 3800/5004. train loss: 0.0495,	1.2071 s / batch. (data: 3.70e-04). ETA=6 days, 20:50:27, max mem: 15.0 GB 
[06/14 14:55:25][INFO] visual_prompt:  301: 	Training 3900/5004. train loss: 0.0547,	1.2049 s / batch. (data: 3.07e-04). ETA=6 days, 20:29:45, max mem: 15.0 GB 
[06/14 14:57:26][INFO] visual_prompt:  301: 	Training 4000/5004. train loss: 0.0526,	1.2027 s / batch. (data: 3.24e-04). ETA=6 days, 20:09:51, max mem: 15.0 GB 
[06/14 14:59:26][INFO] visual_prompt:  301: 	Training 4100/5004. train loss: 0.0208,	1.2068 s / batch. (data: 3.37e-04). ETA=6 days, 20:41:41, max mem: 15.0 GB 
[06/14 15:01:27][INFO] visual_prompt:  301: 	Training 4200/5004. train loss: 0.0705,	1.2057 s / batch. (data: 2.97e-04). ETA=6 days, 20:30:34, max mem: 15.0 GB 
[06/14 15:03:28][INFO] visual_prompt:  301: 	Training 4300/5004. train loss: 0.0383,	1.2104 s / batch. (data: 3.48e-04). ETA=6 days, 21:07:15, max mem: 15.0 GB 
[06/14 15:05:29][INFO] visual_prompt:  301: 	Training 4400/5004. train loss: 0.0646,	1.2052 s / batch. (data: 3.32e-04). ETA=6 days, 20:22:39, max mem: 15.0 GB 
[06/14 15:07:30][INFO] visual_prompt:  301: 	Training 4500/5004. train loss: 0.0648,	1.2146 s / batch. (data: 3.40e-04). ETA=6 days, 21:37:27, max mem: 15.0 GB 
[06/14 15:09:31][INFO] visual_prompt:  301: 	Training 4600/5004. train loss: 0.0753,	1.2067 s / batch. (data: 3.19e-04). ETA=6 days, 20:30:26, max mem: 15.0 GB 
[06/14 15:11:32][INFO] visual_prompt:  301: 	Training 4700/5004. train loss: 0.0333,	1.2106 s / batch. (data: 3.14e-04). ETA=6 days, 21:00:34, max mem: 15.0 GB 
[06/14 15:13:33][INFO] visual_prompt:  301: 	Training 4800/5004. train loss: 0.0485,	1.2008 s / batch. (data: 3.34e-04). ETA=6 days, 19:38:35, max mem: 15.0 GB 
[06/14 15:15:34][INFO] visual_prompt:  301: 	Training 4900/5004. train loss: 0.0188,	1.2125 s / batch. (data: 3.59e-04). ETA=6 days, 21:12:12, max mem: 15.0 GB 
[06/14 15:17:35][INFO] visual_prompt:  301: 	Training 5000/5004. train loss: 0.0358,	1.2061 s / batch. (data: 1.19e-04). ETA=6 days, 20:17:34, max mem: 15.0 GB 
[06/14 15:17:41][INFO] visual_prompt:  314: Epoch 2 / 100: avg data time: 4.07e-03, avg batch time: 1.2169, average train loss: 0.0579
[06/14 15:26:37][INFO] visual_prompt:  434: 	Test 100/196. loss: 0.056, 5.1925 s / batch. (data: 1.72e-04)max mem: 14.99516 GB 
[06/14 15:34:54][INFO] visual_prompt:  471: Inference (val):avg data time: 1.45e-04, avg batch time: 5.1765, average loss: 0.0553
[06/14 15:34:54][INFO] visual_prompt:  488: Saved invariances for val_imagenet at output_shallow_10/val_imagenet_invariances.json
[06/14 15:34:54][INFO] visual_prompt:  252: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[06/14 15:37:39][INFO] visual_prompt:  301: 	Training 100/5004. train loss: 0.0868,	1.2092 s / batch. (data: 2.91e-04). ETA=6 days, 20:41:22, max mem: 15.0 GB 
[06/14 15:39:40][INFO] visual_prompt:  301: 	Training 200/5004. train loss: 0.0353,	1.2103 s / batch. (data: 3.41e-04). ETA=6 days, 20:47:48, max mem: 15.0 GB 
[06/14 15:41:41][INFO] visual_prompt:  301: 	Training 300/5004. train loss: 0.0585,	1.2133 s / batch. (data: 2.25e-04). ETA=6 days, 21:10:36, max mem: 15.0 GB 
[06/14 15:43:42][INFO] visual_prompt:  301: 	Training 400/5004. train loss: 0.0898,	1.2132 s / batch. (data: 3.36e-04). ETA=6 days, 21:07:16, max mem: 15.0 GB 
[06/14 15:45:43][INFO] visual_prompt:  301: 	Training 500/5004. train loss: 0.0637,	1.2106 s / batch. (data: 3.20e-04). ETA=6 days, 20:44:44, max mem: 15.0 GB 
[06/14 15:47:44][INFO] visual_prompt:  301: 	Training 600/5004. train loss: 0.0555,	1.2097 s / batch. (data: 3.44e-04). ETA=6 days, 20:34:51, max mem: 15.0 GB 
[06/14 15:49:45][INFO] visual_prompt:  301: 	Training 700/5004. train loss: 0.0735,	1.2129 s / batch. (data: 4.33e-04). ETA=6 days, 20:59:19, max mem: 15.0 GB 
[06/14 15:51:46][INFO] visual_prompt:  301: 	Training 800/5004. train loss: 0.0541,	1.2127 s / batch. (data: 3.96e-04). ETA=6 days, 20:55:26, max mem: 15.0 GB 
[06/14 15:53:47][INFO] visual_prompt:  301: 	Training 900/5004. train loss: 0.0527,	1.2058 s / batch. (data: 3.69e-04). ETA=6 days, 19:57:15, max mem: 15.0 GB 
[06/14 15:55:48][INFO] visual_prompt:  301: 	Training 1000/5004. train loss: 0.0205,	1.2058 s / batch. (data: 3.24e-04). ETA=6 days, 19:55:17, max mem: 15.0 GB 
[06/14 15:57:48][INFO] visual_prompt:  301: 	Training 1100/5004. train loss: 0.0192,	1.2049 s / batch. (data: 3.86e-04). ETA=6 days, 19:45:37, max mem: 15.0 GB 
[06/14 15:59:49][INFO] visual_prompt:  301: 	Training 1200/5004. train loss: 0.0487,	1.2071 s / batch. (data: 3.10e-04). ETA=6 days, 20:01:44, max mem: 15.0 GB 
[06/14 16:01:50][INFO] visual_prompt:  301: 	Training 1300/5004. train loss: 0.0314,	1.2075 s / batch. (data: 3.10e-04). ETA=6 days, 20:02:35, max mem: 15.0 GB 
[06/14 16:03:50][INFO] visual_prompt:  301: 	Training 1400/5004. train loss: 0.0863,	1.2080 s / batch. (data: 3.88e-04). ETA=6 days, 20:04:41, max mem: 15.0 GB 
[06/14 16:05:51][INFO] visual_prompt:  301: 	Training 1500/5004. train loss: 0.0834,	1.2084 s / batch. (data: 3.19e-04). ETA=6 days, 20:06:23, max mem: 15.0 GB 
[06/14 16:07:52][INFO] visual_prompt:  301: 	Training 1600/5004. train loss: 0.0568,	1.2039 s / batch. (data: 3.12e-04). ETA=6 days, 19:27:37, max mem: 15.0 GB 
[06/14 16:09:53][INFO] visual_prompt:  301: 	Training 1700/5004. train loss: 0.0739,	1.2069 s / batch. (data: 3.19e-04). ETA=6 days, 19:50:10, max mem: 15.0 GB 
[06/14 16:11:53][INFO] visual_prompt:  301: 	Training 1800/5004. train loss: 0.0214,	1.2109 s / batch. (data: 3.92e-04). ETA=6 days, 20:20:23, max mem: 15.0 GB 
[06/14 16:13:54][INFO] visual_prompt:  301: 	Training 1900/5004. train loss: 0.0458,	1.2119 s / batch. (data: 4.68e-04). ETA=6 days, 20:27:06, max mem: 15.0 GB 
[06/14 16:15:55][INFO] visual_prompt:  301: 	Training 2000/5004. train loss: 0.0354,	1.2085 s / batch. (data: 3.24e-04). ETA=6 days, 19:56:47, max mem: 15.0 GB 
[06/14 16:17:56][INFO] visual_prompt:  301: 	Training 2100/5004. train loss: 0.0558,	1.2078 s / batch. (data: 3.61e-04). ETA=6 days, 19:48:55, max mem: 15.0 GB 
[06/14 16:19:57][INFO] visual_prompt:  301: 	Training 2200/5004. train loss: 0.0473,	1.2101 s / batch. (data: 3.44e-04). ETA=6 days, 20:06:12, max mem: 15.0 GB 
[06/14 16:21:58][INFO] visual_prompt:  301: 	Training 2300/5004. train loss: 0.0537,	1.2118 s / batch. (data: 2.60e-04). ETA=6 days, 20:17:53, max mem: 15.0 GB 
[06/14 16:23:59][INFO] visual_prompt:  301: 	Training 2400/5004. train loss: 0.0885,	1.2104 s / batch. (data: 3.23e-04). ETA=6 days, 20:04:15, max mem: 15.0 GB 
[06/14 16:26:00][INFO] visual_prompt:  301: 	Training 2500/5004. train loss: 0.0516,	1.2073 s / batch. (data: 2.98e-04). ETA=6 days, 19:37:21, max mem: 15.0 GB 
[06/14 16:28:01][INFO] visual_prompt:  301: 	Training 2600/5004. train loss: 0.0565,	1.2115 s / batch. (data: 3.06e-04). ETA=6 days, 20:09:41, max mem: 15.0 GB 
[06/14 16:30:02][INFO] visual_prompt:  301: 	Training 2700/5004. train loss: 0.0710,	1.2103 s / batch. (data: 3.00e-04). ETA=6 days, 19:57:39, max mem: 15.0 GB 
[06/14 16:32:03][INFO] visual_prompt:  301: 	Training 2800/5004. train loss: 0.0298,	1.2031 s / batch. (data: 3.11e-04). ETA=6 days, 18:57:22, max mem: 15.0 GB 
[06/14 16:34:03][INFO] visual_prompt:  301: 	Training 2900/5004. train loss: 0.0553,	1.2095 s / batch. (data: 3.67e-04). ETA=6 days, 19:46:39, max mem: 15.0 GB 
[06/14 16:36:04][INFO] visual_prompt:  301: 	Training 3000/5004. train loss: 0.0794,	1.2058 s / batch. (data: 3.20e-04). ETA=6 days, 19:15:04, max mem: 15.0 GB 
[06/14 16:38:05][INFO] visual_prompt:  301: 	Training 3100/5004. train loss: 0.0419,	1.2066 s / batch. (data: 2.84e-04). ETA=6 days, 19:19:34, max mem: 15.0 GB 
[06/14 16:40:06][INFO] visual_prompt:  301: 	Training 3200/5004. train loss: 0.0527,	1.2123 s / batch. (data: 3.16e-04). ETA=6 days, 20:03:23, max mem: 15.0 GB 
[06/14 16:42:06][INFO] visual_prompt:  301: 	Training 3300/5004. train loss: 0.0787,	1.2070 s / batch. (data: 2.85e-04). ETA=6 days, 19:18:44, max mem: 15.0 GB 
[06/14 16:44:07][INFO] visual_prompt:  301: 	Training 3400/5004. train loss: 0.0339,	1.2092 s / batch. (data: 3.32e-04). ETA=6 days, 19:34:51, max mem: 15.0 GB 
[06/14 16:46:08][INFO] visual_prompt:  301: 	Training 3500/5004. train loss: 0.0283,	1.2092 s / batch. (data: 2.93e-04). ETA=6 days, 19:32:53, max mem: 15.0 GB 
[06/14 16:48:09][INFO] visual_prompt:  301: 	Training 3600/5004. train loss: 0.0475,	1.2177 s / batch. (data: 2.20e-04). ETA=6 days, 20:39:25, max mem: 15.0 GB 
[06/14 16:50:10][INFO] visual_prompt:  301: 	Training 3700/5004. train loss: 0.0470,	1.2127 s / batch. (data: 3.10e-04). ETA=6 days, 19:57:13, max mem: 15.0 GB 
[06/14 16:52:11][INFO] visual_prompt:  301: 	Training 3800/5004. train loss: 0.0561,	1.2119 s / batch. (data: 3.71e-04). ETA=6 days, 19:48:11, max mem: 15.0 GB 
[06/14 16:54:12][INFO] visual_prompt:  301: 	Training 3900/5004. train loss: 0.0670,	1.2068 s / batch. (data: 3.52e-04). ETA=6 days, 19:04:35, max mem: 15.0 GB 
[06/14 16:56:13][INFO] visual_prompt:  301: 	Training 4000/5004. train loss: 0.0841,	1.2139 s / batch. (data: 2.85e-04). ETA=6 days, 20:00:43, max mem: 15.0 GB 
[06/14 16:58:14][INFO] visual_prompt:  301: 	Training 4100/5004. train loss: 0.0231,	1.2117 s / batch. (data: 3.94e-04). ETA=6 days, 19:40:22, max mem: 15.0 GB 
[06/14 17:00:15][INFO] visual_prompt:  301: 	Training 4200/5004. train loss: 0.0197,	1.2062 s / batch. (data: 3.32e-04). ETA=6 days, 18:54:22, max mem: 15.0 GB 
[06/14 17:02:16][INFO] visual_prompt:  301: 	Training 4300/5004. train loss: 0.0499,	1.2086 s / batch. (data: 3.61e-04). ETA=6 days, 19:11:26, max mem: 15.0 GB 
[06/14 17:04:17][INFO] visual_prompt:  301: 	Training 4400/5004. train loss: 0.0575,	1.2078 s / batch. (data: 3.37e-04). ETA=6 days, 19:03:24, max mem: 15.0 GB 
[06/14 17:06:18][INFO] visual_prompt:  301: 	Training 4500/5004. train loss: 0.0522,	1.2087 s / batch. (data: 2.87e-04). ETA=6 days, 19:08:15, max mem: 15.0 GB 
[06/14 17:08:19][INFO] visual_prompt:  301: 	Training 4600/5004. train loss: 0.0727,	1.2064 s / batch. (data: 3.24e-04). ETA=6 days, 18:47:52, max mem: 15.0 GB 
[06/14 17:10:20][INFO] visual_prompt:  301: 	Training 4700/5004. train loss: 0.0517,	1.2143 s / batch. (data: 3.52e-04). ETA=6 days, 19:49:33, max mem: 15.0 GB 
[06/14 17:12:20][INFO] visual_prompt:  301: 	Training 4800/5004. train loss: 0.0415,	1.2028 s / batch. (data: 2.77e-04). ETA=6 days, 18:14:18, max mem: 15.0 GB 
[06/14 17:14:21][INFO] visual_prompt:  301: 	Training 4900/5004. train loss: 0.0424,	1.2039 s / batch. (data: 3.04e-04). ETA=6 days, 18:21:27, max mem: 15.0 GB 
[06/14 17:16:22][INFO] visual_prompt:  301: 	Training 5000/5004. train loss: 0.0607,	1.2024 s / batch. (data: 1.13e-04). ETA=6 days, 18:07:03, max mem: 15.0 GB 
[06/14 17:16:28][INFO] visual_prompt:  314: Epoch 3 / 100: avg data time: 4.10e-03, avg batch time: 1.2175, average train loss: 0.0533
[06/14 17:25:22][INFO] visual_prompt:  434: 	Test 100/196. loss: 0.045, 5.2058 s / batch. (data: 1.33e-04)max mem: 14.99516 GB 
[06/14 17:33:41][INFO] visual_prompt:  471: Inference (val):avg data time: 1.41e-04, avg batch time: 5.1824, average loss: 0.0446
[06/14 17:33:41][INFO] visual_prompt:  488: Saved invariances for val_imagenet at output_shallow_10/val_imagenet_invariances.json
[06/14 17:33:41][INFO] visual_prompt:  252: Training 4 / 100 epoch, with learning rate 0.03
[06/14 17:36:28][INFO] visual_prompt:  301: 	Training 100/5004. train loss: 0.0559,	1.2093 s / batch. (data: 3.59e-04). ETA=6 days, 19:01:12, max mem: 15.0 GB 
[06/14 17:38:29][INFO] visual_prompt:  301: 	Training 200/5004. train loss: 0.0559,	1.2072 s / batch. (data: 2.91e-04). ETA=6 days, 18:42:18, max mem: 15.0 GB 
[06/14 17:40:30][INFO] visual_prompt:  301: 	Training 300/5004. train loss: 0.0304,	1.2065 s / batch. (data: 3.21e-04). ETA=6 days, 18:34:36, max mem: 15.0 GB 
[06/14 17:42:31][INFO] visual_prompt:  301: 	Training 400/5004. train loss: 0.0499,	1.2076 s / batch. (data: 2.79e-04). ETA=6 days, 18:40:57, max mem: 15.0 GB 
[06/14 17:44:32][INFO] visual_prompt:  301: 	Training 500/5004. train loss: 0.0198,	1.2061 s / batch. (data: 4.54e-04). ETA=6 days, 18:27:22, max mem: 15.0 GB 
[06/14 17:46:33][INFO] visual_prompt:  301: 	Training 600/5004. train loss: 0.0301,	1.2061 s / batch. (data: 3.12e-04). ETA=6 days, 18:24:52, max mem: 15.0 GB 
[06/14 17:48:33][INFO] visual_prompt:  301: 	Training 700/5004. train loss: 0.0239,	1.2089 s / batch. (data: 2.93e-04). ETA=6 days, 18:45:44, max mem: 15.0 GB 
[06/14 17:50:34][INFO] visual_prompt:  301: 	Training 800/5004. train loss: 0.0450,	1.2069 s / batch. (data: 3.01e-04). ETA=6 days, 18:27:40, max mem: 15.0 GB 
[06/14 17:52:35][INFO] visual_prompt:  301: 	Training 900/5004. train loss: 0.0378,	1.2085 s / batch. (data: 3.60e-04). ETA=6 days, 18:38:23, max mem: 15.0 GB 
[06/14 17:54:36][INFO] visual_prompt:  301: 	Training 1000/5004. train loss: 0.0177,	1.2086 s / batch. (data: 3.42e-04). ETA=6 days, 18:36:56, max mem: 15.0 GB 
[06/14 17:56:36][INFO] visual_prompt:  301: 	Training 1100/5004. train loss: 0.0333,	1.2055 s / batch. (data: 3.41e-04). ETA=6 days, 18:10:24, max mem: 15.0 GB 
[06/14 17:58:37][INFO] visual_prompt:  301: 	Training 1200/5004. train loss: 0.0405,	1.2088 s / batch. (data: 2.99e-04). ETA=6 days, 18:34:30, max mem: 15.0 GB 
[06/14 18:00:38][INFO] visual_prompt:  301: 	Training 1300/5004. train loss: 0.0298,	1.2152 s / batch. (data: 3.31e-04). ETA=6 days, 19:24:15, max mem: 15.0 GB 
[06/14 18:02:39][INFO] visual_prompt:  301: 	Training 1400/5004. train loss: 0.0519,	1.2119 s / batch. (data: 3.07e-04). ETA=6 days, 18:55:44, max mem: 15.0 GB 
[06/14 18:04:40][INFO] visual_prompt:  301: 	Training 1500/5004. train loss: 0.0310,	1.2104 s / batch. (data: 3.15e-04). ETA=6 days, 18:41:21, max mem: 15.0 GB 
[06/14 18:06:41][INFO] visual_prompt:  301: 	Training 1600/5004. train loss: 0.0433,	1.2082 s / batch. (data: 3.10e-04). ETA=6 days, 18:21:54, max mem: 15.0 GB 
[06/14 18:08:42][INFO] visual_prompt:  301: 	Training 1700/5004. train loss: 0.0298,	1.2087 s / batch. (data: 3.01e-04). ETA=6 days, 18:23:36, max mem: 15.0 GB 
[06/14 18:10:43][INFO] visual_prompt:  301: 	Training 1800/5004. train loss: 0.0283,	1.2113 s / batch. (data: 3.09e-04). ETA=6 days, 18:43:00, max mem: 15.0 GB 
[06/14 18:12:44][INFO] visual_prompt:  301: 	Training 1900/5004. train loss: 0.0435,	1.2092 s / batch. (data: 3.22e-04). ETA=6 days, 18:24:00, max mem: 15.0 GB 
[06/14 18:14:45][INFO] visual_prompt:  301: 	Training 2000/5004. train loss: 0.0497,	1.2103 s / batch. (data: 1.80e-04). ETA=6 days, 18:30:54, max mem: 15.0 GB 
[06/14 18:16:46][INFO] visual_prompt:  301: 	Training 2100/5004. train loss: 0.0278,	1.2086 s / batch. (data: 3.02e-04). ETA=6 days, 18:15:19, max mem: 15.0 GB 
[06/14 18:18:47][INFO] visual_prompt:  301: 	Training 2200/5004. train loss: 0.0303,	1.2099 s / batch. (data: 3.06e-04). ETA=6 days, 18:23:52, max mem: 15.0 GB 
[06/14 18:20:47][INFO] visual_prompt:  301: 	Training 2300/5004. train loss: 0.0171,	1.2114 s / batch. (data: 3.31e-04). ETA=6 days, 18:33:34, max mem: 15.0 GB 
[06/14 18:22:48][INFO] visual_prompt:  301: 	Training 2400/5004. train loss: 0.0457,	1.2076 s / batch. (data: 3.37e-04). ETA=6 days, 18:01:10, max mem: 15.0 GB 
[06/14 18:24:49][INFO] visual_prompt:  301: 	Training 2500/5004. train loss: 0.0414,	1.2053 s / batch. (data: 2.93e-04). ETA=6 days, 17:40:22, max mem: 15.0 GB 
[06/14 18:26:50][INFO] visual_prompt:  301: 	Training 2600/5004. train loss: 0.0471,	1.2051 s / batch. (data: 3.38e-04). ETA=6 days, 17:37:09, max mem: 15.0 GB 
[06/14 18:28:51][INFO] visual_prompt:  301: 	Training 2700/5004. train loss: 0.0197,	1.2112 s / batch. (data: 2.49e-04). ETA=6 days, 18:23:48, max mem: 15.0 GB 
[06/14 18:30:52][INFO] visual_prompt:  301: 	Training 2800/5004. train loss: 0.0417,	1.2102 s / batch. (data: 4.19e-04). ETA=6 days, 18:14:07, max mem: 15.0 GB 
[06/14 18:32:52][INFO] visual_prompt:  301: 	Training 2900/5004. train loss: 0.0242,	1.2069 s / batch. (data: 2.77e-04). ETA=6 days, 17:45:28, max mem: 15.0 GB 
[06/14 18:34:53][INFO] visual_prompt:  301: 	Training 3000/5004. train loss: 0.0399,	1.2091 s / batch. (data: 3.24e-04). ETA=6 days, 18:01:15, max mem: 15.0 GB 
[06/14 18:36:54][INFO] visual_prompt:  301: 	Training 3100/5004. train loss: 0.0397,	1.2143 s / batch. (data: 3.50e-04). ETA=6 days, 18:40:54, max mem: 15.0 GB 
[06/14 18:38:55][INFO] visual_prompt:  301: 	Training 3200/5004. train loss: 0.0479,	1.2064 s / batch. (data: 3.73e-04). ETA=6 days, 17:34:54, max mem: 15.0 GB 
[06/14 18:40:56][INFO] visual_prompt:  301: 	Training 3300/5004. train loss: 0.0414,	1.2119 s / batch. (data: 3.22e-04). ETA=6 days, 18:17:19, max mem: 15.0 GB 
[06/14 18:42:57][INFO] visual_prompt:  301: 	Training 3400/5004. train loss: 0.0271,	1.2108 s / batch. (data: 4.09e-04). ETA=6 days, 18:06:49, max mem: 15.0 GB 
[06/14 18:44:58][INFO] visual_prompt:  301: 	Training 3500/5004. train loss: 0.0387,	1.2107 s / batch. (data: 3.45e-04). ETA=6 days, 18:03:33, max mem: 15.0 GB 
[06/14 18:46:59][INFO] visual_prompt:  301: 	Training 3600/5004. train loss: 0.0471,	1.2129 s / batch. (data: 3.14e-04). ETA=6 days, 18:19:24, max mem: 15.0 GB 
[06/14 18:49:01][INFO] visual_prompt:  301: 	Training 3700/5004. train loss: 0.0231,	1.2088 s / batch. (data: 2.91e-04). ETA=6 days, 17:44:21, max mem: 15.0 GB 
[06/14 18:51:02][INFO] visual_prompt:  301: 	Training 3800/5004. train loss: 0.0465,	1.2060 s / batch. (data: 3.41e-04). ETA=6 days, 17:19:56, max mem: 15.0 GB 
[06/14 18:53:03][INFO] visual_prompt:  301: 	Training 3900/5004. train loss: 0.0372,	1.2047 s / batch. (data: 3.40e-04). ETA=6 days, 17:07:51, max mem: 15.0 GB 
[06/14 18:55:03][INFO] visual_prompt:  301: 	Training 4000/5004. train loss: 0.0188,	1.2063 s / batch. (data: 2.92e-04). ETA=6 days, 17:18:24, max mem: 15.0 GB 
[06/14 18:57:04][INFO] visual_prompt:  301: 	Training 4100/5004. train loss: 0.0156,	1.2072 s / batch. (data: 3.27e-04). ETA=6 days, 17:23:21, max mem: 15.0 GB 
[06/14 18:59:05][INFO] visual_prompt:  301: 	Training 4200/5004. train loss: 0.0388,	1.2057 s / batch. (data: 2.92e-04). ETA=6 days, 17:09:24, max mem: 15.0 GB 
[06/14 19:01:06][INFO] visual_prompt:  301: 	Training 4300/5004. train loss: 0.0181,	1.2099 s / batch. (data: 3.33e-04). ETA=6 days, 17:40:56, max mem: 15.0 GB 
[06/14 19:03:06][INFO] visual_prompt:  301: 	Training 4400/5004. train loss: 0.0115,	1.2056 s / batch. (data: 3.33e-04). ETA=6 days, 17:05:00, max mem: 15.0 GB 
[06/14 19:05:07][INFO] visual_prompt:  301: 	Training 4500/5004. train loss: 0.0360,	1.2122 s / batch. (data: 3.24e-04). ETA=6 days, 17:55:52, max mem: 15.0 GB 
[06/14 19:07:08][INFO] visual_prompt:  301: 	Training 4600/5004. train loss: 0.0169,	1.2051 s / batch. (data: 3.21e-04). ETA=6 days, 16:56:34, max mem: 15.0 GB 
[06/14 19:09:09][INFO] visual_prompt:  301: 	Training 4700/5004. train loss: 0.0229,	1.2050 s / batch. (data: 3.41e-04). ETA=6 days, 16:54:09, max mem: 15.0 GB 
[06/14 19:11:09][INFO] visual_prompt:  301: 	Training 4800/5004. train loss: 0.0158,	1.2097 s / batch. (data: 3.43e-04). ETA=6 days, 17:29:35, max mem: 15.0 GB 
[06/14 19:13:10][INFO] visual_prompt:  301: 	Training 4900/5004. train loss: 0.0360,	1.2092 s / batch. (data: 3.37e-04). ETA=6 days, 17:23:35, max mem: 15.0 GB 
[06/14 19:15:11][INFO] visual_prompt:  301: 	Training 5000/5004. train loss: 0.0180,	1.2107 s / batch. (data: 1.28e-04). ETA=6 days, 17:33:46, max mem: 15.0 GB 
[06/14 19:15:17][INFO] visual_prompt:  314: Epoch 4 / 100: avg data time: 4.19e-03, avg batch time: 1.2181, average train loss: 0.0363
[06/14 19:24:13][INFO] visual_prompt:  434: 	Test 100/196. loss: 0.025, 5.2048 s / batch. (data: 2.19e-04)max mem: 14.99516 GB 
[06/14 19:32:31][INFO] visual_prompt:  471: Inference (val):avg data time: 1.49e-04, avg batch time: 5.1821, average loss: 0.0256
[06/14 19:32:31][INFO] visual_prompt:  488: Saved invariances for val_imagenet at output_shallow_10/val_imagenet_invariances.json
[06/14 19:32:31][INFO] visual_prompt:  252: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[06/14 19:35:17][INFO] visual_prompt:  301: 	Training 100/5004. train loss: 0.0108,	1.2054 s / batch. (data: 3.17e-04). ETA=6 days, 16:48:58, max mem: 15.0 GB 
[06/14 19:37:17][INFO] visual_prompt:  301: 	Training 200/5004. train loss: 0.0132,	1.2077 s / batch. (data: 3.10e-04). ETA=6 days, 17:05:27, max mem: 15.0 GB 
[06/14 19:39:18][INFO] visual_prompt:  301: 	Training 300/5004. train loss: 0.0368,	1.2105 s / batch. (data: 2.96e-04). ETA=6 days, 17:25:54, max mem: 15.0 GB 
[06/14 19:41:19][INFO] visual_prompt:  301: 	Training 400/5004. train loss: 0.0119,	1.2070 s / batch. (data: 3.36e-04). ETA=6 days, 16:55:35, max mem: 15.0 GB 
[06/14 19:43:19][INFO] visual_prompt:  301: 	Training 500/5004. train loss: 0.0400,	1.2060 s / batch. (data: 2.89e-04). ETA=6 days, 16:45:35, max mem: 15.0 GB 
[06/14 19:45:20][INFO] visual_prompt:  301: 	Training 600/5004. train loss: 0.0451,	1.2079 s / batch. (data: 3.96e-04). ETA=6 days, 16:58:36, max mem: 15.0 GB 
[06/14 19:47:21][INFO] visual_prompt:  301: 	Training 700/5004. train loss: 0.0393,	1.2052 s / batch. (data: 4.59e-04). ETA=6 days, 16:35:06, max mem: 15.0 GB 
[06/14 19:49:22][INFO] visual_prompt:  301: 	Training 800/5004. train loss: 0.0279,	1.2061 s / batch. (data: 3.36e-04). ETA=6 days, 16:40:44, max mem: 15.0 GB 
[06/14 19:51:23][INFO] visual_prompt:  301: 	Training 900/5004. train loss: 0.0230,	1.2094 s / batch. (data: 3.09e-04). ETA=6 days, 17:04:44, max mem: 15.0 GB 
[06/14 19:53:24][INFO] visual_prompt:  301: 	Training 1000/5004. train loss: 0.0318,	1.2048 s / batch. (data: 2.90e-04). ETA=6 days, 16:26:12, max mem: 15.0 GB 
[06/14 19:55:25][INFO] visual_prompt:  301: 	Training 1100/5004. train loss: 0.0166,	1.2124 s / batch. (data: 4.27e-04). ETA=6 days, 17:24:26, max mem: 15.0 GB 
[06/14 19:57:26][INFO] visual_prompt:  301: 	Training 1200/5004. train loss: 0.0226,	1.2026 s / batch. (data: 3.29e-04). ETA=6 days, 16:04:25, max mem: 15.0 GB 
[06/14 19:59:27][INFO] visual_prompt:  301: 	Training 1300/5004. train loss: 0.0422,	1.2105 s / batch. (data: 2.75e-04). ETA=6 days, 17:05:27, max mem: 15.0 GB 
[06/14 20:01:28][INFO] visual_prompt:  301: 	Training 1400/5004. train loss: 0.0448,	1.2103 s / batch. (data: 3.58e-04). ETA=6 days, 17:01:44, max mem: 15.0 GB 
[06/14 20:03:29][INFO] visual_prompt:  301: 	Training 1500/5004. train loss: 0.0436,	1.2052 s / batch. (data: 3.98e-04). ETA=6 days, 16:18:57, max mem: 15.0 GB 
[06/14 20:05:29][INFO] visual_prompt:  301: 	Training 1600/5004. train loss: 0.0403,	1.2072 s / batch. (data: 2.89e-04). ETA=6 days, 16:32:48, max mem: 15.0 GB 
[06/14 20:07:30][INFO] visual_prompt:  301: 	Training 1700/5004. train loss: 0.0262,	1.2096 s / batch. (data: 3.59e-04). ETA=6 days, 16:50:07, max mem: 15.0 GB 
[06/14 20:09:31][INFO] visual_prompt:  301: 	Training 1800/5004. train loss: 0.0385,	1.2119 s / batch. (data: 3.27e-04). ETA=6 days, 17:06:34, max mem: 15.0 GB 
[06/14 20:11:31][INFO] visual_prompt:  301: 	Training 1900/5004. train loss: 0.0083,	1.2075 s / batch. (data: 2.91e-04). ETA=6 days, 16:29:25, max mem: 15.0 GB 
[06/14 20:13:32][INFO] visual_prompt:  301: 	Training 2000/5004. train loss: 0.0243,	1.2061 s / batch. (data: 3.26e-04). ETA=6 days, 16:16:19, max mem: 15.0 GB 
[06/14 20:15:33][INFO] visual_prompt:  301: 	Training 2100/5004. train loss: 0.0170,	1.2100 s / batch. (data: 2.76e-04). ETA=6 days, 16:45:36, max mem: 15.0 GB 
[06/14 20:17:34][INFO] visual_prompt:  301: 	Training 2200/5004. train loss: 0.0084,	1.2057 s / batch. (data: 3.12e-04). ETA=6 days, 16:09:23, max mem: 15.0 GB 
[06/14 20:19:34][INFO] visual_prompt:  301: 	Training 2300/5004. train loss: 0.0085,	1.2041 s / batch. (data: 3.22e-04). ETA=6 days, 15:54:29, max mem: 15.0 GB 
[06/14 20:21:35][INFO] visual_prompt:  301: 	Training 2400/5004. train loss: 0.0413,	1.2103 s / batch. (data: 3.43e-04). ETA=6 days, 16:42:02, max mem: 15.0 GB 
[06/14 20:23:36][INFO] visual_prompt:  301: 	Training 2500/5004. train loss: 0.0403,	1.2131 s / batch. (data: 3.29e-04). ETA=6 days, 17:01:39, max mem: 15.0 GB 
[06/14 20:25:37][INFO] visual_prompt:  301: 	Training 2600/5004. train loss: 0.0370,	1.2137 s / batch. (data: 4.04e-04). ETA=6 days, 17:05:09, max mem: 15.0 GB 
[06/14 20:27:38][INFO] visual_prompt:  301: 	Training 2700/5004. train loss: 0.0339,	1.2119 s / batch. (data: 3.51e-04). ETA=6 days, 16:48:07, max mem: 15.0 GB 
[06/14 20:29:39][INFO] visual_prompt:  301: 	Training 2800/5004. train loss: 0.0173,	1.2083 s / batch. (data: 3.98e-04). ETA=6 days, 16:18:08, max mem: 15.0 GB 
[06/14 20:31:40][INFO] visual_prompt:  301: 	Training 2900/5004. train loss: 0.0101,	1.2105 s / batch. (data: 3.42e-04). ETA=6 days, 16:33:32, max mem: 15.0 GB 
[06/14 20:33:41][INFO] visual_prompt:  301: 	Training 3000/5004. train loss: 0.0128,	1.2086 s / batch. (data: 3.45e-04). ETA=6 days, 16:16:18, max mem: 15.0 GB 
[06/14 20:35:42][INFO] visual_prompt:  301: 	Training 3100/5004. train loss: 0.0192,	1.2086 s / batch. (data: 2.76e-04). ETA=6 days, 16:14:00, max mem: 15.0 GB 
[06/14 20:37:43][INFO] visual_prompt:  301: 	Training 3200/5004. train loss: 0.0386,	1.2072 s / batch. (data: 3.50e-04). ETA=6 days, 16:00:35, max mem: 15.0 GB 
[06/14 20:39:43][INFO] visual_prompt:  301: 	Training 3300/5004. train loss: 0.0165,	1.2074 s / batch. (data: 3.11e-04). ETA=6 days, 16:00:10, max mem: 15.0 GB 
[06/14 20:41:44][INFO] visual_prompt:  301: 	Training 3400/5004. train loss: 0.0122,	1.2082 s / batch. (data: 3.08e-04). ETA=6 days, 16:04:49, max mem: 15.0 GB 
[06/14 20:43:45][INFO] visual_prompt:  301: 	Training 3500/5004. train loss: 0.0437,	1.2078 s / batch. (data: 2.91e-04). ETA=6 days, 15:59:50, max mem: 15.0 GB 
[06/14 20:45:46][INFO] visual_prompt:  301: 	Training 3600/5004. train loss: 0.0328,	1.2084 s / batch. (data: 3.23e-04). ETA=6 days, 16:02:14, max mem: 15.0 GB 
[06/14 20:47:46][INFO] visual_prompt:  301: 	Training 3700/5004. train loss: 0.0118,	1.2075 s / batch. (data: 2.77e-04). ETA=6 days, 15:53:26, max mem: 15.0 GB 
[06/14 20:49:47][INFO] visual_prompt:  301: 	Training 3800/5004. train loss: 0.0227,	1.2113 s / batch. (data: 3.40e-04). ETA=6 days, 16:21:08, max mem: 15.0 GB 
[06/14 20:51:48][INFO] visual_prompt:  301: 	Training 3900/5004. train loss: 0.0253,	1.2078 s / batch. (data: 2.90e-04). ETA=6 days, 15:51:34, max mem: 15.0 GB 
[06/14 20:53:49][INFO] visual_prompt:  301: 	Training 4000/5004. train loss: 0.0450,	1.2074 s / batch. (data: 2.83e-04). ETA=6 days, 15:46:42, max mem: 15.0 GB 
[06/14 20:55:49][INFO] visual_prompt:  301: 	Training 4100/5004. train loss: 0.0143,	1.2093 s / batch. (data: 2.74e-04). ETA=6 days, 15:59:34, max mem: 15.0 GB 
[06/14 20:57:50][INFO] visual_prompt:  301: 	Training 4200/5004. train loss: 0.0204,	1.2125 s / batch. (data: 3.08e-04). ETA=6 days, 16:22:58, max mem: 15.0 GB 
[06/14 20:59:51][INFO] visual_prompt:  301: 	Training 4300/5004. train loss: 0.0426,	1.2135 s / batch. (data: 3.22e-04). ETA=6 days, 16:28:47, max mem: 15.0 GB 
[06/14 21:01:52][INFO] visual_prompt:  301: 	Training 4400/5004. train loss: 0.0156,	1.2103 s / batch. (data: 3.05e-04). ETA=6 days, 16:01:36, max mem: 15.0 GB 
[06/14 21:03:53][INFO] visual_prompt:  301: 	Training 4500/5004. train loss: 0.0121,	1.2064 s / batch. (data: 3.50e-04). ETA=6 days, 15:28:16, max mem: 15.0 GB 
[06/14 21:05:54][INFO] visual_prompt:  301: 	Training 4600/5004. train loss: 0.0390,	1.2097 s / batch. (data: 3.73e-04). ETA=6 days, 15:52:54, max mem: 15.0 GB 
[06/14 21:07:55][INFO] visual_prompt:  301: 	Training 4700/5004. train loss: 0.0152,	1.2107 s / batch. (data: 2.84e-04). ETA=6 days, 15:58:26, max mem: 15.0 GB 
[06/14 21:09:56][INFO] visual_prompt:  301: 	Training 4800/5004. train loss: 0.0152,	1.2175 s / batch. (data: 3.78e-04). ETA=6 days, 16:50:26, max mem: 15.0 GB 
[06/14 21:11:57][INFO] visual_prompt:  301: 	Training 4900/5004. train loss: 0.0081,	1.2087 s / batch. (data: 3.37e-04). ETA=6 days, 15:38:19, max mem: 15.0 GB 
[06/14 21:13:58][INFO] visual_prompt:  301: 	Training 5000/5004. train loss: 0.0105,	1.2011 s / batch. (data: 1.11e-04). ETA=6 days, 14:36:16, max mem: 15.0 GB 
[06/14 21:14:04][INFO] visual_prompt:  314: Epoch 5 / 100: avg data time: 4.09e-03, avg batch time: 1.2174, average train loss: 0.0246
[06/14 21:22:59][INFO] visual_prompt:  434: 	Test 100/196. loss: 0.020, 5.1845 s / batch. (data: 1.86e-04)max mem: 14.99516 GB 
[06/14 21:31:15][INFO] visual_prompt:  471: Inference (val):avg data time: 1.47e-04, avg batch time: 5.1700, average loss: 0.0200
[06/14 21:31:15][INFO] visual_prompt:  488: Saved invariances for val_imagenet at output_shallow_10/val_imagenet_invariances.json
[06/14 21:31:15][INFO] visual_prompt:  252: Training 6 / 100 epoch, with learning rate 0.05
[06/14 21:34:02][INFO] visual_prompt:  301: 	Training 100/5004. train loss: 0.0410,	1.2092 s / batch. (data: 3.44e-04). ETA=6 days, 15:38:49, max mem: 15.0 GB 
[06/14 21:36:03][INFO] visual_prompt:  301: 	Training 200/5004. train loss: 0.0426,	1.2064 s / batch. (data: 4.03e-04). ETA=6 days, 15:14:27, max mem: 15.0 GB 
[06/14 21:38:04][INFO] visual_prompt:  301: 	Training 300/5004. train loss: 0.0149,	1.2078 s / batch. (data: 3.17e-04). ETA=6 days, 15:23:22, max mem: 15.0 GB 
[06/14 21:40:04][INFO] visual_prompt:  301: 	Training 400/5004. train loss: 0.0398,	1.2064 s / batch. (data: 3.15e-04). ETA=6 days, 15:09:58, max mem: 15.0 GB 
[06/14 21:42:05][INFO] visual_prompt:  301: 	Training 500/5004. train loss: 0.0093,	1.2051 s / batch. (data: 3.39e-04). ETA=6 days, 14:58:11, max mem: 15.0 GB 
[06/14 21:44:06][INFO] visual_prompt:  301: 	Training 600/5004. train loss: 0.0422,	1.2099 s / batch. (data: 3.10e-04). ETA=6 days, 15:33:38, max mem: 15.0 GB 
[06/14 21:46:07][INFO] visual_prompt:  301: 	Training 700/5004. train loss: 0.0096,	1.2097 s / batch. (data: 3.35e-04). ETA=6 days, 15:30:02, max mem: 15.0 GB 
[06/14 21:48:08][INFO] visual_prompt:  301: 	Training 800/5004. train loss: 0.0083,	1.2119 s / batch. (data: 3.42e-04). ETA=6 days, 15:45:43, max mem: 15.0 GB 
[06/14 21:50:09][INFO] visual_prompt:  301: 	Training 900/5004. train loss: 0.0109,	1.2079 s / batch. (data: 3.28e-04). ETA=6 days, 15:11:58, max mem: 15.0 GB 
[06/14 21:52:10][INFO] visual_prompt:  301: 	Training 1000/5004. train loss: 0.0213,	1.2047 s / batch. (data: 3.28e-04). ETA=6 days, 14:44:21, max mem: 15.0 GB 
[06/14 21:54:10][INFO] visual_prompt:  301: 	Training 1100/5004. train loss: 0.0416,	1.2068 s / batch. (data: 3.39e-04). ETA=6 days, 14:59:07, max mem: 15.0 GB 
[06/14 21:56:11][INFO] visual_prompt:  301: 	Training 1200/5004. train loss: 0.0086,	1.2083 s / batch. (data: 3.69e-04). ETA=6 days, 15:08:50, max mem: 15.0 GB 
[06/14 21:58:12][INFO] visual_prompt:  301: 	Training 1300/5004. train loss: 0.0472,	1.2032 s / batch. (data: 3.02e-04). ETA=6 days, 14:26:33, max mem: 15.0 GB 
[06/14 22:00:12][INFO] visual_prompt:  301: 	Training 1400/5004. train loss: 0.0082,	1.2022 s / batch. (data: 2.86e-04). ETA=6 days, 14:17:11, max mem: 15.0 GB 
[06/14 22:02:13][INFO] visual_prompt:  301: 	Training 1500/5004. train loss: 0.0246,	1.2017 s / batch. (data: 3.47e-04). ETA=6 days, 14:10:57, max mem: 15.0 GB 
[06/14 22:04:14][INFO] visual_prompt:  301: 	Training 1600/5004. train loss: 0.0084,	1.2056 s / batch. (data: 3.96e-04). ETA=6 days, 14:39:26, max mem: 15.0 GB 
[06/14 22:06:14][INFO] visual_prompt:  301: 	Training 1700/5004. train loss: 0.0118,	1.2072 s / batch. (data: 3.41e-04). ETA=6 days, 14:50:05, max mem: 15.0 GB 
[06/14 22:08:15][INFO] visual_prompt:  301: 	Training 1800/5004. train loss: 0.0202,	1.2057 s / batch. (data: 3.41e-04). ETA=6 days, 14:36:20, max mem: 15.0 GB 
[06/14 22:10:16][INFO] visual_prompt:  301: 	Training 1900/5004. train loss: 0.0089,	1.2101 s / batch. (data: 3.40e-04). ETA=6 days, 15:09:11, max mem: 15.0 GB 
[06/14 22:12:17][INFO] visual_prompt:  301: 	Training 2000/5004. train loss: 0.0159,	1.2064 s / batch. (data: 3.20e-04). ETA=6 days, 14:37:54, max mem: 15.0 GB 
[06/14 22:14:18][INFO] visual_prompt:  301: 	Training 2100/5004. train loss: 0.0155,	1.2122 s / batch. (data: 3.30e-04). ETA=6 days, 15:21:30, max mem: 15.0 GB 
[06/14 22:16:19][INFO] visual_prompt:  301: 	Training 2200/5004. train loss: 0.0223,	1.2083 s / batch. (data: 3.52e-04). ETA=6 days, 14:49:17, max mem: 15.0 GB 
[06/14 22:18:20][INFO] visual_prompt:  301: 	Training 2300/5004. train loss: 0.0413,	1.2118 s / batch. (data: 3.37e-04). ETA=6 days, 15:14:20, max mem: 15.0 GB 
[06/14 22:20:21][INFO] visual_prompt:  301: 	Training 2400/5004. train loss: 0.0067,	1.2090 s / batch. (data: 3.76e-04). ETA=6 days, 14:50:37, max mem: 15.0 GB 
[06/14 22:22:21][INFO] visual_prompt:  301: 	Training 2500/5004. train loss: 0.0766,	1.2083 s / batch. (data: 3.51e-04). ETA=6 days, 14:42:39, max mem: 15.0 GB 
[06/14 22:24:22][INFO] visual_prompt:  301: 	Training 2600/5004. train loss: 0.0233,	1.2098 s / batch. (data: 3.62e-04). ETA=6 days, 14:52:49, max mem: 15.0 GB 
[06/14 22:26:23][INFO] visual_prompt:  301: 	Training 2700/5004. train loss: 0.0075,	1.2054 s / batch. (data: 3.09e-04). ETA=6 days, 14:16:29, max mem: 15.0 GB 
[06/14 22:28:24][INFO] visual_prompt:  301: 	Training 2800/5004. train loss: 0.0093,	1.2026 s / batch. (data: 3.16e-04). ETA=6 days, 13:52:03, max mem: 15.0 GB 
[06/14 22:30:24][INFO] visual_prompt:  301: 	Training 2900/5004. train loss: 0.0062,	1.2096 s / batch. (data: 3.43e-04). ETA=6 days, 14:45:08, max mem: 15.0 GB 
[06/14 22:32:25][INFO] visual_prompt:  301: 	Training 3000/5004. train loss: 0.0168,	1.2040 s / batch. (data: 3.44e-04). ETA=6 days, 13:58:52, max mem: 15.0 GB 
[06/14 22:34:26][INFO] visual_prompt:  301: 	Training 3100/5004. train loss: 0.0087,	1.2083 s / batch. (data: 3.56e-04). ETA=6 days, 14:30:40, max mem: 15.0 GB 
[06/14 22:36:26][INFO] visual_prompt:  301: 	Training 3200/5004. train loss: 0.0469,	1.2069 s / batch. (data: 3.77e-04). ETA=6 days, 14:17:34, max mem: 15.0 GB 
[06/14 22:38:27][INFO] visual_prompt:  301: 	Training 3300/5004. train loss: 0.0082,	1.2089 s / batch. (data: 3.60e-04). ETA=6 days, 14:31:14, max mem: 15.0 GB 
[06/14 22:40:28][INFO] visual_prompt:  301: 	Training 3400/5004. train loss: 0.0082,	1.2058 s / batch. (data: 3.56e-04). ETA=6 days, 14:05:29, max mem: 15.0 GB 
[06/14 22:42:28][INFO] visual_prompt:  301: 	Training 3500/5004. train loss: 0.0074,	1.2088 s / batch. (data: 3.68e-04). ETA=6 days, 14:26:57, max mem: 15.0 GB 
[06/14 22:44:29][INFO] visual_prompt:  301: 	Training 3600/5004. train loss: 0.0364,	1.2093 s / batch. (data: 3.16e-04). ETA=6 days, 14:29:03, max mem: 15.0 GB 
[06/14 22:46:30][INFO] visual_prompt:  301: 	Training 3700/5004. train loss: 0.0207,	1.2130 s / batch. (data: 3.02e-04). ETA=6 days, 14:55:42, max mem: 15.0 GB 
[06/14 22:48:31][INFO] visual_prompt:  301: 	Training 3800/5004. train loss: 0.0087,	1.2068 s / batch. (data: 3.63e-04). ETA=6 days, 14:05:12, max mem: 15.0 GB 
[06/14 22:50:32][INFO] visual_prompt:  301: 	Training 3900/5004. train loss: 0.0122,	1.2111 s / batch. (data: 3.25e-04). ETA=6 days, 14:37:06, max mem: 15.0 GB 
[06/14 22:52:33][INFO] visual_prompt:  301: 	Training 4000/5004. train loss: 0.0486,	1.2094 s / batch. (data: 2.85e-04). ETA=6 days, 14:21:39, max mem: 15.0 GB 
[06/14 22:54:34][INFO] visual_prompt:  301: 	Training 4100/5004. train loss: 0.0447,	1.2134 s / batch. (data: 3.64e-04). ETA=6 days, 14:51:07, max mem: 15.0 GB 
[06/14 22:56:35][INFO] visual_prompt:  301: 	Training 4200/5004. train loss: 0.0434,	1.2127 s / batch. (data: 3.49e-04). ETA=6 days, 14:43:01, max mem: 15.0 GB 
[06/14 22:58:36][INFO] visual_prompt:  301: 	Training 4300/5004. train loss: 0.0120,	1.2041 s / batch. (data: 3.01e-04). ETA=6 days, 13:33:30, max mem: 15.0 GB 
[06/14 23:00:36][INFO] visual_prompt:  301: 	Training 4400/5004. train loss: 0.0070,	1.2132 s / batch. (data: 3.24e-04). ETA=6 days, 14:43:25, max mem: 15.0 GB 
[06/14 23:02:37][INFO] visual_prompt:  301: 	Training 4500/5004. train loss: 0.0150,	1.2053 s / batch. (data: 3.24e-04). ETA=6 days, 13:39:04, max mem: 15.0 GB 
[06/14 23:04:37][INFO] visual_prompt:  301: 	Training 4600/5004. train loss: 0.0051,	1.2078 s / batch. (data: 2.88e-04). ETA=6 days, 13:56:37, max mem: 15.0 GB 
[06/14 23:06:38][INFO] visual_prompt:  301: 	Training 4700/5004. train loss: 0.0052,	1.2068 s / batch. (data: 3.23e-04). ETA=6 days, 13:47:05, max mem: 15.0 GB 
[06/14 23:08:39][INFO] visual_prompt:  301: 	Training 4800/5004. train loss: 0.0007,	1.2019 s / batch. (data: 3.15e-04). ETA=6 days, 13:06:07, max mem: 15.0 GB 
[06/14 23:10:39][INFO] visual_prompt:  301: 	Training 4900/5004. train loss: 0.0301,	1.2055 s / batch. (data: 3.24e-04). ETA=6 days, 13:32:58, max mem: 15.0 GB 
[06/14 23:12:40][INFO] visual_prompt:  301: 	Training 5000/5004. train loss: 0.0054,	1.2045 s / batch. (data: 1.10e-04). ETA=6 days, 13:22:52, max mem: 15.0 GB 
[06/14 23:12:45][INFO] visual_prompt:  314: Epoch 6 / 100: avg data time: 4.22e-03, avg batch time: 1.2168, average train loss: 0.0184
[06/14 23:21:41][INFO] visual_prompt:  434: 	Test 100/196. loss: 0.015, 5.2087 s / batch. (data: 6.77e-05)max mem: 14.99516 GB 
[06/14 23:29:59][INFO] visual_prompt:  471: Inference (val):avg data time: 1.58e-04, avg batch time: 5.1839, average loss: 0.0154
[06/14 23:29:59][INFO] visual_prompt:  488: Saved invariances for val_imagenet at output_shallow_10/val_imagenet_invariances.json
[06/14 23:29:59][INFO] visual_prompt:  252: Training 7 / 100 epoch, with learning rate 0.06
[06/14 23:32:45][INFO] visual_prompt:  301: 	Training 100/5004. train loss: 0.0072,	1.2154 s / batch. (data: 4.28e-04). ETA=6 days, 14:46:33, max mem: 15.0 GB 
[06/14 23:34:46][INFO] visual_prompt:  301: 	Training 200/5004. train loss: 0.0006,	1.2057 s / batch. (data: 2.71e-04). ETA=6 days, 13:27:48, max mem: 15.0 GB 
[06/14 23:36:47][INFO] visual_prompt:  301: 	Training 300/5004. train loss: 0.0006,	1.2108 s / batch. (data: 3.09e-04). ETA=6 days, 14:05:55, max mem: 15.0 GB 
[06/14 23:38:48][INFO] visual_prompt:  301: 	Training 400/5004. train loss: 0.0154,	1.2114 s / batch. (data: 3.01e-04). ETA=6 days, 14:09:02, max mem: 15.0 GB 
[06/14 23:40:48][INFO] visual_prompt:  301: 	Training 500/5004. train loss: 0.0062,	1.2021 s / batch. (data: 3.26e-04). ETA=6 days, 12:54:09, max mem: 15.0 GB 
[06/14 23:42:49][INFO] visual_prompt:  301: 	Training 600/5004. train loss: 0.0063,	1.2072 s / batch. (data: 3.40e-04). ETA=6 days, 13:31:38, max mem: 15.0 GB 
[06/14 23:44:49][INFO] visual_prompt:  301: 	Training 700/5004. train loss: 0.0120,	1.2088 s / batch. (data: 2.90e-04). ETA=6 days, 13:42:10, max mem: 15.0 GB 
[06/14 23:46:50][INFO] visual_prompt:  301: 	Training 800/5004. train loss: 0.0211,	1.2101 s / batch. (data: 3.24e-04). ETA=6 days, 13:50:16, max mem: 15.0 GB 
[06/14 23:48:50][INFO] visual_prompt:  301: 	Training 900/5004. train loss: 0.0162,	1.2053 s / batch. (data: 3.23e-04). ETA=6 days, 13:11:14, max mem: 15.0 GB 
[06/14 23:50:51][INFO] visual_prompt:  301: 	Training 1000/5004. train loss: 0.0064,	1.2063 s / batch. (data: 3.45e-04). ETA=6 days, 13:17:09, max mem: 15.0 GB 
[06/14 23:52:52][INFO] visual_prompt:  301: 	Training 1100/5004. train loss: 0.0410,	1.2095 s / batch. (data: 3.22e-04). ETA=6 days, 13:39:40, max mem: 15.0 GB 
[06/14 23:54:53][INFO] visual_prompt:  301: 	Training 1200/5004. train loss: 0.0061,	1.2082 s / batch. (data: 3.25e-04). ETA=6 days, 13:27:37, max mem: 15.0 GB 
[06/14 23:56:53][INFO] visual_prompt:  301: 	Training 1300/5004. train loss: 0.0027,	1.2125 s / batch. (data: 3.14e-04). ETA=6 days, 13:59:19, max mem: 15.0 GB 
[06/14 23:58:54][INFO] visual_prompt:  301: 	Training 1400/5004. train loss: 0.0223,	1.2098 s / batch. (data: 3.19e-04). ETA=6 days, 13:36:08, max mem: 15.0 GB 
[06/15 00:00:55][INFO] visual_prompt:  301: 	Training 1500/5004. train loss: 0.0351,	1.2064 s / batch. (data: 3.71e-04). ETA=6 days, 13:07:35, max mem: 15.0 GB 
[06/15 00:02:56][INFO] visual_prompt:  301: 	Training 1600/5004. train loss: 0.0358,	1.2082 s / batch. (data: 3.36e-04). ETA=6 days, 13:19:27, max mem: 15.0 GB 
[06/15 00:04:57][INFO] visual_prompt:  301: 	Training 1700/5004. train loss: 0.0023,	1.2057 s / batch. (data: 2.87e-04). ETA=6 days, 12:58:05, max mem: 15.0 GB 
[06/15 00:06:58][INFO] visual_prompt:  301: 	Training 1800/5004. train loss: 0.0210,	1.2120 s / batch. (data: 3.77e-04). ETA=6 days, 13:45:06, max mem: 15.0 GB 
[06/15 00:08:59][INFO] visual_prompt:  301: 	Training 1900/5004. train loss: 0.0157,	1.2117 s / batch. (data: 3.06e-04). ETA=6 days, 13:41:03, max mem: 15.0 GB 
[06/15 00:11:00][INFO] visual_prompt:  301: 	Training 2000/5004. train loss: 0.0053,	1.2053 s / batch. (data: 3.38e-04). ETA=6 days, 12:48:59, max mem: 15.0 GB 
[06/15 00:13:01][INFO] visual_prompt:  301: 	Training 2100/5004. train loss: 0.0065,	1.2063 s / batch. (data: 2.82e-04). ETA=6 days, 12:54:37, max mem: 15.0 GB 
[06/15 00:15:01][INFO] visual_prompt:  301: 	Training 2200/5004. train loss: 0.0056,	1.2083 s / batch. (data: 3.26e-04). ETA=6 days, 13:08:23, max mem: 15.0 GB 
[06/15 00:17:02][INFO] visual_prompt:  301: 	Training 2300/5004. train loss: 0.0019,	1.2070 s / batch. (data: 2.95e-04). ETA=6 days, 12:56:19, max mem: 15.0 GB 
[06/15 00:19:03][INFO] visual_prompt:  301: 	Training 2400/5004. train loss: 0.0442,	1.2071 s / batch. (data: 3.41e-04). ETA=6 days, 12:54:52, max mem: 15.0 GB 
[06/15 00:21:03][INFO] visual_prompt:  301: 	Training 2500/5004. train loss: 0.0275,	1.2045 s / batch. (data: 3.07e-04). ETA=6 days, 12:32:19, max mem: 15.0 GB 
[06/15 00:23:04][INFO] visual_prompt:  301: 	Training 2600/5004. train loss: 0.0342,	1.2078 s / batch. (data: 2.88e-04). ETA=6 days, 12:56:00, max mem: 15.0 GB 
[06/15 00:25:04][INFO] visual_prompt:  301: 	Training 2700/5004. train loss: 0.0351,	1.2060 s / batch. (data: 3.47e-04). ETA=6 days, 12:39:54, max mem: 15.0 GB 
[06/15 00:27:05][INFO] visual_prompt:  301: 	Training 2800/5004. train loss: 0.0389,	1.2098 s / batch. (data: 4.12e-04). ETA=6 days, 13:07:53, max mem: 15.0 GB 
[06/15 00:29:06][INFO] visual_prompt:  301: 	Training 2900/5004. train loss: 0.0158,	1.2092 s / batch. (data: 3.84e-04). ETA=6 days, 13:01:23, max mem: 15.0 GB 
[06/15 00:31:06][INFO] visual_prompt:  301: 	Training 3000/5004. train loss: 0.0051,	1.2029 s / batch. (data: 3.31e-04). ETA=6 days, 12:10:21, max mem: 15.0 GB 
[06/15 00:33:07][INFO] visual_prompt:  301: 	Training 3100/5004. train loss: 0.0049,	1.2129 s / batch. (data: 3.57e-04). ETA=6 days, 13:26:00, max mem: 15.0 GB 
[06/15 00:35:08][INFO] visual_prompt:  301: 	Training 3200/5004. train loss: 0.0259,	1.2081 s / batch. (data: 3.14e-04). ETA=6 days, 12:46:44, max mem: 15.0 GB 
[06/15 00:37:09][INFO] visual_prompt:  301: 	Training 3300/5004. train loss: 0.0065,	1.2055 s / batch. (data: 3.35e-04). ETA=6 days, 12:24:31, max mem: 15.0 GB 
[06/15 00:39:10][INFO] visual_prompt:  301: 	Training 3400/5004. train loss: 0.0190,	1.2082 s / batch. (data: 3.05e-04). ETA=6 days, 12:43:04, max mem: 15.0 GB 
[06/15 00:41:11][INFO] visual_prompt:  301: 	Training 3500/5004. train loss: 0.0430,	1.2103 s / batch. (data: 4.07e-04). ETA=6 days, 12:57:51, max mem: 15.0 GB 
[06/15 00:43:12][INFO] visual_prompt:  301: 	Training 3600/5004. train loss: 0.0060,	1.2114 s / batch. (data: 3.45e-04). ETA=6 days, 13:04:00, max mem: 15.0 GB 
[06/15 00:45:13][INFO] visual_prompt:  301: 	Training 3700/5004. train loss: 0.0186,	1.2080 s / batch. (data: 3.85e-04). ETA=6 days, 12:35:39, max mem: 15.0 GB 
[06/15 00:47:13][INFO] visual_prompt:  301: 	Training 3800/5004. train loss: 0.0063,	1.2083 s / batch. (data: 3.30e-04). ETA=6 days, 12:35:42, max mem: 15.0 GB 
[06/15 00:49:14][INFO] visual_prompt:  301: 	Training 3900/5004. train loss: 0.0452,	1.2121 s / batch. (data: 2.99e-04). ETA=6 days, 13:03:28, max mem: 15.0 GB 
[06/15 00:51:14][INFO] visual_prompt:  301: 	Training 4000/5004. train loss: 0.0254,	1.2051 s / batch. (data: 2.71e-04). ETA=6 days, 12:07:07, max mem: 15.0 GB 
[06/15 00:53:15][INFO] visual_prompt:  301: 	Training 4100/5004. train loss: 0.0185,	1.2040 s / batch. (data: 3.28e-04). ETA=6 days, 11:56:14, max mem: 15.0 GB 
[06/15 00:55:15][INFO] visual_prompt:  301: 	Training 4200/5004. train loss: 0.0457,	1.2053 s / batch. (data: 3.31e-04). ETA=6 days, 12:04:29, max mem: 15.0 GB 
[06/15 00:57:16][INFO] visual_prompt:  301: 	Training 4300/5004. train loss: 0.0144,	1.2029 s / batch. (data: 2.90e-04). ETA=6 days, 11:44:09, max mem: 15.0 GB 
[06/15 00:59:16][INFO] visual_prompt:  301: 	Training 4400/5004. train loss: 0.0366,	1.2105 s / batch. (data: 3.21e-04). ETA=6 days, 12:40:51, max mem: 15.0 GB 
[06/15 01:01:17][INFO] visual_prompt:  301: 	Training 4500/5004. train loss: 0.0229,	1.2065 s / batch. (data: 2.74e-04). ETA=6 days, 12:08:21, max mem: 15.0 GB 
[06/15 01:03:18][INFO] visual_prompt:  301: 	Training 4600/5004. train loss: 0.0384,	1.2077 s / batch. (data: 3.20e-04). ETA=6 days, 12:15:00, max mem: 15.0 GB 
[06/15 01:05:18][INFO] visual_prompt:  301: 	Training 4700/5004. train loss: 0.0385,	1.2129 s / batch. (data: 3.81e-04). ETA=6 days, 12:53:20, max mem: 15.0 GB 
[06/15 01:07:19][INFO] visual_prompt:  301: 	Training 4800/5004. train loss: 0.0177,	1.2076 s / batch. (data: 3.04e-04). ETA=6 days, 12:10:08, max mem: 15.0 GB 
[06/15 01:09:20][INFO] visual_prompt:  301: 	Training 4900/5004. train loss: 0.0310,	1.2182 s / batch. (data: 3.26e-04). ETA=6 days, 13:30:52, max mem: 15.0 GB 
[06/15 01:11:22][INFO] visual_prompt:  301: 	Training 5000/5004. train loss: 0.0059,	1.2160 s / batch. (data: 1.17e-04). ETA=6 days, 13:11:53, max mem: 15.0 GB 
[06/15 01:11:28][INFO] visual_prompt:  314: Epoch 7 / 100: avg data time: 4.18e-03, avg batch time: 1.2166, average train loss: 0.0203
[06/15 01:20:23][INFO] visual_prompt:  434: 	Test 100/196. loss: 0.021, 5.2016 s / batch. (data: 1.48e-04)max mem: 14.99516 GB 
[06/15 01:28:42][INFO] visual_prompt:  471: Inference (val):avg data time: 1.58e-04, avg batch time: 5.1836, average loss: 0.0217
[06/15 01:28:42][INFO] visual_prompt:  488: Saved invariances for val_imagenet at output_shallow_10/val_imagenet_invariances.json
[06/15 01:28:42][INFO] visual_prompt:  252: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[06/15 01:31:30][INFO] visual_prompt:  301: 	Training 100/5004. train loss: 0.0546,	1.2118 s / batch. (data: 3.21e-04). ETA=6 days, 12:36:35, max mem: 15.0 GB 
[06/15 01:33:31][INFO] visual_prompt:  301: 	Training 200/5004. train loss: 0.0252,	1.2052 s / batch. (data: 3.41e-04). ETA=6 days, 11:43:37, max mem: 15.0 GB 
[06/15 01:35:32][INFO] visual_prompt:  301: 	Training 300/5004. train loss: 0.0238,	1.2104 s / batch. (data: 2.86e-04). ETA=6 days, 12:22:21, max mem: 15.0 GB 
[06/15 01:37:33][INFO] visual_prompt:  301: 	Training 400/5004. train loss: 0.0421,	1.2096 s / batch. (data: 3.00e-04). ETA=6 days, 12:13:56, max mem: 15.0 GB 
[06/15 01:39:34][INFO] visual_prompt:  301: 	Training 500/5004. train loss: 0.0238,	1.2094 s / batch. (data: 3.10e-04). ETA=6 days, 12:10:16, max mem: 15.0 GB 
[06/15 01:41:35][INFO] visual_prompt:  301: 	Training 600/5004. train loss: 0.0471,	1.2095 s / batch. (data: 3.44e-04). ETA=6 days, 12:09:14, max mem: 15.0 GB 
[06/15 01:43:35][INFO] visual_prompt:  301: 	Training 700/5004. train loss: 0.0019,	1.2063 s / batch. (data: 2.88e-04). ETA=6 days, 11:42:08, max mem: 15.0 GB 
[06/15 01:45:36][INFO] visual_prompt:  301: 	Training 800/5004. train loss: 0.0053,	1.2087 s / batch. (data: 3.08e-04). ETA=6 days, 11:58:39, max mem: 15.0 GB 
[06/15 01:47:37][INFO] visual_prompt:  301: 	Training 900/5004. train loss: 0.0057,	1.2094 s / batch. (data: 4.19e-04). ETA=6 days, 12:01:51, max mem: 15.0 GB 
[06/15 01:49:38][INFO] visual_prompt:  301: 	Training 1000/5004. train loss: 0.0346,	1.2070 s / batch. (data: 2.85e-04). ETA=6 days, 11:41:49, max mem: 15.0 GB 
[06/15 01:51:38][INFO] visual_prompt:  301: 	Training 1100/5004. train loss: 0.0187,	1.2107 s / batch. (data: 3.46e-04). ETA=6 days, 12:08:22, max mem: 15.0 GB 
[06/15 01:53:39][INFO] visual_prompt:  301: 	Training 1200/5004. train loss: 0.0629,	1.2075 s / batch. (data: 4.32e-04). ETA=6 days, 11:41:43, max mem: 15.0 GB 
[06/15 01:55:40][INFO] visual_prompt:  301: 	Training 1300/5004. train loss: 0.0219,	1.2059 s / batch. (data: 3.55e-04). ETA=6 days, 11:27:10, max mem: 15.0 GB 
[06/15 01:57:41][INFO] visual_prompt:  301: 	Training 1400/5004. train loss: 0.0012,	1.2102 s / batch. (data: 2.90e-04). ETA=6 days, 11:58:36, max mem: 15.0 GB 
[06/15 01:59:42][INFO] visual_prompt:  301: 	Training 1500/5004. train loss: 0.0012,	1.2059 s / batch. (data: 3.27e-04). ETA=6 days, 11:22:42, max mem: 15.0 GB 
[06/15 02:01:42][INFO] visual_prompt:  301: 	Training 1600/5004. train loss: 0.0643,	1.2081 s / batch. (data: 3.06e-04). ETA=6 days, 11:38:05, max mem: 15.0 GB 
[06/15 02:03:43][INFO] visual_prompt:  301: 	Training 1700/5004. train loss: 0.0053,	1.2098 s / batch. (data: 2.98e-04). ETA=6 days, 11:49:00, max mem: 15.0 GB 
[06/15 02:05:44][INFO] visual_prompt:  301: 	Training 1800/5004. train loss: 0.0012,	1.2153 s / batch. (data: 3.08e-04). ETA=6 days, 12:29:56, max mem: 15.0 GB 
[06/15 02:07:45][INFO] visual_prompt:  301: 	Training 1900/5004. train loss: 0.0156,	1.2115 s / batch. (data: 3.12e-04). ETA=6 days, 11:58:14, max mem: 15.0 GB 
[06/15 02:09:46][INFO] visual_prompt:  301: 	Training 2000/5004. train loss: 0.0054,	1.2040 s / batch. (data: 3.00e-04). ETA=6 days, 10:58:27, max mem: 15.0 GB 
[06/15 02:11:47][INFO] visual_prompt:  301: 	Training 2100/5004. train loss: 0.0153,	1.2096 s / batch. (data: 3.59e-04). ETA=6 days, 11:39:33, max mem: 15.0 GB 
[06/15 02:13:48][INFO] visual_prompt:  301: 	Training 2200/5004. train loss: 0.0050,	1.2085 s / batch. (data: 3.27e-04). ETA=6 days, 11:29:03, max mem: 15.0 GB 
[06/15 02:15:49][INFO] visual_prompt:  301: 	Training 2300/5004. train loss: 0.0047,	1.2080 s / batch. (data: 2.97e-04). ETA=6 days, 11:23:32, max mem: 15.0 GB 
[06/15 02:17:50][INFO] visual_prompt:  301: 	Training 2400/5004. train loss: 0.0093,	1.2070 s / batch. (data: 3.05e-04). ETA=6 days, 11:13:24, max mem: 15.0 GB 
[06/15 02:19:50][INFO] visual_prompt:  301: 	Training 2500/5004. train loss: 0.0012,	1.2074 s / batch. (data: 3.20e-04). ETA=6 days, 11:14:17, max mem: 15.0 GB 
[06/15 02:21:51][INFO] visual_prompt:  301: 	Training 2600/5004. train loss: 0.0252,	1.2086 s / batch. (data: 3.40e-04). ETA=6 days, 11:22:05, max mem: 15.0 GB 
[06/15 02:23:52][INFO] visual_prompt:  301: 	Training 2700/5004. train loss: 0.0086,	1.2035 s / batch. (data: 3.30e-04). ETA=6 days, 10:40:09, max mem: 15.0 GB 
[06/15 02:25:52][INFO] visual_prompt:  301: 	Training 2800/5004. train loss: 0.0292,	1.2041 s / batch. (data: 3.45e-04). ETA=6 days, 10:42:49, max mem: 15.0 GB 
[06/15 02:27:53][INFO] visual_prompt:  301: 	Training 2900/5004. train loss: 0.0387,	1.2051 s / batch. (data: 3.44e-04). ETA=6 days, 10:48:50, max mem: 15.0 GB 
[06/15 02:29:54][INFO] visual_prompt:  301: 	Training 3000/5004. train loss: 0.0051,	1.2038 s / batch. (data: 3.21e-04). ETA=6 days, 10:36:58, max mem: 15.0 GB 
[06/15 02:31:54][INFO] visual_prompt:  301: 	Training 3100/5004. train loss: 0.0268,	1.2071 s / batch. (data: 3.18e-04). ETA=6 days, 11:00:02, max mem: 15.0 GB 
[06/15 02:33:55][INFO] visual_prompt:  301: 	Training 3200/5004. train loss: 0.0080,	1.2086 s / batch. (data: 3.07e-04). ETA=6 days, 11:10:02, max mem: 15.0 GB 
[06/15 02:35:56][INFO] visual_prompt:  301: 	Training 3300/5004. train loss: 0.0154,	1.2107 s / batch. (data: 2.97e-04). ETA=6 days, 11:23:51, max mem: 15.0 GB 
[06/15 02:37:57][INFO] visual_prompt:  301: 	Training 3400/5004. train loss: 0.0157,	1.2090 s / batch. (data: 3.60e-04). ETA=6 days, 11:08:43, max mem: 15.0 GB 
[06/15 02:39:58][INFO] visual_prompt:  301: 	Training 3500/5004. train loss: 0.0047,	1.2050 s / batch. (data: 3.20e-04). ETA=6 days, 10:35:53, max mem: 15.0 GB 
[06/15 02:41:59][INFO] visual_prompt:  301: 	Training 3600/5004. train loss: 0.0046,	1.2088 s / batch. (data: 3.76e-04). ETA=6 days, 11:03:21, max mem: 15.0 GB 
[06/15 02:43:59][INFO] visual_prompt:  301: 	Training 3700/5004. train loss: 0.0166,	1.2076 s / batch. (data: 3.23e-04). ETA=6 days, 10:51:41, max mem: 15.0 GB 
[06/15 02:46:00][INFO] visual_prompt:  301: 	Training 3800/5004. train loss: 0.0014,	1.2038 s / batch. (data: 4.42e-04). ETA=6 days, 10:21:02, max mem: 15.0 GB 
[06/15 02:48:01][INFO] visual_prompt:  301: 	Training 3900/5004. train loss: 0.0013,	1.2090 s / batch. (data: 3.46e-04). ETA=6 days, 10:58:46, max mem: 15.0 GB 
[06/15 02:50:02][INFO] visual_prompt:  301: 	Training 4000/5004. train loss: 0.0111,	1.2114 s / batch. (data: 3.84e-04). ETA=6 days, 11:15:25, max mem: 15.0 GB 
[06/15 02:52:03][INFO] visual_prompt:  301: 	Training 4100/5004. train loss: 0.0145,	1.2030 s / batch. (data: 2.85e-04). ETA=6 days, 10:08:31, max mem: 15.0 GB 
[06/15 02:54:04][INFO] visual_prompt:  301: 	Training 4200/5004. train loss: 0.0098,	1.2104 s / batch. (data: 3.20e-04). ETA=6 days, 11:03:18, max mem: 15.0 GB 
[06/15 02:56:05][INFO] visual_prompt:  301: 	Training 4300/5004. train loss: 0.0099,	1.2062 s / batch. (data: 3.22e-04). ETA=6 days, 10:29:23, max mem: 15.0 GB 
[06/15 02:58:05][INFO] visual_prompt:  301: 	Training 4400/5004. train loss: 0.0153,	1.2073 s / batch. (data: 3.18e-04). ETA=6 days, 10:35:25, max mem: 15.0 GB 
[06/15 03:00:06][INFO] visual_prompt:  301: 	Training 4500/5004. train loss: 0.0397,	1.2035 s / batch. (data: 3.98e-04). ETA=6 days, 10:04:41, max mem: 15.0 GB 
[06/15 03:02:07][INFO] visual_prompt:  301: 	Training 4600/5004. train loss: 0.0360,	1.2061 s / batch. (data: 2.75e-04). ETA=6 days, 10:22:07, max mem: 15.0 GB 
[06/15 03:04:07][INFO] visual_prompt:  301: 	Training 4700/5004. train loss: 0.0087,	1.2097 s / batch. (data: 2.68e-04). ETA=6 days, 10:47:51, max mem: 15.0 GB 
[06/15 03:06:08][INFO] visual_prompt:  301: 	Training 4800/5004. train loss: 0.0375,	1.2115 s / batch. (data: 2.89e-04). ETA=6 days, 11:00:05, max mem: 15.0 GB 
[06/15 03:08:09][INFO] visual_prompt:  301: 	Training 4900/5004. train loss: 0.0400,	1.2059 s / batch. (data: 3.41e-04). ETA=6 days, 10:14:50, max mem: 15.0 GB 
[06/15 03:10:09][INFO] visual_prompt:  301: 	Training 5000/5004. train loss: 0.0281,	1.2034 s / batch. (data: 1.14e-04). ETA=6 days, 9:53:14, max mem: 15.0 GB 
[06/15 03:10:15][INFO] visual_prompt:  314: Epoch 8 / 100: avg data time: 4.25e-03, avg batch time: 1.2175, average train loss: 0.0209
[06/15 03:19:11][INFO] visual_prompt:  434: 	Test 100/196. loss: 0.019, 5.2061 s / batch. (data: 1.22e-04)max mem: 14.99516 GB 
[06/15 03:27:30][INFO] visual_prompt:  471: Inference (val):avg data time: 1.54e-04, avg batch time: 5.1854, average loss: 0.0190
[06/15 03:27:30][INFO] visual_prompt:  488: Saved invariances for val_imagenet at output_shallow_10/val_imagenet_invariances.json
[06/15 03:27:30][INFO] visual_prompt:  252: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[06/15 03:30:15][INFO] visual_prompt:  301: 	Training 100/5004. train loss: 0.0047,	1.2062 s / batch. (data: 3.69e-04). ETA=6 days, 10:12:35, max mem: 15.0 GB 
[06/15 03:32:16][INFO] visual_prompt:  301: 	Training 200/5004. train loss: 0.0189,	1.2070 s / batch. (data: 3.27e-04). ETA=6 days, 10:16:51, max mem: 15.0 GB 
[06/15 03:34:17][INFO] visual_prompt:  301: 	Training 300/5004. train loss: 0.0011,	1.2042 s / batch. (data: 3.33e-04). ETA=6 days, 9:53:52, max mem: 15.0 GB 
[06/15 03:36:17][INFO] visual_prompt:  301: 	Training 400/5004. train loss: 0.0195,	1.2092 s / batch. (data: 2.80e-04). ETA=6 days, 10:30:01, max mem: 15.0 GB 
[06/15 03:38:18][INFO] visual_prompt:  301: 	Training 500/5004. train loss: 0.0032,	1.2054 s / batch. (data: 3.65e-04). ETA=6 days, 9:59:05, max mem: 15.0 GB 
[06/15 03:40:19][INFO] visual_prompt:  301: 	Training 600/5004. train loss: 0.0358,	1.2055 s / batch. (data: 3.56e-04). ETA=6 days, 9:57:08, max mem: 15.0 GB 
[06/15 03:42:19][INFO] visual_prompt:  301: 	Training 700/5004. train loss: 0.0365,	1.2088 s / batch. (data: 3.24e-04). ETA=6 days, 10:21:06, max mem: 15.0 GB 
[06/15 03:44:20][INFO] visual_prompt:  301: 	Training 800/5004. train loss: 0.0014,	1.2077 s / batch. (data: 3.21e-04). ETA=6 days, 10:10:36, max mem: 15.0 GB 
[06/15 03:46:21][INFO] visual_prompt:  301: 	Training 900/5004. train loss: 0.0203,	1.2074 s / batch. (data: 3.68e-04). ETA=6 days, 10:06:11, max mem: 15.0 GB 
[06/15 03:48:21][INFO] visual_prompt:  301: 	Training 1000/5004. train loss: 0.0375,	1.2096 s / batch. (data: 3.40e-04). ETA=6 days, 10:20:37, max mem: 15.0 GB 
[06/15 03:50:22][INFO] visual_prompt:  301: 	Training 1100/5004. train loss: 0.0071,	1.2087 s / batch. (data: 3.27e-04). ETA=6 days, 10:11:51, max mem: 15.0 GB 
[06/15 03:52:23][INFO] visual_prompt:  301: 	Training 1200/5004. train loss: 0.0071,	1.2130 s / batch. (data: 3.06e-04). ETA=6 days, 10:42:28, max mem: 15.0 GB 
[06/15 03:54:24][INFO] visual_prompt:  301: 	Training 1300/5004. train loss: 0.0425,	1.2087 s / batch. (data: 2.92e-04). ETA=6 days, 10:07:40, max mem: 15.0 GB 
[06/15 03:56:25][INFO] visual_prompt:  301: 	Training 1400/5004. train loss: 0.0069,	1.2069 s / batch. (data: 2.73e-04). ETA=6 days, 9:52:23, max mem: 15.0 GB 
[06/15 03:58:26][INFO] visual_prompt:  301: 	Training 1500/5004. train loss: 0.0151,	1.2099 s / batch. (data: 3.24e-04). ETA=6 days, 10:13:06, max mem: 15.0 GB 
[06/15 04:00:26][INFO] visual_prompt:  301: 	Training 1600/5004. train loss: 0.0218,	1.2150 s / batch. (data: 3.41e-04). ETA=6 days, 10:49:50, max mem: 15.0 GB 
[06/15 04:02:27][INFO] visual_prompt:  301: 	Training 1700/5004. train loss: 0.0011,	1.2123 s / batch. (data: 3.97e-04). ETA=6 days, 10:27:22, max mem: 15.0 GB 
[06/15 04:04:28][INFO] visual_prompt:  301: 	Training 1800/5004. train loss: 0.0256,	1.2088 s / batch. (data: 3.01e-04). ETA=6 days, 9:58:47, max mem: 15.0 GB 
[06/15 04:06:29][INFO] visual_prompt:  301: 	Training 1900/5004. train loss: 0.0107,	1.2132 s / batch. (data: 3.62e-04). ETA=6 days, 10:30:17, max mem: 15.0 GB 
[06/15 04:08:30][INFO] visual_prompt:  301: 	Training 2000/5004. train loss: 0.0047,	1.2073 s / batch. (data: 3.08e-04). ETA=6 days, 9:42:57, max mem: 15.0 GB 
[06/15 04:10:31][INFO] visual_prompt:  301: 	Training 2100/5004. train loss: 0.0154,	1.2072 s / batch. (data: 3.15e-04). ETA=6 days, 9:40:08, max mem: 15.0 GB 
[06/15 04:12:32][INFO] visual_prompt:  301: 	Training 2200/5004. train loss: 0.0196,	1.2069 s / batch. (data: 3.59e-04). ETA=6 days, 9:36:10, max mem: 15.0 GB 
[06/15 04:14:32][INFO] visual_prompt:  301: 	Training 2300/5004. train loss: 0.0391,	1.2092 s / batch. (data: 3.87e-04). ETA=6 days, 9:51:29, max mem: 15.0 GB 
[06/15 04:16:33][INFO] visual_prompt:  301: 	Training 2400/5004. train loss: 0.0250,	1.2058 s / batch. (data: 3.29e-04). ETA=6 days, 9:23:48, max mem: 15.0 GB 
[06/15 04:18:34][INFO] visual_prompt:  301: 	Training 2500/5004. train loss: 0.0259,	1.2058 s / batch. (data: 3.23e-04). ETA=6 days, 9:21:26, max mem: 15.0 GB 
[06/15 04:20:34][INFO] visual_prompt:  301: 	Training 2600/5004. train loss: 0.0112,	1.2088 s / batch. (data: 3.07e-04). ETA=6 days, 9:42:33, max mem: 15.0 GB 
[06/15 04:22:35][INFO] visual_prompt:  301: 	Training 2700/5004. train loss: 0.0347,	1.2068 s / batch. (data: 3.21e-04). ETA=6 days, 9:25:12, max mem: 15.0 GB 
[06/15 04:24:36][INFO] visual_prompt:  301: 	Training 2800/5004. train loss: 0.0368,	1.2067 s / batch. (data: 2.93e-04). ETA=6 days, 9:22:48, max mem: 15.0 GB 
[06/15 04:26:36][INFO] visual_prompt:  301: 	Training 2900/5004. train loss: 0.0010,	1.2031 s / batch. (data: 3.63e-04). ETA=6 days, 8:52:55, max mem: 15.0 GB 
[06/15 04:28:37][INFO] visual_prompt:  301: 	Training 3000/5004. train loss: 0.0047,	1.2135 s / batch. (data: 3.29e-04). ETA=6 days, 10:10:05, max mem: 15.0 GB 
[06/15 04:30:38][INFO] visual_prompt:  301: 	Training 3100/5004. train loss: 0.0118,	1.2099 s / batch. (data: 2.80e-04). ETA=6 days, 9:41:05, max mem: 15.0 GB 
[06/15 04:32:39][INFO] visual_prompt:  301: 	Training 3200/5004. train loss: 0.0044,	1.2116 s / batch. (data: 3.12e-04). ETA=6 days, 9:51:50, max mem: 15.0 GB 
[06/15 04:34:40][INFO] visual_prompt:  301: 	Training 3300/5004. train loss: 0.0242,	1.2110 s / batch. (data: 3.63e-04). ETA=6 days, 9:44:49, max mem: 15.0 GB 
[06/15 04:36:41][INFO] visual_prompt:  301: 	Training 3400/5004. train loss: 0.0434,	1.2065 s / batch. (data: 3.10e-04). ETA=6 days, 9:08:40, max mem: 15.0 GB 
[06/15 04:38:42][INFO] visual_prompt:  301: 	Training 3500/5004. train loss: 0.0010,	1.2011 s / batch. (data: 3.87e-04). ETA=6 days, 8:25:53, max mem: 15.0 GB 
[06/15 04:40:43][INFO] visual_prompt:  301: 	Training 3600/5004. train loss: 0.0366,	1.2149 s / batch. (data: 4.27e-04). ETA=6 days, 10:09:03, max mem: 15.0 GB 
[06/15 04:42:44][INFO] visual_prompt:  301: 	Training 3700/5004. train loss: 0.0200,	1.2066 s / batch. (data: 3.51e-04). ETA=6 days, 9:03:35, max mem: 15.0 GB 
[06/15 04:44:45][INFO] visual_prompt:  301: 	Training 3800/5004. train loss: 0.0400,	1.2042 s / batch. (data: 3.63e-04). ETA=6 days, 8:43:06, max mem: 15.0 GB 
[06/15 04:46:45][INFO] visual_prompt:  301: 	Training 3900/5004. train loss: 0.0363,	1.2038 s / batch. (data: 3.28e-04). ETA=6 days, 8:38:07, max mem: 15.0 GB 
[06/15 04:48:46][INFO] visual_prompt:  301: 	Training 4000/5004. train loss: 0.0528,	1.2076 s / batch. (data: 5.34e-04). ETA=6 days, 9:04:49, max mem: 15.0 GB 
[06/15 04:50:47][INFO] visual_prompt:  301: 	Training 4100/5004. train loss: 0.0220,	1.2051 s / batch. (data: 3.02e-04). ETA=6 days, 8:43:46, max mem: 15.0 GB 
[06/15 04:52:48][INFO] visual_prompt:  301: 	Training 4200/5004. train loss: 0.0371,	1.2070 s / batch. (data: 4.60e-04). ETA=6 days, 8:56:54, max mem: 15.0 GB 
[06/15 04:54:48][INFO] visual_prompt:  301: 	Training 4300/5004. train loss: 0.0257,	1.2070 s / batch. (data: 3.41e-04). ETA=6 days, 8:54:30, max mem: 15.0 GB 
[06/15 04:56:49][INFO] visual_prompt:  301: 	Training 4400/5004. train loss: 0.0329,	1.2044 s / batch. (data: 2.77e-04). ETA=6 days, 8:32:40, max mem: 15.0 GB 
[06/15 04:58:50][INFO] visual_prompt:  301: 	Training 4500/5004. train loss: 0.0168,	1.2098 s / batch. (data: 3.32e-04). ETA=6 days, 9:11:42, max mem: 15.0 GB 
[06/15 05:00:50][INFO] visual_prompt:  301: 	Training 4600/5004. train loss: 0.0315,	1.2096 s / batch. (data: 3.60e-04). ETA=6 days, 9:08:18, max mem: 15.0 GB 
[06/15 05:02:51][INFO] visual_prompt:  301: 	Training 4700/5004. train loss: 0.0142,	1.2194 s / batch. (data: 3.45e-04). ETA=6 days, 10:20:29, max mem: 15.0 GB 
[06/15 05:04:52][INFO] visual_prompt:  301: 	Training 4800/5004. train loss: 0.0053,	1.2112 s / batch. (data: 3.57e-04). ETA=6 days, 9:16:36, max mem: 15.0 GB 
[06/15 05:06:53][INFO] visual_prompt:  301: 	Training 4900/5004. train loss: 0.0205,	1.2075 s / batch. (data: 3.35e-04). ETA=6 days, 8:46:01, max mem: 15.0 GB 
[06/15 05:08:54][INFO] visual_prompt:  301: 	Training 5000/5004. train loss: 0.0013,	1.2052 s / batch. (data: 1.88e-04). ETA=6 days, 8:26:35, max mem: 15.0 GB 
[06/15 05:09:00][INFO] visual_prompt:  314: Epoch 9 / 100: avg data time: 4.35e-03, avg batch time: 1.2168, average train loss: 0.0201
[06/15 05:17:58][INFO] visual_prompt:  434: 	Test 100/196. loss: 0.020, 5.1932 s / batch. (data: 1.35e-04)max mem: 14.99516 GB 
[06/15 05:26:16][INFO] visual_prompt:  471: Inference (val):avg data time: 1.56e-04, avg batch time: 5.1831, average loss: 0.0210
[06/15 05:26:16][INFO] visual_prompt:  488: Saved invariances for val_imagenet at output_shallow_10/val_imagenet_invariances.json
[06/15 05:26:16][INFO] visual_prompt:  252: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[06/15 05:29:00][INFO] visual_prompt:  301: 	Training 100/5004. train loss: 0.0438,	1.2053 s / batch. (data: 3.07e-04). ETA=6 days, 8:25:39, max mem: 15.0 GB 
[06/15 05:31:01][INFO] visual_prompt:  301: 	Training 200/5004. train loss: 0.0048,	1.2059 s / batch. (data: 3.29e-04). ETA=6 days, 8:28:06, max mem: 15.0 GB 
[06/15 05:33:01][INFO] visual_prompt:  301: 	Training 300/5004. train loss: 0.0012,	1.2028 s / batch. (data: 3.41e-04). ETA=6 days, 8:02:18, max mem: 15.0 GB 
[06/15 05:35:02][INFO] visual_prompt:  301: 	Training 400/5004. train loss: 0.0184,	1.2044 s / batch. (data: 3.01e-04). ETA=6 days, 8:12:51, max mem: 15.0 GB 
[06/15 05:37:03][INFO] visual_prompt:  301: 	Training 500/5004. train loss: 0.0203,	1.2002 s / batch. (data: 4.23e-04). ETA=6 days, 7:38:34, max mem: 15.0 GB 
[06/15 05:39:04][INFO] visual_prompt:  301: 	Training 600/5004. train loss: 0.0205,	1.2080 s / batch. (data: 2.96e-04). ETA=6 days, 8:35:53, max mem: 15.0 GB 
[06/15 05:41:04][INFO] visual_prompt:  301: 	Training 700/5004. train loss: 0.0095,	1.2106 s / batch. (data: 3.38e-04). ETA=6 days, 8:53:34, max mem: 15.0 GB 
[06/15 05:43:05][INFO] visual_prompt:  301: 	Training 800/5004. train loss: 0.0090,	1.2084 s / batch. (data: 3.25e-04). ETA=6 days, 8:35:05, max mem: 15.0 GB 
[06/15 05:45:07][INFO] visual_prompt:  301: 	Training 900/5004. train loss: 0.0490,	1.2151 s / batch. (data: 3.32e-04). ETA=6 days, 9:23:58, max mem: 15.0 GB 
[06/15 05:47:08][INFO] visual_prompt:  301: 	Training 1000/5004. train loss: 0.0145,	1.2121 s / batch. (data: 3.52e-04). ETA=6 days, 8:59:10, max mem: 15.0 GB 
[06/15 05:49:08][INFO] visual_prompt:  301: 	Training 1100/5004. train loss: 0.0408,	1.2089 s / batch. (data: 3.16e-04). ETA=6 days, 8:32:58, max mem: 15.0 GB 
[06/15 05:51:09][INFO] visual_prompt:  301: 	Training 1200/5004. train loss: 0.0201,	1.2081 s / batch. (data: 3.69e-04). ETA=6 days, 8:24:18, max mem: 15.0 GB 
[06/15 05:53:10][INFO] visual_prompt:  301: 	Training 1300/5004. train loss: 0.0456,	1.2112 s / batch. (data: 3.25e-04). ETA=6 days, 8:46:01, max mem: 15.0 GB 
[06/15 05:55:11][INFO] visual_prompt:  301: 	Training 1400/5004. train loss: 0.0272,	1.2068 s / batch. (data: 3.80e-04). ETA=6 days, 8:10:56, max mem: 15.0 GB 
[06/15 05:57:12][INFO] visual_prompt:  301: 	Training 1500/5004. train loss: 0.0109,	1.2043 s / batch. (data: 3.09e-04). ETA=6 days, 7:49:52, max mem: 15.0 GB 
[06/15 05:59:12][INFO] visual_prompt:  301: 	Training 1600/5004. train loss: 0.0228,	1.2041 s / batch. (data: 3.28e-04). ETA=6 days, 7:45:59, max mem: 15.0 GB 
[06/15 06:01:13][INFO] visual_prompt:  301: 	Training 1700/5004. train loss: 0.0269,	1.2052 s / batch. (data: 3.65e-04). ETA=6 days, 7:52:33, max mem: 15.0 GB 
[06/15 06:03:14][INFO] visual_prompt:  301: 	Training 1800/5004. train loss: 0.0112,	1.2020 s / batch. (data: 3.53e-04). ETA=6 days, 7:26:46, max mem: 15.0 GB 
[06/15 06:05:14][INFO] visual_prompt:  301: 	Training 1900/5004. train loss: 0.0413,	1.2059 s / batch. (data: 3.41e-04). ETA=6 days, 7:53:50, max mem: 15.0 GB 
[06/15 06:07:15][INFO] visual_prompt:  301: 	Training 2000/5004. train loss: 0.0011,	1.2099 s / batch. (data: 3.53e-04). ETA=6 days, 8:22:19, max mem: 15.0 GB 
[06/15 06:09:16][INFO] visual_prompt:  301: 	Training 2100/5004. train loss: 0.0377,	1.2077 s / batch. (data: 4.04e-04). ETA=6 days, 8:03:07, max mem: 15.0 GB 
[06/15 06:11:17][INFO] visual_prompt:  301: 	Training 2200/5004. train loss: 0.0053,	1.2011 s / batch. (data: 3.45e-04). ETA=6 days, 7:11:50, max mem: 15.0 GB 
[06/15 06:13:17][INFO] visual_prompt:  301: 	Training 2300/5004. train loss: 0.0435,	1.2102 s / batch. (data: 3.41e-04). ETA=6 days, 8:18:39, max mem: 15.0 GB 
[06/15 06:15:18][INFO] visual_prompt:  301: 	Training 2400/5004. train loss: 0.0056,	1.2034 s / batch. (data: 3.20e-04). ETA=6 days, 7:25:10, max mem: 15.0 GB 
[06/15 06:17:19][INFO] visual_prompt:  301: 	Training 2500/5004. train loss: 0.0099,	1.2051 s / batch. (data: 3.43e-04). ETA=6 days, 7:35:37, max mem: 15.0 GB 
[06/15 06:19:20][INFO] visual_prompt:  301: 	Training 2600/5004. train loss: 0.0074,	1.2069 s / batch. (data: 3.41e-04). ETA=6 days, 7:47:11, max mem: 15.0 GB 
[06/15 06:21:21][INFO] visual_prompt:  301: 	Training 2700/5004. train loss: 0.0266,	1.2065 s / batch. (data: 2.91e-04). ETA=6 days, 7:42:26, max mem: 15.0 GB 
[06/15 06:23:22][INFO] visual_prompt:  301: 	Training 2800/5004. train loss: 0.0431,	1.2072 s / batch. (data: 3.16e-04). ETA=6 days, 7:45:13, max mem: 15.0 GB 
[06/15 06:25:23][INFO] visual_prompt:  301: 	Training 2900/5004. train loss: 0.0011,	1.2053 s / batch. (data: 3.50e-04). ETA=6 days, 7:29:22, max mem: 15.0 GB 
[06/15 06:27:24][INFO] visual_prompt:  301: 	Training 3000/5004. train loss: 0.0213,	1.2084 s / batch. (data: 3.29e-04). ETA=6 days, 7:50:28, max mem: 15.0 GB 
[06/15 06:29:25][INFO] visual_prompt:  301: 	Training 3100/5004. train loss: 0.0011,	1.2111 s / batch. (data: 3.20e-04). ETA=6 days, 8:08:34, max mem: 15.0 GB 
[06/15 06:31:26][INFO] visual_prompt:  301: 	Training 3200/5004. train loss: 0.0183,	1.2055 s / batch. (data: 3.83e-04). ETA=6 days, 7:24:54, max mem: 15.0 GB 
[06/15 06:33:27][INFO] visual_prompt:  301: 	Training 3300/5004. train loss: 0.0172,	1.2047 s / batch. (data: 3.57e-04). ETA=6 days, 7:16:58, max mem: 15.0 GB 
[06/15 06:35:28][INFO] visual_prompt:  301: 	Training 3400/5004. train loss: 0.0421,	1.2114 s / batch. (data: 3.44e-04). ETA=6 days, 8:04:49, max mem: 15.0 GB 
[06/15 06:37:28][INFO] visual_prompt:  301: 	Training 3500/5004. train loss: 0.0424,	1.2047 s / batch. (data: 3.04e-04). ETA=6 days, 7:13:02, max mem: 15.0 GB 
[06/15 06:39:29][INFO] visual_prompt:  301: 	Training 3600/5004. train loss: 0.0363,	1.2078 s / batch. (data: 3.26e-04). ETA=6 days, 7:34:19, max mem: 15.0 GB 
[06/15 06:41:30][INFO] visual_prompt:  301: 	Training 3700/5004. train loss: 0.0084,	1.2082 s / batch. (data: 3.28e-04). ETA=6 days, 7:35:02, max mem: 15.0 GB 
[06/15 06:43:31][INFO] visual_prompt:  301: 	Training 3800/5004. train loss: 0.0439,	1.2074 s / batch. (data: 3.46e-04). ETA=6 days, 7:27:18, max mem: 15.0 GB 
[06/15 06:45:32][INFO] visual_prompt:  301: 	Training 3900/5004. train loss: 0.0153,	1.2074 s / batch. (data: 3.37e-04). ETA=6 days, 7:25:19, max mem: 15.0 GB 
[06/15 06:47:33][INFO] visual_prompt:  301: 	Training 4000/5004. train loss: 0.0149,	1.2092 s / batch. (data: 3.78e-04). ETA=6 days, 7:36:11, max mem: 15.0 GB 
[06/15 06:49:33][INFO] visual_prompt:  301: 	Training 4100/5004. train loss: 0.0071,	1.2105 s / batch. (data: 3.68e-04). ETA=6 days, 7:44:07, max mem: 15.0 GB 
[06/15 06:51:34][INFO] visual_prompt:  301: 	Training 4200/5004. train loss: 0.0438,	1.2140 s / batch. (data: 3.61e-04). ETA=6 days, 8:08:21, max mem: 15.0 GB 
[06/15 06:53:35][INFO] visual_prompt:  301: 	Training 4300/5004. train loss: 0.0076,	1.2103 s / batch. (data: 3.25e-04). ETA=6 days, 7:38:54, max mem: 15.0 GB 
[06/15 06:55:36][INFO] visual_prompt:  301: 	Training 4400/5004. train loss: 0.0079,	1.2124 s / batch. (data: 3.40e-04). ETA=6 days, 7:52:31, max mem: 15.0 GB 
[06/15 06:57:37][INFO] visual_prompt:  301: 	Training 4500/5004. train loss: 0.0020,	1.2067 s / batch. (data: 3.72e-04). ETA=6 days, 7:07:21, max mem: 15.0 GB 
[06/15 06:59:38][INFO] visual_prompt:  301: 	Training 4600/5004. train loss: 0.0153,	1.2111 s / batch. (data: 3.23e-04). ETA=6 days, 7:38:30, max mem: 15.0 GB 
[06/15 07:01:39][INFO] visual_prompt:  301: 	Training 4700/5004. train loss: 0.0371,	1.2078 s / batch. (data: 3.19e-04). ETA=6 days, 7:12:04, max mem: 15.0 GB 
[06/15 07:03:40][INFO] visual_prompt:  301: 	Training 4800/5004. train loss: 0.0066,	1.2084 s / batch. (data: 3.09e-04). ETA=6 days, 7:14:34, max mem: 15.0 GB 
[06/15 07:05:41][INFO] visual_prompt:  301: 	Training 4900/5004. train loss: 0.0085,	1.2123 s / batch. (data: 3.41e-04). ETA=6 days, 7:41:29, max mem: 15.0 GB 
[06/15 07:07:42][INFO] visual_prompt:  301: 	Training 5000/5004. train loss: 0.0058,	1.2055 s / batch. (data: 1.18e-04). ETA=6 days, 6:48:28, max mem: 15.0 GB 
[06/15 07:07:48][INFO] visual_prompt:  314: Epoch 10 / 100: avg data time: 4.15e-03, avg batch time: 1.2172, average train loss: 0.0216
[06/15 07:16:43][INFO] visual_prompt:  434: 	Test 100/196. loss: 0.019, 5.2057 s / batch. (data: 1.50e-04)max mem: 14.99516 GB 
[06/15 07:25:01][INFO] visual_prompt:  471: Inference (val):avg data time: 1.51e-04, avg batch time: 5.1816, average loss: 0.0196
[06/15 07:25:01][INFO] visual_prompt:  488: Saved invariances for val_imagenet at output_shallow_10/val_imagenet_invariances.json
[06/15 07:25:01][INFO] visual_prompt:  252: Training 11 / 100 epoch, with learning rate 0.1
[06/15 07:27:48][INFO] visual_prompt:  301: 	Training 100/5004. train loss: 0.0082,	1.2100 s / batch. (data: 4.68e-04). ETA=6 days, 7:19:55, max mem: 15.0 GB 
[06/15 07:29:48][INFO] visual_prompt:  301: 	Training 200/5004. train loss: 0.0114,	1.2131 s / batch. (data: 4.84e-04). ETA=6 days, 7:41:44, max mem: 15.0 GB 
[06/15 07:31:49][INFO] visual_prompt:  301: 	Training 300/5004. train loss: 0.0075,	1.2073 s / batch. (data: 3.61e-04). ETA=6 days, 6:55:40, max mem: 15.0 GB 
[06/15 07:33:50][INFO] visual_prompt:  301: 	Training 400/5004. train loss: 0.0061,	1.2122 s / batch. (data: 3.37e-04). ETA=6 days, 7:30:23, max mem: 15.0 GB 
[06/15 07:35:51][INFO] visual_prompt:  301: 	Training 500/5004. train loss: 0.0471,	1.2126 s / batch. (data: 2.84e-04). ETA=6 days, 7:31:34, max mem: 15.0 GB 
[06/15 07:37:52][INFO] visual_prompt:  301: 	Training 600/5004. train loss: 0.0081,	1.2067 s / batch. (data: 2.83e-04). ETA=6 days, 6:45:23, max mem: 15.0 GB 
[06/15 07:39:53][INFO] visual_prompt:  301: 	Training 700/5004. train loss: 0.0117,	1.2121 s / batch. (data: 3.46e-04). ETA=6 days, 7:23:44, max mem: 15.0 GB 
[06/15 07:41:54][INFO] visual_prompt:  301: 	Training 800/5004. train loss: 0.0259,	1.2062 s / batch. (data: 3.38e-04). ETA=6 days, 6:37:29, max mem: 15.0 GB 
[06/15 07:43:55][INFO] visual_prompt:  301: 	Training 900/5004. train loss: 0.0393,	1.2116 s / batch. (data: 3.90e-04). ETA=6 days, 7:16:13, max mem: 15.0 GB 
[06/15 07:45:55][INFO] visual_prompt:  301: 	Training 1000/5004. train loss: 0.0242,	1.2095 s / batch. (data: 3.25e-04). ETA=6 days, 6:58:16, max mem: 15.0 GB 
[06/15 07:47:56][INFO] visual_prompt:  301: 	Training 1100/5004. train loss: 0.0152,	1.2050 s / batch. (data: 3.36e-04). ETA=6 days, 6:22:53, max mem: 15.0 GB 
[06/15 07:49:57][INFO] visual_prompt:  301: 	Training 1200/5004. train loss: 0.0056,	1.2091 s / batch. (data: 3.28e-04). ETA=6 days, 6:51:12, max mem: 15.0 GB 
[06/15 07:51:58][INFO] visual_prompt:  301: 	Training 1300/5004. train loss: 0.0056,	1.2071 s / batch. (data: 5.18e-04). ETA=6 days, 6:34:00, max mem: 15.0 GB 
[06/15 07:53:59][INFO] visual_prompt:  301: 	Training 1400/5004. train loss: 0.0055,	1.2069 s / batch. (data: 2.87e-04). ETA=6 days, 6:31:05, max mem: 15.0 GB 
[06/15 07:56:00][INFO] visual_prompt:  301: 	Training 1500/5004. train loss: 0.0224,	1.2082 s / batch. (data: 4.01e-04). ETA=6 days, 6:38:53, max mem: 15.0 GB 
[06/15 07:58:00][INFO] visual_prompt:  301: 	Training 1600/5004. train loss: 0.0156,	1.2056 s / batch. (data: 3.34e-04). ETA=6 days, 6:16:55, max mem: 15.0 GB 
[06/15 08:00:01][INFO] visual_prompt:  301: 	Training 1700/5004. train loss: 0.0414,	1.2063 s / batch. (data: 3.40e-04). ETA=6 days, 6:20:03, max mem: 15.0 GB 
[06/15 08:02:02][INFO] visual_prompt:  301: 	Training 1800/5004. train loss: 0.0265,	1.2104 s / batch. (data: 3.23e-04). ETA=6 days, 6:48:38, max mem: 15.0 GB 
[06/15 08:04:03][INFO] visual_prompt:  301: 	Training 1900/5004. train loss: 0.0154,	1.2086 s / batch. (data: 3.42e-04). ETA=6 days, 6:33:27, max mem: 15.0 GB 
[06/15 08:06:04][INFO] visual_prompt:  301: 	Training 2000/5004. train loss: 0.0233,	1.2066 s / batch. (data: 3.81e-04). ETA=6 days, 6:16:34, max mem: 15.0 GB 
[06/15 08:08:05][INFO] visual_prompt:  301: 	Training 2100/5004. train loss: 0.0208,	1.2122 s / batch. (data: 3.55e-04). ETA=6 days, 6:56:28, max mem: 15.0 GB 
[06/15 08:10:06][INFO] visual_prompt:  301: 	Training 2200/5004. train loss: 0.0185,	1.2082 s / batch. (data: 3.38e-04). ETA=6 days, 6:24:15, max mem: 15.0 GB 
[06/15 08:12:07][INFO] visual_prompt:  301: 	Training 2300/5004. train loss: 0.0016,	1.2120 s / batch. (data: 3.07e-04). ETA=6 days, 6:50:55, max mem: 15.0 GB 
[06/15 08:14:07][INFO] visual_prompt:  301: 	Training 2400/5004. train loss: 0.0362,	1.2108 s / batch. (data: 3.59e-04). ETA=6 days, 6:39:41, max mem: 15.0 GB 
[06/15 08:16:08][INFO] visual_prompt:  301: 	Training 2500/5004. train loss: 0.0014,	1.2124 s / batch. (data: 3.31e-04). ETA=6 days, 6:49:54, max mem: 15.0 GB 
[06/15 08:18:09][INFO] visual_prompt:  301: 	Training 2600/5004. train loss: 0.0084,	1.2075 s / batch. (data: 3.71e-04). ETA=6 days, 6:11:09, max mem: 15.0 GB 
[06/15 08:20:10][INFO] visual_prompt:  301: 	Training 2700/5004. train loss: 0.0117,	1.2088 s / batch. (data: 5.41e-04). ETA=6 days, 6:18:43, max mem: 15.0 GB 
[06/15 08:22:11][INFO] visual_prompt:  301: 	Training 2800/5004. train loss: 0.0102,	1.2078 s / batch. (data: 3.04e-04). ETA=6 days, 6:09:43, max mem: 15.0 GB 
[06/15 08:24:12][INFO] visual_prompt:  301: 	Training 2900/5004. train loss: 0.0119,	1.2066 s / batch. (data: 3.45e-04). ETA=6 days, 5:58:24, max mem: 15.0 GB 
[06/15 08:26:12][INFO] visual_prompt:  301: 	Training 3000/5004. train loss: 0.0057,	1.2062 s / batch. (data: 3.00e-04). ETA=6 days, 5:53:26, max mem: 15.0 GB 
[06/15 08:28:13][INFO] visual_prompt:  301: 	Training 3100/5004. train loss: 0.0420,	1.2034 s / batch. (data: 3.25e-04). ETA=6 days, 5:30:23, max mem: 15.0 GB 
[06/15 08:30:14][INFO] visual_prompt:  301: 	Training 3200/5004. train loss: 0.0208,	1.2037 s / batch. (data: 3.26e-04). ETA=6 days, 5:30:49, max mem: 15.0 GB 
[06/15 08:32:14][INFO] visual_prompt:  301: 	Training 3300/5004. train loss: 0.0206,	1.2053 s / batch. (data: 3.28e-04). ETA=6 days, 5:40:56, max mem: 15.0 GB 
[06/15 08:34:15][INFO] visual_prompt:  301: 	Training 3400/5004. train loss: 0.0480,	1.2052 s / batch. (data: 3.30e-04). ETA=6 days, 5:38:13, max mem: 15.0 GB 
[06/15 08:36:16][INFO] visual_prompt:  301: 	Training 3500/5004. train loss: 0.0048,	1.2101 s / batch. (data: 3.82e-04). ETA=6 days, 6:12:33, max mem: 15.0 GB 
[06/15 08:38:16][INFO] visual_prompt:  301: 	Training 3600/5004. train loss: 0.0037,	1.2144 s / batch. (data: 3.02e-04). ETA=6 days, 6:42:14, max mem: 15.0 GB 
[06/15 08:40:17][INFO] visual_prompt:  301: 	Training 3700/5004. train loss: 0.0060,	1.2114 s / batch. (data: 3.95e-04). ETA=6 days, 6:17:44, max mem: 15.0 GB 
[06/15 08:42:18][INFO] visual_prompt:  301: 	Training 3800/5004. train loss: 0.0163,	1.2146 s / batch. (data: 3.28e-04). ETA=6 days, 6:39:59, max mem: 15.0 GB 
[06/15 08:44:19][INFO] visual_prompt:  301: 	Training 3900/5004. train loss: 0.0070,	1.2065 s / batch. (data: 3.18e-04). ETA=6 days, 5:37:49, max mem: 15.0 GB 
[06/15 08:46:20][INFO] visual_prompt:  301: 	Training 4000/5004. train loss: 0.0437,	1.2117 s / batch. (data: 3.14e-04). ETA=6 days, 6:14:11, max mem: 15.0 GB 
[06/15 08:48:21][INFO] visual_prompt:  301: 	Training 4100/5004. train loss: 0.0047,	1.2072 s / batch. (data: 2.75e-04). ETA=6 days, 5:39:07, max mem: 15.0 GB 
[06/15 08:50:22][INFO] visual_prompt:  301: 	Training 4200/5004. train loss: 0.0152,	1.2129 s / batch. (data: 2.82e-04). ETA=6 days, 6:19:02, max mem: 15.0 GB 
[06/15 08:52:23][INFO] visual_prompt:  301: 	Training 4300/5004. train loss: 0.0047,	1.2106 s / batch. (data: 2.99e-04). ETA=6 days, 6:00:03, max mem: 15.0 GB 
[06/15 08:54:24][INFO] visual_prompt:  301: 	Training 4400/5004. train loss: 0.0154,	1.2072 s / batch. (data: 4.01e-04). ETA=6 days, 5:32:38, max mem: 15.0 GB 
[06/15 08:56:25][INFO] visual_prompt:  301: 	Training 4500/5004. train loss: 0.0153,	1.2087 s / batch. (data: 3.91e-04). ETA=6 days, 5:41:50, max mem: 15.0 GB 
[06/15 08:58:26][INFO] visual_prompt:  301: 	Training 4600/5004. train loss: 0.0107,	1.2063 s / batch. (data: 3.93e-04). ETA=6 days, 5:22:13, max mem: 15.0 GB 
[06/15 09:00:26][INFO] visual_prompt:  301: 	Training 4700/5004. train loss: 0.0188,	1.2062 s / batch. (data: 3.33e-04). ETA=6 days, 5:19:25, max mem: 15.0 GB 
[06/15 09:02:27][INFO] visual_prompt:  301: 	Training 4800/5004. train loss: 0.0069,	1.2081 s / batch. (data: 4.42e-04). ETA=6 days, 5:31:25, max mem: 15.0 GB 
[06/15 09:04:28][INFO] visual_prompt:  301: 	Training 4900/5004. train loss: 0.0139,	1.2084 s / batch. (data: 3.28e-04). ETA=6 days, 5:31:38, max mem: 15.0 GB 
[06/15 09:06:29][INFO] visual_prompt:  301: 	Training 5000/5004. train loss: 0.0161,	1.2041 s / batch. (data: 1.17e-04). ETA=6 days, 4:57:41, max mem: 15.0 GB 
[06/15 09:06:35][INFO] visual_prompt:  314: Epoch 11 / 100: avg data time: 4.19e-03, avg batch time: 1.2175, average train loss: 0.0176
[06/15 09:15:30][INFO] visual_prompt:  434: 	Test 100/196. loss: 0.016, 5.2014 s / batch. (data: 1.34e-04)max mem: 14.99516 GB 
[06/15 09:23:50][INFO] visual_prompt:  471: Inference (val):avg data time: 1.48e-04, avg batch time: 5.1902, average loss: 0.0168
[06/15 09:23:50][INFO] visual_prompt:  488: Saved invariances for val_imagenet at output_shallow_10/val_imagenet_invariances.json
[06/15 09:23:50][INFO] visual_prompt:  252: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[06/15 09:26:36][INFO] visual_prompt:  301: 	Training 100/5004. train loss: 0.0149,	1.2109 s / batch. (data: 3.16e-04). ETA=6 days, 5:46:16, max mem: 15.0 GB 
[06/15 09:28:37][INFO] visual_prompt:  301: 	Training 200/5004. train loss: 0.0059,	1.2050 s / batch. (data: 3.75e-04). ETA=6 days, 5:00:23, max mem: 15.0 GB 
[06/15 09:30:37][INFO] visual_prompt:  301: 	Training 300/5004. train loss: 0.0064,	1.2083 s / batch. (data: 2.81e-04). ETA=6 days, 5:22:49, max mem: 15.0 GB 
[06/15 09:32:38][INFO] visual_prompt:  301: 	Training 400/5004. train loss: 0.0208,	1.2079 s / batch. (data: 2.99e-04). ETA=6 days, 5:17:29, max mem: 15.0 GB 
[06/15 09:34:39][INFO] visual_prompt:  301: 	Training 500/5004. train loss: 0.0051,	1.2100 s / batch. (data: 3.47e-04). ETA=6 days, 5:31:36, max mem: 15.0 GB 
[06/15 09:36:40][INFO] visual_prompt:  301: 	Training 600/5004. train loss: 0.0457,	1.2086 s / batch. (data: 3.36e-04). ETA=6 days, 5:18:30, max mem: 15.0 GB 
[06/15 09:38:41][INFO] visual_prompt:  301: 	Training 700/5004. train loss: 0.0055,	1.2066 s / batch. (data: 3.47e-04). ETA=6 days, 5:02:15, max mem: 15.0 GB 
[06/15 09:40:41][INFO] visual_prompt:  301: 	Training 800/5004. train loss: 0.0234,	1.2021 s / batch. (data: 3.68e-04). ETA=6 days, 4:26:21, max mem: 15.0 GB 
[06/15 09:42:42][INFO] visual_prompt:  301: 	Training 900/5004. train loss: 0.0277,	1.2074 s / batch. (data: 3.34e-04). ETA=6 days, 5:03:38, max mem: 15.0 GB 
[06/15 09:44:43][INFO] visual_prompt:  301: 	Training 1000/5004. train loss: 0.0417,	1.2082 s / batch. (data: 3.57e-04). ETA=6 days, 5:07:28, max mem: 15.0 GB 
[06/15 09:46:44][INFO] visual_prompt:  301: 	Training 1100/5004. train loss: 0.0140,	1.2076 s / batch. (data: 2.94e-04). ETA=6 days, 5:01:24, max mem: 15.0 GB 
[06/15 09:48:44][INFO] visual_prompt:  301: 	Training 1200/5004. train loss: 0.0051,	1.2081 s / batch. (data: 4.01e-04). ETA=6 days, 5:03:04, max mem: 15.0 GB 
[06/15 09:50:45][INFO] visual_prompt:  301: 	Training 1300/5004. train loss: 0.0103,	1.2106 s / batch. (data: 3.10e-04). ETA=6 days, 5:19:38, max mem: 15.0 GB 
[06/15 09:52:46][INFO] visual_prompt:  301: 	Training 1400/5004. train loss: 0.0051,	1.2043 s / batch. (data: 3.47e-04). ETA=6 days, 4:30:40, max mem: 15.0 GB 
[06/15 09:54:47][INFO] visual_prompt:  301: 	Training 1500/5004. train loss: 0.0052,	1.2070 s / batch. (data: 4.27e-04). ETA=6 days, 4:48:52, max mem: 15.0 GB 
[06/15 09:56:47][INFO] visual_prompt:  301: 	Training 1600/5004. train loss: 0.0102,	1.2086 s / batch. (data: 3.00e-04). ETA=6 days, 4:58:58, max mem: 15.0 GB 
[06/15 09:58:48][INFO] visual_prompt:  301: 	Training 1700/5004. train loss: 0.0088,	1.2154 s / batch. (data: 3.54e-04). ETA=6 days, 5:46:56, max mem: 15.0 GB 
[06/15 10:00:49][INFO] visual_prompt:  301: 	Training 1800/5004. train loss: 0.0441,	1.2061 s / batch. (data: 3.47e-04). ETA=6 days, 4:36:21, max mem: 15.0 GB 
[06/15 10:02:50][INFO] visual_prompt:  301: 	Training 1900/5004. train loss: 0.0072,	1.2084 s / batch. (data: 3.76e-04). ETA=6 days, 4:51:11, max mem: 15.0 GB 
[06/15 10:04:51][INFO] visual_prompt:  301: 	Training 2000/5004. train loss: 0.0108,	1.2094 s / batch. (data: 3.51e-04). ETA=6 days, 4:56:23, max mem: 15.0 GB 
[06/15 10:06:52][INFO] visual_prompt:  301: 	Training 2100/5004. train loss: 0.0244,	1.2070 s / batch. (data: 3.10e-04). ETA=6 days, 4:36:55, max mem: 15.0 GB 
[06/15 10:08:53][INFO] visual_prompt:  301: 	Training 2200/5004. train loss: 0.0237,	1.2135 s / batch. (data: 3.21e-04). ETA=6 days, 5:22:37, max mem: 15.0 GB 
[06/15 10:10:54][INFO] visual_prompt:  301: 	Training 2300/5004. train loss: 0.0393,	1.2078 s / batch. (data: 3.00e-04). ETA=6 days, 4:38:37, max mem: 15.0 GB 
[06/15 10:12:55][INFO] visual_prompt:  301: 	Training 2400/5004. train loss: 0.0156,	1.2056 s / batch. (data: 3.19e-04). ETA=6 days, 4:20:24, max mem: 15.0 GB 
[06/15 12:42:38][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/15 12:42:38][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,4,5,6
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/15 12:42:38][INFO] visual_prompt:   99: Command line arguments: None
[06/15 12:42:38][INFO] visual_prompt:  108: Training with config:
[06/15 12:42:38][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_shallow_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.1,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'mse',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/15 12:42:41][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/15 12:42:41][INFO] visual_prompt:   56: Total Parameters: 93879568	 Gradient Parameters: 8081680
[06/15 12:42:41][INFO] visual_prompt:   58: tuned percent:8.609
[06/15 12:42:42][INFO] visual_prompt:   44: Device used for model: 0
[06/15 12:42:42][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/15 12:42:42][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/15 12:42:44][INFO] visual_prompt:  158: Number of images: 1281167
[06/15 12:42:44][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 12:42:44][INFO] visual_prompt:   78: Loading validation data...
[06/15 12:42:44][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/15 12:42:45][INFO] visual_prompt:  158: Number of images: 50000
[06/15 12:42:45][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 12:42:45][INFO] visual_prompt:   81: Loading test data...
[06/15 12:42:45][INFO] visual_prompt:   83: ...no test data is constructed
[06/15 12:42:45][INFO] visual_prompt:  111: Constructing models...
[06/15 12:42:45][INFO] visual_prompt:  114: Setting up Evalutator...
[06/15 12:42:45][INFO] visual_prompt:  116: Setting up Trainer...
[06/15 12:42:45][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/15 12:42:45][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/15 12:42:45][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/15 12:42:45][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/15 12:42:45][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/15 12:42:45][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/15 12:42:45][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/15 12:42:45][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/15 12:42:45][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/15 12:42:45][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/15 12:42:45][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/15 12:42:45][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/15 12:42:45][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/15 12:42:45][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/15 12:42:45][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/15 12:42:45][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/15 12:42:45][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/15 12:42:45][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/15 12:42:45][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/15 12:42:45][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/15 12:42:45][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/15 12:42:45][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/15 12:42:45][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/15 12:43:04][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:43:04][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:43:07][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:43:09][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:43:09][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:43:11][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:43:11][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:43:11][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:43:13][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:43:14][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:43:14][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:43:16][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:43:17][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:43:17][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:43:18][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:43:19][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:43:19][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:43:21][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:43:22][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:43:22][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:43:24][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:43:24][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:43:24][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:43:26][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:43:27][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:43:27][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:43:29][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:43:29][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:43:29][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:43:31][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:43:32][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:43:32][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:43:34][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:43:35][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:43:35][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:43:37][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:43:37][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:43:37][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:43:39][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:43:40][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:43:40][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:43:42][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:43:42][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:43:42][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:43:44][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:43:45][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:43:45][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:43:47][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:43:48][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:43:48][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:43:50][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:43:50][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:43:50][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:43:52][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:43:53][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:43:53][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:43:55][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:43:56][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:43:56][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:43:58][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:44:00][INFO] visual_prompt:  314: Epoch 1 / 100: avg data time: 9.57e-01, avg batch time: 3.6815, average train loss: 0.0654
[06/15 12:44:14][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 12:44:14][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 12:44:14][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 12:44:22][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 12:44:22][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 12:44:22][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 12:44:22][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 12:44:27][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 12:44:27][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 12:44:27][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 12:44:27][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 12:44:32][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 12:44:32][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 12:44:32][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 12:44:32][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 12:44:37][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 12:44:37][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 12:44:37][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 12:44:37][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 12:44:42][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 12:44:42][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 12:44:42][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 12:44:42][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 12:44:47][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 12:44:47][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 12:44:47][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 12:44:47][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 12:44:53][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 12:44:53][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 12:44:53][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 12:44:53][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 12:44:58][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 12:44:58][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 12:44:58][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 12:44:58][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 12:45:03][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 12:45:03][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 12:45:03][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 12:45:03][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 12:45:08][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 12:45:09][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 12:45:09][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 12:45:09][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 12:45:14][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 12:45:14][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 12:45:14][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 12:45:14][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 12:45:19][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 12:45:19][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 12:45:19][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 12:45:19][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 12:45:24][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 12:45:24][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 12:45:24][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 12:45:24][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 12:45:30][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 12:45:30][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 12:45:30][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 12:45:30][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 12:45:35][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 12:45:35][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 12:45:35][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 12:45:35][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 12:51:52][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/15 12:51:52][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,4,5,6
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/15 12:51:52][INFO] visual_prompt:   99: Command line arguments: None
[06/15 12:51:52][INFO] visual_prompt:  108: Training with config:
[06/15 12:51:52][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_shallow_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.1,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'mse',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/15 12:51:55][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/15 12:51:55][INFO] visual_prompt:   56: Total Parameters: 93879568	 Gradient Parameters: 8081680
[06/15 12:51:55][INFO] visual_prompt:   58: tuned percent:8.609
[06/15 12:51:55][INFO] visual_prompt:   44: Device used for model: 0
[06/15 12:51:55][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/15 12:51:55][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/15 12:51:58][INFO] visual_prompt:  158: Number of images: 1281167
[06/15 12:51:58][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 12:51:58][INFO] visual_prompt:   78: Loading validation data...
[06/15 12:51:58][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/15 12:51:58][INFO] visual_prompt:  158: Number of images: 50000
[06/15 12:51:58][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 12:51:58][INFO] visual_prompt:   81: Loading test data...
[06/15 12:51:58][INFO] visual_prompt:   83: ...no test data is constructed
[06/15 12:51:58][INFO] visual_prompt:  111: Constructing models...
[06/15 12:51:58][INFO] visual_prompt:  114: Setting up Evalutator...
[06/15 12:51:58][INFO] visual_prompt:  116: Setting up Trainer...
[06/15 12:51:58][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/15 12:51:58][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/15 12:51:58][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/15 12:51:58][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/15 12:51:58][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/15 12:51:58][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/15 12:51:58][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/15 12:51:58][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/15 12:51:58][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/15 12:51:58][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/15 12:51:58][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/15 12:51:58][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/15 12:51:58][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/15 12:51:58][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/15 12:51:58][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/15 12:51:58][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/15 12:51:58][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/15 12:51:58][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/15 12:51:58][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/15 12:51:58][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/15 12:51:58][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/15 12:51:58][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/15 12:51:58][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/15 12:52:18][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:52:18][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:52:22][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:52:23][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:52:23][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:52:25][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:52:25][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:52:25][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:52:27][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:52:28][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:52:28][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:52:30][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:52:31][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:52:31][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:54:31][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/15 12:54:31][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,4,5,6
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/15 12:54:31][INFO] visual_prompt:   99: Command line arguments: None
[06/15 12:54:31][INFO] visual_prompt:  108: Training with config:
[06/15 12:54:31][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_shallow_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.1,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'mse',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/15 12:54:35][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/15 12:54:35][INFO] visual_prompt:   56: Total Parameters: 93879568	 Gradient Parameters: 8081680
[06/15 12:54:35][INFO] visual_prompt:   58: tuned percent:8.609
[06/15 12:54:35][INFO] visual_prompt:   44: Device used for model: 0
[06/15 12:54:35][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/15 12:54:35][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/15 12:54:38][INFO] visual_prompt:  158: Number of images: 1281167
[06/15 12:54:38][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 12:54:38][INFO] visual_prompt:   78: Loading validation data...
[06/15 12:54:38][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/15 12:54:38][INFO] visual_prompt:  158: Number of images: 50000
[06/15 12:54:38][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 12:54:38][INFO] visual_prompt:   81: Loading test data...
[06/15 12:54:38][INFO] visual_prompt:   83: ...no test data is constructed
[06/15 12:54:38][INFO] visual_prompt:  111: Constructing models...
[06/15 12:54:38][INFO] visual_prompt:  114: Setting up Evalutator...
[06/15 12:54:38][INFO] visual_prompt:  116: Setting up Trainer...
[06/15 12:54:38][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/15 12:54:38][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/15 12:54:38][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/15 12:54:38][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/15 12:54:38][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/15 12:54:38][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/15 12:54:38][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/15 12:54:38][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/15 12:54:38][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/15 12:54:38][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/15 12:54:38][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/15 12:54:38][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/15 12:54:38][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/15 12:54:38][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/15 12:54:38][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/15 12:54:38][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/15 12:54:38][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/15 12:54:38][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/15 12:54:38][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/15 12:54:38][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/15 12:54:38][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/15 12:54:38][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/15 12:54:38][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/15 12:54:58][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:54:58][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:55:54][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/15 12:55:54][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,4,5,6
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/15 12:55:54][INFO] visual_prompt:   99: Command line arguments: None
[06/15 12:55:54][INFO] visual_prompt:  108: Training with config:
[06/15 12:55:54][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_shallow_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.1,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'mse',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/15 12:55:57][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/15 12:55:57][INFO] visual_prompt:   56: Total Parameters: 93879568	 Gradient Parameters: 8081680
[06/15 12:55:57][INFO] visual_prompt:   58: tuned percent:8.609
[06/15 12:55:58][INFO] visual_prompt:   44: Device used for model: 0
[06/15 12:55:58][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/15 12:55:58][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/15 12:56:01][INFO] visual_prompt:  158: Number of images: 1281167
[06/15 12:56:01][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 12:56:01][INFO] visual_prompt:   78: Loading validation data...
[06/15 12:56:01][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/15 12:56:01][INFO] visual_prompt:  158: Number of images: 50000
[06/15 12:56:01][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 12:56:01][INFO] visual_prompt:   81: Loading test data...
[06/15 12:56:01][INFO] visual_prompt:   83: ...no test data is constructed
[06/15 12:56:01][INFO] visual_prompt:  111: Constructing models...
[06/15 12:56:01][INFO] visual_prompt:  114: Setting up Evalutator...
[06/15 12:56:01][INFO] visual_prompt:  116: Setting up Trainer...
[06/15 12:56:01][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/15 12:56:01][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/15 12:56:01][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/15 12:56:01][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/15 12:56:01][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/15 12:56:01][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/15 12:56:01][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/15 12:56:01][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/15 12:56:01][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/15 12:56:01][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/15 12:56:01][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/15 12:56:01][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/15 12:56:01][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/15 12:56:01][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/15 12:56:01][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/15 12:56:01][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/15 12:56:01][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/15 12:56:01][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/15 12:56:01][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/15 12:56:01][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/15 12:56:01][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/15 12:56:01][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/15 12:56:01][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/15 12:56:21][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:56:21][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:56:24][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:56:25][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:56:25][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:56:27][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:56:28][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:56:28][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:56:30][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:56:30][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:56:30][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:56:32][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:56:33][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:56:33][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:56:35][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:56:36][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:56:36][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:56:38][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:56:38][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:56:38][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:56:40][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:56:41][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:56:41][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:57:37][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/15 12:57:37][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,4,5,6
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/15 12:57:37][INFO] visual_prompt:   99: Command line arguments: None
[06/15 12:57:37][INFO] visual_prompt:  108: Training with config:
[06/15 12:57:37][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_shallow_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.1,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'mse',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/15 12:57:41][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/15 12:57:41][INFO] visual_prompt:   56: Total Parameters: 93879568	 Gradient Parameters: 8081680
[06/15 12:57:41][INFO] visual_prompt:   58: tuned percent:8.609
[06/15 12:57:41][INFO] visual_prompt:   44: Device used for model: 0
[06/15 12:57:41][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/15 12:57:41][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/15 12:57:44][INFO] visual_prompt:  158: Number of images: 1281167
[06/15 12:57:44][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 12:57:44][INFO] visual_prompt:   78: Loading validation data...
[06/15 12:57:44][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/15 12:57:44][INFO] visual_prompt:  158: Number of images: 50000
[06/15 12:57:44][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 12:57:44][INFO] visual_prompt:   81: Loading test data...
[06/15 12:57:44][INFO] visual_prompt:   83: ...no test data is constructed
[06/15 12:57:44][INFO] visual_prompt:  111: Constructing models...
[06/15 12:57:44][INFO] visual_prompt:  114: Setting up Evalutator...
[06/15 12:57:44][INFO] visual_prompt:  116: Setting up Trainer...
[06/15 12:57:44][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/15 12:57:44][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/15 12:57:44][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/15 12:57:44][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/15 12:57:44][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/15 12:57:44][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/15 12:57:44][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/15 12:57:44][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/15 12:57:44][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/15 12:57:44][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/15 12:57:44][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/15 12:57:44][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/15 12:57:44][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/15 12:57:44][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/15 12:57:44][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/15 12:57:44][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/15 12:57:44][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/15 12:57:44][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/15 12:57:44][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/15 12:57:44][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/15 12:57:44][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/15 12:57:44][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/15 12:57:44][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/15 12:58:04][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:58:04][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:58:54][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/15 12:58:54][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,4,5,6
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/15 12:58:54][INFO] visual_prompt:   99: Command line arguments: None
[06/15 12:58:54][INFO] visual_prompt:  108: Training with config:
[06/15 12:58:54][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_shallow_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.1,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'mse',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/15 12:58:57][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/15 12:58:57][INFO] visual_prompt:   56: Total Parameters: 93879568	 Gradient Parameters: 8081680
[06/15 12:58:57][INFO] visual_prompt:   58: tuned percent:8.609
[06/15 12:58:58][INFO] visual_prompt:   44: Device used for model: 0
[06/15 12:58:58][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/15 12:58:58][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/15 12:59:01][INFO] visual_prompt:  158: Number of images: 1281167
[06/15 12:59:01][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 12:59:01][INFO] visual_prompt:   78: Loading validation data...
[06/15 12:59:01][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/15 12:59:01][INFO] visual_prompt:  158: Number of images: 50000
[06/15 12:59:01][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 12:59:01][INFO] visual_prompt:   81: Loading test data...
[06/15 12:59:01][INFO] visual_prompt:   83: ...no test data is constructed
[06/15 12:59:01][INFO] visual_prompt:  111: Constructing models...
[06/15 12:59:01][INFO] visual_prompt:  114: Setting up Evalutator...
[06/15 12:59:01][INFO] visual_prompt:  116: Setting up Trainer...
[06/15 12:59:01][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/15 12:59:01][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/15 12:59:01][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/15 12:59:01][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/15 12:59:01][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/15 12:59:01][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/15 12:59:01][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/15 12:59:01][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/15 12:59:01][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/15 12:59:01][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/15 12:59:01][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/15 12:59:01][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/15 12:59:01][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/15 12:59:01][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/15 12:59:01][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/15 12:59:01][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/15 12:59:01][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/15 12:59:01][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/15 12:59:01][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/15 12:59:01][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/15 12:59:01][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/15 12:59:01][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/15 12:59:01][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/15 12:59:20][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:59:20][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:59:24][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:59:26][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:59:26][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 12:59:28][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 12:59:29][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 12:59:29][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:02:56][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/15 13:02:56][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,4,5,6
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/15 13:02:56][INFO] visual_prompt:   99: Command line arguments: None
[06/15 13:02:56][INFO] visual_prompt:  108: Training with config:
[06/15 13:02:56][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_shallow_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.1,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'mse',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/15 13:02:59][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/15 13:02:59][INFO] visual_prompt:   56: Total Parameters: 93879568	 Gradient Parameters: 8081680
[06/15 13:02:59][INFO] visual_prompt:   58: tuned percent:8.609
[06/15 13:03:00][INFO] visual_prompt:   44: Device used for model: 0
[06/15 13:03:00][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/15 13:03:00][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/15 13:03:03][INFO] visual_prompt:  158: Number of images: 1281167
[06/15 13:03:03][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 13:03:03][INFO] visual_prompt:   78: Loading validation data...
[06/15 13:03:03][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/15 13:03:03][INFO] visual_prompt:  158: Number of images: 50000
[06/15 13:03:03][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 13:03:03][INFO] visual_prompt:   81: Loading test data...
[06/15 13:03:03][INFO] visual_prompt:   83: ...no test data is constructed
[06/15 13:03:03][INFO] visual_prompt:  111: Constructing models...
[06/15 13:03:03][INFO] visual_prompt:  114: Setting up Evalutator...
[06/15 13:03:03][INFO] visual_prompt:  116: Setting up Trainer...
[06/15 13:03:03][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/15 13:03:03][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/15 13:03:03][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/15 13:03:03][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/15 13:03:03][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/15 13:03:03][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/15 13:03:03][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/15 13:03:03][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/15 13:03:03][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/15 13:03:03][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/15 13:03:03][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/15 13:03:03][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/15 13:03:03][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/15 13:03:03][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/15 13:03:03][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/15 13:03:03][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/15 13:03:03][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/15 13:03:03][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/15 13:03:03][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/15 13:03:03][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/15 13:03:03][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/15 13:03:03][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/15 13:03:03][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/15 13:03:23][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:03:23][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:03:27][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:03:27][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:03:27][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:03:29][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:03:30][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:03:30][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:03:32][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:03:33][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:03:33][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:03:35][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:03:35][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:03:35][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:03:37][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:06:59][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/15 13:06:59][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,4,5,6
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/15 13:06:59][INFO] visual_prompt:   99: Command line arguments: None
[06/15 13:06:59][INFO] visual_prompt:  108: Training with config:
[06/15 13:06:59][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_shallow_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.1,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'mse',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/15 13:07:02][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/15 13:07:02][INFO] visual_prompt:   56: Total Parameters: 93879568	 Gradient Parameters: 8081680
[06/15 13:07:02][INFO] visual_prompt:   58: tuned percent:8.609
[06/15 13:07:03][INFO] visual_prompt:   44: Device used for model: 0
[06/15 13:07:03][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/15 13:07:03][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/15 13:07:06][INFO] visual_prompt:  158: Number of images: 1281167
[06/15 13:07:06][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 13:07:06][INFO] visual_prompt:   78: Loading validation data...
[06/15 13:07:06][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/15 13:07:06][INFO] visual_prompt:  158: Number of images: 50000
[06/15 13:07:06][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 13:07:06][INFO] visual_prompt:   81: Loading test data...
[06/15 13:07:06][INFO] visual_prompt:   83: ...no test data is constructed
[06/15 13:07:06][INFO] visual_prompt:  111: Constructing models...
[06/15 13:07:06][INFO] visual_prompt:  114: Setting up Evalutator...
[06/15 13:07:06][INFO] visual_prompt:  116: Setting up Trainer...
[06/15 13:07:06][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/15 13:07:06][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/15 13:07:06][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/15 13:07:06][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/15 13:07:06][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/15 13:07:06][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/15 13:07:06][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/15 13:07:06][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/15 13:07:06][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/15 13:07:06][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/15 13:07:06][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/15 13:07:06][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/15 13:07:06][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/15 13:07:06][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/15 13:07:06][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/15 13:07:06][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/15 13:07:06][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/15 13:07:06][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/15 13:07:06][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/15 13:07:06][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/15 13:07:06][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/15 13:07:06][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/15 13:07:06][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/15 13:07:25][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:07:25][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:07:29][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:07:30][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:07:30][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:07:32][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:09:36][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/15 13:09:36][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,4,5,6
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/15 13:09:36][INFO] visual_prompt:   99: Command line arguments: None
[06/15 13:09:36][INFO] visual_prompt:  108: Training with config:
[06/15 13:09:36][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_shallow_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.1,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'mse',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/15 13:09:39][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/15 13:09:39][INFO] visual_prompt:   56: Total Parameters: 93879568	 Gradient Parameters: 8081680
[06/15 13:09:39][INFO] visual_prompt:   58: tuned percent:8.609
[06/15 13:09:40][INFO] visual_prompt:   44: Device used for model: 0
[06/15 13:09:40][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/15 13:09:40][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/15 13:09:42][INFO] visual_prompt:  158: Number of images: 1281167
[06/15 13:09:42][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 13:09:42][INFO] visual_prompt:   78: Loading validation data...
[06/15 13:09:42][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/15 13:09:43][INFO] visual_prompt:  158: Number of images: 50000
[06/15 13:09:43][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 13:09:43][INFO] visual_prompt:   81: Loading test data...
[06/15 13:09:43][INFO] visual_prompt:   83: ...no test data is constructed
[06/15 13:09:43][INFO] visual_prompt:  111: Constructing models...
[06/15 13:09:43][INFO] visual_prompt:  114: Setting up Evalutator...
[06/15 13:09:43][INFO] visual_prompt:  116: Setting up Trainer...
[06/15 13:09:43][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/15 13:09:43][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/15 13:09:43][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/15 13:09:43][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/15 13:09:43][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/15 13:09:43][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/15 13:09:43][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/15 13:09:43][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/15 13:09:43][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/15 13:09:43][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/15 13:09:43][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/15 13:09:43][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/15 13:09:43][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/15 13:09:43][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/15 13:09:43][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/15 13:09:43][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/15 13:09:43][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/15 13:09:43][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/15 13:09:43][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/15 13:09:43][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/15 13:09:43][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/15 13:09:43][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/15 13:09:43][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/15 13:10:02][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:10:02][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:10:06][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:10:06][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:10:06][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:10:08][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:10:09][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:10:09][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:10:11][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:10:11][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:10:11][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:10:13][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:10:14][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:10:14][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:10:16][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:10:17][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:10:17][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:10:19][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:10:19][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:10:19][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:10:21][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:10:22][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:10:22][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:10:24][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:10:24][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:10:24][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:10:26][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:10:27][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:10:27][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:10:29][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:10:29][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:10:29][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:10:31][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:10:32][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:10:32][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:10:34][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:10:35][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:10:35][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:10:36][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:10:37][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:10:37][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:10:39][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:10:40][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:10:40][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:10:42][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:10:42][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:10:42][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:10:44][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:10:45][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:10:45][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:10:47][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:10:47][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:10:47][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:10:49][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:10:50][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:10:50][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:10:52][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:10:53][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:10:53][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:10:54][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:10:56][INFO] visual_prompt:  314: Epoch 1 / 100: avg data time: 9.81e-01, avg batch time: 3.6196, average train loss: 0.0607
[06/15 13:11:12][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 13:11:12][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 13:11:12][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 13:11:19][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 13:11:19][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 13:11:19][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 13:11:19][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 13:11:24][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 13:11:24][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 13:11:24][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 13:11:24][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 13:11:29][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 13:11:29][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 13:11:29][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 13:11:29][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 13:11:34][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 13:11:34][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 13:11:34][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 13:11:34][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 13:11:39][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 13:11:39][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 13:11:39][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 13:11:39][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 13:12:37][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/15 13:12:37][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,4,5,6
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/15 13:12:37][INFO] visual_prompt:   99: Command line arguments: None
[06/15 13:12:37][INFO] visual_prompt:  108: Training with config:
[06/15 13:12:37][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_shallow_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.1,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'mse',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/15 13:12:40][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/15 13:12:40][INFO] visual_prompt:   56: Total Parameters: 93879568	 Gradient Parameters: 8081680
[06/15 13:12:40][INFO] visual_prompt:   58: tuned percent:8.609
[06/15 13:12:40][INFO] visual_prompt:   44: Device used for model: 0
[06/15 13:12:40][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/15 13:12:40][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/15 13:12:43][INFO] visual_prompt:  158: Number of images: 1281167
[06/15 13:12:43][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 13:12:43][INFO] visual_prompt:   78: Loading validation data...
[06/15 13:12:43][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/15 13:12:43][INFO] visual_prompt:  158: Number of images: 50000
[06/15 13:12:43][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 13:12:43][INFO] visual_prompt:   81: Loading test data...
[06/15 13:12:43][INFO] visual_prompt:   83: ...no test data is constructed
[06/15 13:12:43][INFO] visual_prompt:  111: Constructing models...
[06/15 13:12:43][INFO] visual_prompt:  114: Setting up Evalutator...
[06/15 13:12:43][INFO] visual_prompt:  116: Setting up Trainer...
[06/15 13:12:43][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/15 13:12:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/15 13:12:44][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/15 13:12:44][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/15 13:12:44][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/15 13:12:44][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/15 13:12:44][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/15 13:12:44][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/15 13:12:44][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/15 13:12:44][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/15 13:12:44][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/15 13:12:44][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/15 13:12:44][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/15 13:12:44][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/15 13:12:44][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/15 13:12:44][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/15 13:12:44][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/15 13:12:44][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/15 13:12:44][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/15 13:12:44][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/15 13:12:44][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/15 13:12:44][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/15 13:12:44][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/15 13:12:44][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/15 13:13:04][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:13:04][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:14:48][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/15 13:14:48][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,4,5,6
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/15 13:14:48][INFO] visual_prompt:   99: Command line arguments: None
[06/15 13:14:48][INFO] visual_prompt:  108: Training with config:
[06/15 13:14:48][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_shallow_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.1,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'mse',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/15 13:14:51][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/15 13:14:51][INFO] visual_prompt:   56: Total Parameters: 93879568	 Gradient Parameters: 8081680
[06/15 13:14:51][INFO] visual_prompt:   58: tuned percent:8.609
[06/15 13:14:52][INFO] visual_prompt:   44: Device used for model: 0
[06/15 13:14:52][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/15 13:14:52][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/15 13:14:54][INFO] visual_prompt:  158: Number of images: 1281167
[06/15 13:14:54][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 13:14:54][INFO] visual_prompt:   78: Loading validation data...
[06/15 13:14:54][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/15 13:14:54][INFO] visual_prompt:  158: Number of images: 50000
[06/15 13:14:54][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 13:14:54][INFO] visual_prompt:   81: Loading test data...
[06/15 13:14:54][INFO] visual_prompt:   83: ...no test data is constructed
[06/15 13:14:54][INFO] visual_prompt:  111: Constructing models...
[06/15 13:14:54][INFO] visual_prompt:  114: Setting up Evalutator...
[06/15 13:14:54][INFO] visual_prompt:  116: Setting up Trainer...
[06/15 13:14:54][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/15 13:14:54][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/15 13:14:54][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/15 13:14:54][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/15 13:14:54][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/15 13:14:54][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/15 13:14:54][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/15 13:14:54][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/15 13:14:54][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/15 13:14:54][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/15 13:14:54][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/15 13:14:54][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/15 13:14:54][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/15 13:14:54][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/15 13:14:54][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/15 13:14:54][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/15 13:14:54][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/15 13:14:54][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/15 13:14:54][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/15 13:14:54][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/15 13:14:54][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/15 13:14:54][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/15 13:14:54][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/15 13:15:14][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:15:14][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:19:20][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/15 13:19:20][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,4,5,6
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/15 13:19:20][INFO] visual_prompt:   99: Command line arguments: None
[06/15 13:19:20][INFO] visual_prompt:  108: Training with config:
[06/15 13:19:20][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_shallow_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.1,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'mse',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/15 13:19:23][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/15 13:19:23][INFO] visual_prompt:   56: Total Parameters: 93879568	 Gradient Parameters: 8081680
[06/15 13:19:23][INFO] visual_prompt:   58: tuned percent:8.609
[06/15 13:19:23][INFO] visual_prompt:   44: Device used for model: 0
[06/15 13:19:23][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/15 13:19:23][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/15 13:19:26][INFO] visual_prompt:  158: Number of images: 1281167
[06/15 13:19:26][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 13:19:26][INFO] visual_prompt:   78: Loading validation data...
[06/15 13:19:26][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/15 13:19:26][INFO] visual_prompt:  158: Number of images: 50000
[06/15 13:19:26][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 13:19:26][INFO] visual_prompt:   81: Loading test data...
[06/15 13:19:26][INFO] visual_prompt:   83: ...no test data is constructed
[06/15 13:19:26][INFO] visual_prompt:  111: Constructing models...
[06/15 13:19:26][INFO] visual_prompt:  114: Setting up Evalutator...
[06/15 13:19:26][INFO] visual_prompt:  116: Setting up Trainer...
[06/15 13:19:26][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/15 13:19:26][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/15 13:19:26][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/15 13:19:26][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/15 13:19:26][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/15 13:19:26][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/15 13:19:26][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/15 13:19:26][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/15 13:19:26][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/15 13:19:26][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/15 13:19:26][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/15 13:19:26][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/15 13:19:26][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/15 13:19:26][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/15 13:19:26][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/15 13:19:26][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/15 13:19:26][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/15 13:19:26][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/15 13:19:26][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/15 13:19:26][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/15 13:19:26][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/15 13:19:26][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/15 13:19:26][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/15 13:19:46][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:19:46][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:21:36][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/15 13:21:36][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,4,5,6
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/15 13:21:36][INFO] visual_prompt:   99: Command line arguments: None
[06/15 13:21:36][INFO] visual_prompt:  108: Training with config:
[06/15 13:21:36][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_shallow_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.1,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'mse',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/15 13:21:39][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/15 13:21:39][INFO] visual_prompt:   56: Total Parameters: 93879568	 Gradient Parameters: 8081680
[06/15 13:21:39][INFO] visual_prompt:   58: tuned percent:8.609
[06/15 13:21:40][INFO] visual_prompt:   44: Device used for model: 0
[06/15 13:21:40][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/15 13:21:40][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/15 13:21:43][INFO] visual_prompt:  158: Number of images: 1281167
[06/15 13:21:43][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 13:21:43][INFO] visual_prompt:   78: Loading validation data...
[06/15 13:21:43][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/15 13:21:43][INFO] visual_prompt:  158: Number of images: 50000
[06/15 13:21:43][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 13:21:43][INFO] visual_prompt:   81: Loading test data...
[06/15 13:21:43][INFO] visual_prompt:   83: ...no test data is constructed
[06/15 13:21:43][INFO] visual_prompt:  111: Constructing models...
[06/15 13:21:43][INFO] visual_prompt:  114: Setting up Evalutator...
[06/15 13:21:43][INFO] visual_prompt:  116: Setting up Trainer...
[06/15 13:21:43][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/15 13:21:43][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/15 13:21:43][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/15 13:21:43][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/15 13:21:43][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/15 13:21:43][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/15 13:21:43][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/15 13:21:43][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/15 13:21:43][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/15 13:21:43][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/15 13:21:43][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/15 13:21:43][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/15 13:21:43][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/15 13:21:43][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/15 13:21:43][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/15 13:21:43][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/15 13:21:43][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/15 13:21:43][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/15 13:21:43][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/15 13:21:43][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/15 13:21:43][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/15 13:21:43][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/15 13:21:43][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/15 13:22:02][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:22:02][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:22:05][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:22:07][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:22:07][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:22:09][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:22:10][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:22:10][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:22:12][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:22:13][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:22:13][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:23:23][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/15 13:23:23][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,4,5,6
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/15 13:23:23][INFO] visual_prompt:   99: Command line arguments: None
[06/15 13:23:23][INFO] visual_prompt:  108: Training with config:
[06/15 13:23:23][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_shallow_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.1,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'mse',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/15 13:23:26][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/15 13:23:26][INFO] visual_prompt:   56: Total Parameters: 93879568	 Gradient Parameters: 8081680
[06/15 13:23:26][INFO] visual_prompt:   58: tuned percent:8.609
[06/15 13:23:26][INFO] visual_prompt:   44: Device used for model: 0
[06/15 13:23:26][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/15 13:23:26][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/15 13:23:29][INFO] visual_prompt:  158: Number of images: 1281167
[06/15 13:23:29][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 13:23:29][INFO] visual_prompt:   78: Loading validation data...
[06/15 13:23:29][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/15 13:23:29][INFO] visual_prompt:  158: Number of images: 50000
[06/15 13:23:29][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 13:23:29][INFO] visual_prompt:   81: Loading test data...
[06/15 13:23:29][INFO] visual_prompt:   83: ...no test data is constructed
[06/15 13:23:29][INFO] visual_prompt:  111: Constructing models...
[06/15 13:23:29][INFO] visual_prompt:  114: Setting up Evalutator...
[06/15 13:23:29][INFO] visual_prompt:  116: Setting up Trainer...
[06/15 13:23:29][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/15 13:23:29][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/15 13:23:29][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/15 13:23:29][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/15 13:23:29][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/15 13:23:29][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/15 13:23:29][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/15 13:23:29][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/15 13:23:29][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/15 13:23:29][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/15 13:23:29][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/15 13:23:29][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/15 13:23:29][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/15 13:23:29][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/15 13:23:29][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/15 13:23:29][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/15 13:23:29][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/15 13:23:29][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/15 13:23:29][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/15 13:23:29][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/15 13:23:29][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/15 13:23:29][INFO] visual_prompt:  233: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/15 13:23:29][INFO] visual_prompt:  252: Training 1 / 100 epoch, with learning rate 0.0
[06/15 13:23:48][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:23:48][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:23:51][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:23:52][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:23:52][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:23:54][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:23:55][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:23:55][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:23:57][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:23:57][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:23:57][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:23:59][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:24:00][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:24:00][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:24:02][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:24:03][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:24:03][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:24:05][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:24:05][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:24:05][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:24:07][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:24:08][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:24:08][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:24:10][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:24:10][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:24:10][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:24:12][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:24:13][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:24:13][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:24:15][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:24:15][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:24:15][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:24:17][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:24:18][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:24:18][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:24:20][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:24:20][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:24:20][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:24:22][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:24:23][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:24:23][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:24:25][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:24:25][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:24:25][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:24:27][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:24:28][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:24:28][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:24:30][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:24:30][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:24:30][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:24:32][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:24:33][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:24:33][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:24:35][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:24:35][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:24:35][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:24:37][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:24:38][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:24:38][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:24:40][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:24:42][INFO] visual_prompt:  314: Epoch 1 / 100: avg data time: 9.60e-01, avg batch time: 3.5724, average train loss: 0.0557
[06/15 13:24:58][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 13:24:58][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 13:24:58][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 13:25:05][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 13:25:05][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 13:25:05][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 13:25:05][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 13:25:10][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 13:25:10][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 13:25:10][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 13:25:10][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 13:25:15][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 13:25:15][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 13:25:15][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 13:25:15][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 13:25:20][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 13:25:20][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 13:25:20][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 13:25:20][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 13:25:25][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 13:25:25][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 13:25:25][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 13:25:25][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 13:25:30][INFO] visual_prompt:  141: shape of model output: torch.Size([64, 10, 768]), targets: torch.Size([64])
[06/15 13:25:30][INFO] visual_prompt:  412: during eval: torch.Size([64, 3, 224, 224])
[06/15 13:25:30][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 10, 3, 224, 224])
[06/15 13:25:30][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 10])
[06/15 13:29:27][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/15 13:29:27][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,4,5,6
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/15 13:29:27][INFO] visual_prompt:   99: Command line arguments: None
[06/15 13:29:27][INFO] visual_prompt:  108: Training with config:
[06/15 13:29:27][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': True,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_shallow_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.1,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'mse',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'USE_CLS_TOKEN': True,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/15 13:29:31][INFO] visual_prompt:   52: Classification Model:
ViT(
  (enc): PromptedVisionTransformer(
    (transformer): PromptedTransformer(
      (embeddings): Embeddings(
        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): Encoder(
        (layer): ModuleList(
          (0): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (7): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (8): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (9): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (10): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (11): Block(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
        )
        (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (prompt_dropout): Dropout(p=0.1, inplace=False)
      (prompt_proj): Identity()
    )
    (head): Identity()
  )
  (head): ModuleList(
    (0): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (1): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (2): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (3): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (4): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (5): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (6): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (7): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (8): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
    (9): MLP(
      (projection): Sequential()
      (last_layer): Linear(in_features=768, out_features=1000, bias=True)
    )
  )
)
[06/15 13:29:31][INFO] visual_prompt:   56: Total Parameters: 93879568	 Gradient Parameters: 8081680
[06/15 13:29:31][INFO] visual_prompt:   58: tuned percent:8.609
[06/15 13:29:31][INFO] visual_prompt:   44: Device used for model: 0
[06/15 13:29:31][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/15 13:29:31][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/15 13:29:34][INFO] visual_prompt:  158: Number of images: 1281167
[06/15 13:29:34][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 13:29:34][INFO] visual_prompt:   78: Loading validation data...
[06/15 13:29:34][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/15 13:29:34][INFO] visual_prompt:  158: Number of images: 50000
[06/15 13:29:34][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 13:29:34][INFO] visual_prompt:   81: Loading test data...
[06/15 13:29:34][INFO] visual_prompt:   83: ...no test data is constructed
[06/15 13:29:34][INFO] visual_prompt:  111: Constructing models...
[06/15 13:29:34][INFO] visual_prompt:  114: Setting up Evalutator...
[06/15 13:29:34][INFO] visual_prompt:  116: Setting up Trainer...
[06/15 13:29:34][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/15 13:29:34][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/15 13:29:34][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/15 13:29:34][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/15 13:29:34][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/15 13:29:34][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/15 13:29:34][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/15 13:29:34][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/15 13:29:34][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/15 13:29:34][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/15 13:29:34][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/15 13:29:34][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/15 13:29:34][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/15 13:29:34][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/15 13:29:34][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/15 13:29:34][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/15 13:29:34][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/15 13:29:34][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/15 13:29:34][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/15 13:29:34][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/15 13:29:34][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/15 13:29:34][INFO] visual_prompt:  238: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/15 13:29:34][INFO] visual_prompt:  257: Training 1 / 100 epoch, with learning rate 0.0
[06/15 13:29:54][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:29:54][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:29:57][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:29:58][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:29:58][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:30:00][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:30:01][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:30:01][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:30:03][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:30:03][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:30:03][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:30:05][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:30:06][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:30:06][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:30:08][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:30:08][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:30:08][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:30:10][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:30:11][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:30:11][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:30:13][INFO] visual_prompt:  140: shape of model output: torch.Size([64, 768]), targets: torch.Size([64])
[06/15 13:30:14][INFO] visual_prompt:   98: shape of inputs: torch.Size([64, 3, 224, 224]) torch.Size([64, 3, 224, 224])
[06/15 13:30:14][INFO] visual_prompt:   99: shape of targets: torch.Size([64]) torch.Size([64, 1])
[06/15 13:33:58][INFO] visual_prompt:   95: Rank of current process: 0. World size: 4
[06/15 13:33:58][INFO] visual_prompt:   97: Environment info:
-------------------  ---------------------------------------------------
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
ENV_MODULE           <not set>
PyTorch              1.13.0+cu117
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3,4,5,6
GPU 0,1,2,3          Tesla V100-SXM2-32GB
Pillow               9.3.0
cv2                  4.7.0
-------------------  ---------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[06/15 13:33:58][INFO] visual_prompt:   99: Command line arguments: None
[06/15 13:33:58][INFO] visual_prompt:  108: Training with config:
[06/15 13:33:58][INFO] visual_prompt:  109: {'CUDNN_BENCHMARK': False,
 'DATA': {'AUGMENTED': True,
          'BATCH_SIZE': 256,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': '../../imagenet1k',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MODE': 'classification',
          'MULTILABEL': False,
          'NAME': 'imagenet',
          'NO_TEST': True,
          'NUMBER_CLASSES': 1000,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True,
          'PREDICT_ROTATION': False,
          'TRANSFORM': 'rotation'},
 'DBG': False,
 'DIST_BACKEND': 'nccl',
 'DIST_INIT_FILE': '',
 'DIST_INIT_PATH': 'tcp://localhost:10001',
 'GPU_ID': None,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'checkpoints',
           'MULTIPLE_HEAD': True,
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': False,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_INVAR_TYPES': 10,
                      'NUM_TOKENS': 500,
                      'NUM_TOKENS_PER_TYPE': 50,
                      'PROJECT': -1,
                      'PROMPT_PATH': '',
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': True,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'cls+prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 4,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'output_shallow_10',
 'RANK': 0,
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 0.1,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'mse',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'USE_CLS_TOKEN': True,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.001,
            'WEIGHT_DECAY_BIAS': 0},
 'WORLD_SIZE': 4}
[06/15 13:34:02][INFO] visual_prompt:   56: Total Parameters: 93879568	 Gradient Parameters: 8081680
[06/15 13:34:02][INFO] visual_prompt:   58: tuned percent:8.609
[06/15 13:34:02][INFO] visual_prompt:   44: Device used for model: 0
[06/15 13:34:02][INFO] visual_prompt:   72: Loading training data (final training data for vtab)...
[06/15 13:34:02][INFO] visual_prompt:   71: Constructing imagenet dataset train...
[06/15 13:34:05][INFO] visual_prompt:  158: Number of images: 1281167
[06/15 13:34:05][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 13:34:05][INFO] visual_prompt:   78: Loading validation data...
[06/15 13:34:05][INFO] visual_prompt:   71: Constructing imagenet dataset val...
[06/15 13:34:05][INFO] visual_prompt:  158: Number of images: 50000
[06/15 13:34:05][INFO] visual_prompt:  159: Number of classes: 1000
[06/15 13:34:05][INFO] visual_prompt:   81: Loading test data...
[06/15 13:34:05][INFO] visual_prompt:   83: ...no test data is constructed
[06/15 13:34:05][INFO] visual_prompt:  111: Constructing models...
[06/15 13:34:05][INFO] visual_prompt:  114: Setting up Evalutator...
[06/15 13:34:05][INFO] visual_prompt:  116: Setting up Trainer...
[06/15 13:34:05][INFO] visual_prompt:   47: 	Setting up the optimizer...
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.prompt_embeddings: True
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.embeddings.position_embeddings: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.embeddings.cls_token: True
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.embeddings.patch_embeddings.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attention_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc1.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.ffn.fc2.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.query.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.key.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.value.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.0.attn.out.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attention_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc1.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.ffn.fc2.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.query.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.key.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.value.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.1.attn.out.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attention_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc1.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.ffn.fc2.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.query.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.key.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.value.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.2.attn.out.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attention_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc1.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.ffn.fc2.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.query.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.key.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.value.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.3.attn.out.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attention_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc1.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.ffn.fc2.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.query.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.key.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.value.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.4.attn.out.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attention_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc1.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.ffn.fc2.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.query.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.key.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.value.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.5.attn.out.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attention_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc1.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.ffn.fc2.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.query.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.key.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.value.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.6.attn.out.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attention_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc1.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.ffn.fc2.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.query.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.key.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.value.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.7.attn.out.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attention_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc1.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.ffn.fc2.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.query.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.key.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.value.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.8.attn.out.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attention_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc1.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.ffn.fc2.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.query.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.key.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.value.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.9.attn.out.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attention_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc1.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.ffn.fc2.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.query.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.key.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.value.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.10.attn.out.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attention_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc1.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.ffn.fc2.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.query.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.key.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.value.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.layer.11.attn.out.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.weight: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.enc.transformer.encoder.encoder_norm.bias: False
[06/15 13:34:05][INFO] visual_prompt:   59: module.head.0.last_layer.weight: True
[06/15 13:34:05][INFO] visual_prompt:   59: module.head.0.last_layer.bias: True
[06/15 13:34:05][INFO] visual_prompt:   59: module.head.1.last_layer.weight: True
[06/15 13:34:05][INFO] visual_prompt:   59: module.head.1.last_layer.bias: True
[06/15 13:34:05][INFO] visual_prompt:   59: module.head.2.last_layer.weight: True
[06/15 13:34:05][INFO] visual_prompt:   59: module.head.2.last_layer.bias: True
[06/15 13:34:05][INFO] visual_prompt:   59: module.head.3.last_layer.weight: True
[06/15 13:34:05][INFO] visual_prompt:   59: module.head.3.last_layer.bias: True
[06/15 13:34:05][INFO] visual_prompt:   59: module.head.4.last_layer.weight: True
[06/15 13:34:05][INFO] visual_prompt:   59: module.head.4.last_layer.bias: True
[06/15 13:34:05][INFO] visual_prompt:   59: module.head.5.last_layer.weight: True
[06/15 13:34:05][INFO] visual_prompt:   59: module.head.5.last_layer.bias: True
[06/15 13:34:05][INFO] visual_prompt:   59: module.head.6.last_layer.weight: True
[06/15 13:34:05][INFO] visual_prompt:   59: module.head.6.last_layer.bias: True
[06/15 13:34:05][INFO] visual_prompt:   59: module.head.7.last_layer.weight: True
[06/15 13:34:05][INFO] visual_prompt:   59: module.head.7.last_layer.bias: True
[06/15 13:34:05][INFO] visual_prompt:   59: module.head.8.last_layer.weight: True
[06/15 13:34:05][INFO] visual_prompt:   59: module.head.8.last_layer.bias: True
[06/15 13:34:05][INFO] visual_prompt:   59: module.head.9.last_layer.weight: True
[06/15 13:34:05][INFO] visual_prompt:   59: module.head.9.last_layer.bias: True
[06/15 13:34:05][INFO] visual_prompt:  238: class weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[06/15 13:34:05][INFO] visual_prompt:  257: Training 1 / 100 epoch, with learning rate 0.0
[06/15 13:36:19][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.1047,	1.1255 s / batch. (data: 3.71e-04). ETA=6 days, 12:24:35, max mem: 15.0 GB 
[06/15 13:38:12][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.0350,	1.1242 s / batch. (data: 3.17e-04). ETA=6 days, 12:11:49, max mem: 15.0 GB 
[06/15 13:40:05][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.1000,	1.1283 s / batch. (data: 3.05e-04). ETA=6 days, 12:44:21, max mem: 15.0 GB 
[06/15 13:41:57][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0871,	1.1273 s / batch. (data: 3.28e-04). ETA=6 days, 12:34:01, max mem: 15.0 GB 
[06/15 13:43:50][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0555,	1.1282 s / batch. (data: 2.68e-04). ETA=6 days, 12:39:22, max mem: 15.0 GB 
[06/15 13:45:43][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0611,	1.1253 s / batch. (data: 2.92e-04). ETA=6 days, 12:13:56, max mem: 15.0 GB 
[06/15 13:47:35][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0544,	1.1308 s / batch. (data: 3.61e-04). ETA=6 days, 12:58:05, max mem: 15.0 GB 
[06/15 13:49:28][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.0416,	1.1279 s / batch. (data: 2.89e-04). ETA=6 days, 12:31:30, max mem: 15.0 GB 
[06/15 13:51:20][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.1134,	1.1249 s / batch. (data: 2.64e-04). ETA=6 days, 12:05:07, max mem: 15.0 GB 
[06/15 13:53:13][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0662,	1.1197 s / batch. (data: 3.13e-04). ETA=6 days, 11:19:38, max mem: 15.0 GB 
[06/15 13:55:05][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.1144,	1.1265 s / batch. (data: 3.09e-04). ETA=6 days, 12:14:35, max mem: 15.0 GB 
[06/15 13:56:58][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.0441,	1.1264 s / batch. (data: 2.63e-04). ETA=6 days, 12:11:26, max mem: 15.0 GB 
[06/15 13:58:50][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0668,	1.1277 s / batch. (data: 3.06e-04). ETA=6 days, 12:20:55, max mem: 15.0 GB 
[06/15 14:00:43][INFO] visual_prompt:  306: 	Training 1400/5004. train loss: 0.0386,	1.1266 s / batch. (data: 3.17e-04). ETA=6 days, 12:09:42, max mem: 15.0 GB 
[06/15 14:02:35][INFO] visual_prompt:  306: 	Training 1500/5004. train loss: 0.0494,	1.1282 s / batch. (data: 3.44e-04). ETA=6 days, 12:21:15, max mem: 15.0 GB 
[06/15 14:04:28][INFO] visual_prompt:  306: 	Training 1600/5004. train loss: 0.0563,	1.1216 s / batch. (data: 3.28e-04). ETA=6 days, 11:24:12, max mem: 15.0 GB 
[06/15 14:06:21][INFO] visual_prompt:  306: 	Training 1700/5004. train loss: 0.1001,	1.1275 s / batch. (data: 3.42e-04). ETA=6 days, 12:11:02, max mem: 15.0 GB 
[06/15 14:08:13][INFO] visual_prompt:  306: 	Training 1800/5004. train loss: 0.1227,	1.1273 s / batch. (data: 4.17e-04). ETA=6 days, 12:07:40, max mem: 15.0 GB 
[06/15 14:10:06][INFO] visual_prompt:  306: 	Training 1900/5004. train loss: 0.0715,	1.1247 s / batch. (data: 3.73e-04). ETA=6 days, 11:44:44, max mem: 15.0 GB 
[06/15 14:11:59][INFO] visual_prompt:  306: 	Training 2000/5004. train loss: 0.0655,	1.1266 s / batch. (data: 3.54e-04). ETA=6 days, 11:58:16, max mem: 15.0 GB 
[06/15 14:13:51][INFO] visual_prompt:  306: 	Training 2100/5004. train loss: 0.1160,	1.1249 s / batch. (data: 3.79e-04). ETA=6 days, 11:41:54, max mem: 15.0 GB 
[06/15 14:15:44][INFO] visual_prompt:  306: 	Training 2200/5004. train loss: 0.0563,	1.1235 s / batch. (data: 3.45e-04). ETA=6 days, 11:28:57, max mem: 15.0 GB 
[06/15 14:17:37][INFO] visual_prompt:  306: 	Training 2300/5004. train loss: 0.1113,	1.1255 s / batch. (data: 3.51e-04). ETA=6 days, 11:43:41, max mem: 15.0 GB 
[06/15 14:19:29][INFO] visual_prompt:  306: 	Training 2400/5004. train loss: 0.1117,	1.1250 s / batch. (data: 3.14e-04). ETA=6 days, 11:37:46, max mem: 15.0 GB 
[06/15 14:21:22][INFO] visual_prompt:  306: 	Training 2500/5004. train loss: 0.1079,	1.1233 s / batch. (data: 2.74e-04). ETA=6 days, 11:21:55, max mem: 15.0 GB 
[06/15 14:23:14][INFO] visual_prompt:  306: 	Training 2600/5004. train loss: 0.0482,	1.1244 s / batch. (data: 3.48e-04). ETA=6 days, 11:28:38, max mem: 15.0 GB 
[06/15 14:25:07][INFO] visual_prompt:  306: 	Training 2700/5004. train loss: 0.1004,	1.1252 s / batch. (data: 3.09e-04). ETA=6 days, 11:33:45, max mem: 15.0 GB 
[06/15 14:26:59][INFO] visual_prompt:  306: 	Training 2800/5004. train loss: 0.1202,	1.1248 s / batch. (data: 3.58e-04). ETA=6 days, 11:28:05, max mem: 15.0 GB 
[06/15 14:28:52][INFO] visual_prompt:  306: 	Training 2900/5004. train loss: 0.1186,	1.1323 s / batch. (data: 3.40e-04). ETA=6 days, 12:28:47, max mem: 15.0 GB 
[06/15 14:30:45][INFO] visual_prompt:  306: 	Training 3000/5004. train loss: 0.1066,	1.1349 s / batch. (data: 3.25e-04). ETA=6 days, 12:48:01, max mem: 15.0 GB 
[06/15 14:32:37][INFO] visual_prompt:  306: 	Training 3100/5004. train loss: 0.0529,	1.1275 s / batch. (data: 3.20e-04). ETA=6 days, 11:44:55, max mem: 15.0 GB 
[06/15 14:34:30][INFO] visual_prompt:  306: 	Training 3200/5004. train loss: 0.0543,	1.1288 s / batch. (data: 3.32e-04). ETA=6 days, 11:54:12, max mem: 15.0 GB 
[06/15 14:36:22][INFO] visual_prompt:  306: 	Training 3300/5004. train loss: 0.0823,	1.1248 s / batch. (data: 3.23e-04). ETA=6 days, 11:19:01, max mem: 15.0 GB 
[06/15 14:38:15][INFO] visual_prompt:  306: 	Training 3400/5004. train loss: 0.1115,	1.1228 s / batch. (data: 3.66e-04). ETA=6 days, 11:00:18, max mem: 15.0 GB 
[06/15 14:40:07][INFO] visual_prompt:  306: 	Training 3500/5004. train loss: 0.0461,	1.1279 s / batch. (data: 3.47e-04). ETA=6 days, 11:40:29, max mem: 15.0 GB 
[06/15 14:42:00][INFO] visual_prompt:  306: 	Training 3600/5004. train loss: 0.1000,	1.1214 s / batch. (data: 3.90e-04). ETA=6 days, 10:45:13, max mem: 15.0 GB 
[06/15 14:43:53][INFO] visual_prompt:  306: 	Training 3700/5004. train loss: 0.1146,	1.1232 s / batch. (data: 3.64e-04). ETA=6 days, 10:58:21, max mem: 15.0 GB 
[06/15 14:45:45][INFO] visual_prompt:  306: 	Training 3800/5004. train loss: 0.0688,	1.1297 s / batch. (data: 3.62e-04). ETA=6 days, 11:50:28, max mem: 15.0 GB 
[06/15 14:47:38][INFO] visual_prompt:  306: 	Training 3900/5004. train loss: 0.0550,	1.1278 s / batch. (data: 4.26e-04). ETA=6 days, 11:32:16, max mem: 15.0 GB 
[06/15 14:49:31][INFO] visual_prompt:  306: 	Training 4000/5004. train loss: 0.0648,	1.1285 s / batch. (data: 3.61e-04). ETA=6 days, 11:36:44, max mem: 15.0 GB 
[06/15 14:51:23][INFO] visual_prompt:  306: 	Training 4100/5004. train loss: 0.0587,	1.1310 s / batch. (data: 3.79e-04). ETA=6 days, 11:55:23, max mem: 15.0 GB 
[06/15 14:53:16][INFO] visual_prompt:  306: 	Training 4200/5004. train loss: 0.0942,	1.1277 s / batch. (data: 2.86e-04). ETA=6 days, 11:26:11, max mem: 15.0 GB 
[06/15 14:55:09][INFO] visual_prompt:  306: 	Training 4300/5004. train loss: 0.0459,	1.1242 s / batch. (data: 2.81e-04). ETA=6 days, 10:55:11, max mem: 15.0 GB 
[06/15 14:57:01][INFO] visual_prompt:  306: 	Training 4400/5004. train loss: 0.0946,	1.1273 s / batch. (data: 4.03e-04). ETA=6 days, 11:19:07, max mem: 15.0 GB 
[06/15 14:58:54][INFO] visual_prompt:  306: 	Training 4500/5004. train loss: 0.0666,	1.1212 s / batch. (data: 3.27e-04). ETA=6 days, 10:26:47, max mem: 15.0 GB 
[06/15 15:00:47][INFO] visual_prompt:  306: 	Training 4600/5004. train loss: 0.0566,	1.1307 s / batch. (data: 3.16e-04). ETA=6 days, 11:42:57, max mem: 15.0 GB 
[06/15 15:02:39][INFO] visual_prompt:  306: 	Training 4700/5004. train loss: 0.0847,	1.1210 s / batch. (data: 2.91e-04). ETA=6 days, 10:21:43, max mem: 15.0 GB 
[06/15 15:04:32][INFO] visual_prompt:  306: 	Training 4800/5004. train loss: 0.1004,	1.1261 s / batch. (data: 2.86e-04). ETA=6 days, 11:01:12, max mem: 15.0 GB 
[06/15 15:06:24][INFO] visual_prompt:  306: 	Training 4900/5004. train loss: 0.1220,	1.1249 s / batch. (data: 2.95e-04). ETA=6 days, 10:50:01, max mem: 15.0 GB 
[06/15 15:08:17][INFO] visual_prompt:  306: 	Training 5000/5004. train loss: 0.0558,	1.1216 s / batch. (data: 8.65e-05). ETA=6 days, 10:20:40, max mem: 15.0 GB 
[06/15 15:08:22][INFO] visual_prompt:  319: Epoch 1 / 100: avg data time: 4.29e-03, avg batch time: 1.1304, average train loss: 0.0733
[06/15 15:17:11][INFO] visual_prompt:  439: 	Test 100/196. loss: 0.071, 5.1356 s / batch. (data: 1.07e-04)max mem: 14.95011 GB 
[06/15 15:25:22][INFO] visual_prompt:  476: Inference (val):avg data time: 1.57e-04, avg batch time: 5.1130, average loss: 0.0722
[06/15 15:25:22][INFO] visual_prompt:  493: Saved invariances for val_imagenet at output_shallow_10/val_imagenet_invariances.json
[06/15 15:25:22][INFO] visual_prompt:  257: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[06/15 15:28:00][INFO] visual_prompt:  306: 	Training 100/5004. train loss: 0.0774,	1.1298 s / batch. (data: 3.10e-04). ETA=6 days, 11:26:30, max mem: 15.0 GB 
[06/15 15:29:53][INFO] visual_prompt:  306: 	Training 200/5004. train loss: 0.1226,	1.1302 s / batch. (data: 3.65e-04). ETA=6 days, 11:27:35, max mem: 15.0 GB 
[06/15 15:31:46][INFO] visual_prompt:  306: 	Training 300/5004. train loss: 0.0638,	1.1275 s / batch. (data: 3.39e-04). ETA=6 days, 11:03:33, max mem: 15.0 GB 
[06/15 15:33:38][INFO] visual_prompt:  306: 	Training 400/5004. train loss: 0.0280,	1.1288 s / batch. (data: 2.63e-04). ETA=6 days, 11:12:13, max mem: 15.0 GB 
[06/15 15:35:31][INFO] visual_prompt:  306: 	Training 500/5004. train loss: 0.0678,	1.1280 s / batch. (data: 3.15e-04). ETA=6 days, 11:03:47, max mem: 15.0 GB 
[06/15 15:37:24][INFO] visual_prompt:  306: 	Training 600/5004. train loss: 0.0411,	1.1280 s / batch. (data: 3.01e-04). ETA=6 days, 11:01:52, max mem: 15.0 GB 
[06/15 15:39:16][INFO] visual_prompt:  306: 	Training 700/5004. train loss: 0.0641,	1.1244 s / batch. (data: 3.13e-04). ETA=6 days, 10:30:11, max mem: 15.0 GB 
[06/15 15:41:09][INFO] visual_prompt:  306: 	Training 800/5004. train loss: 0.1126,	1.1263 s / batch. (data: 2.86e-04). ETA=6 days, 10:44:26, max mem: 15.0 GB 
[06/15 15:43:01][INFO] visual_prompt:  306: 	Training 900/5004. train loss: 0.0571,	1.1248 s / batch. (data: 3.92e-04). ETA=6 days, 10:29:55, max mem: 15.0 GB 
[06/15 15:44:54][INFO] visual_prompt:  306: 	Training 1000/5004. train loss: 0.0630,	1.1283 s / batch. (data: 3.08e-04). ETA=6 days, 10:57:28, max mem: 15.0 GB 
[06/15 15:46:46][INFO] visual_prompt:  306: 	Training 1100/5004. train loss: 0.0789,	1.1251 s / batch. (data: 3.77e-04). ETA=6 days, 10:28:46, max mem: 15.0 GB 
[06/15 15:48:39][INFO] visual_prompt:  306: 	Training 1200/5004. train loss: 0.1091,	1.1257 s / batch. (data: 3.15e-04). ETA=6 days, 10:31:34, max mem: 15.0 GB 
[06/15 15:50:32][INFO] visual_prompt:  306: 	Training 1300/5004. train loss: 0.0379,	1.1246 s / batch. (data: 3.73e-04). ETA=6 days, 10:21:20, max mem: 15.0 GB 
